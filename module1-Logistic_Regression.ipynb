{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "num_samples_class = 1000\n",
    "positive_samples = 4 * np.random.randn(num_samples_class) + 4\n",
    "negative_samples = 6 * np.random.randn(num_samples_class) - 8\n",
    "x = np.concatenate((negative_samples, positive_samples), axis=0)\n",
    "y = np.zeros(num_samples_class*2)\n",
    "y[num_samples_class:] = 1\n",
    "y_onehot = np.zeros((num_samples_class*2, 2))\n",
    "y_onehot[:num_samples_class, 0] = 1\n",
    "y_onehot[num_samples_class:, 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAJCCAYAAADky0LWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13f91c79cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2,1,1)\n",
    "res = plt.hist(x, bins=100)\n",
    "plt.subplot(2,1,2)\n",
    "res = plt.hist([positive_samples, negative_samples], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1)\n",
      "(2000, 2)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "x = np.atleast_2d(x).T\n",
    "print(x.shape)\n",
    "print(y_onehot.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight value = 0.002802628635216813\n"
     ]
    }
   ],
   "source": [
    "# Create logistic regression model\n",
    "w_init = np.random.randn(1)\n",
    "print('Initial weight value = {}'.format(w_init[0]))\n",
    "w = tf.Variable(w_init, dtype=tf.float32)\n",
    "b = tf.Variable(1.0)\n",
    "weighted_x = w * x + b\n",
    "y_prob_pos = tf.nn.sigmoid(weighted_x)\n",
    "y_prob_neg = 1 - y_prob_pos\n",
    "y_prob = tf.concat([y_prob_neg, y_prob_pos], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss: MSE\n",
    "loss = tf.nn.l2_loss(y_prob - y_onehot)\n",
    "# Logistic accuracy\n",
    "correct_prediction = tf.equal(tf.arg_max(y_prob, 1), tf.arg_max(y_onehot, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.0001)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial values: loss=599.0918579101562  w=[ 0.00280263]  b=1.0\n",
      "Step 0\n",
      "loss=599.0918579101562  accuracy=0.5  w=[ 0.27712464]  b=0.9815681576728821\n",
      "\n",
      "Step 1\n",
      "loss=194.53900146484375  accuracy=0.8734999895095825  w=[ 0.31355897]  b=0.9752329587936401\n",
      "\n",
      "Step 2\n",
      "loss=182.98492431640625  accuracy=0.8790000081062317  w=[ 0.33918166]  b=0.9702316522598267\n",
      "\n",
      "Step 3\n",
      "loss=176.9552001953125  accuracy=0.8840000033378601  w=[ 0.35904855]  b=0.9660159945487976\n",
      "\n",
      "Step 4\n",
      "loss=173.21588134765625  accuracy=0.8880000114440918  w=[ 0.37525526]  b=0.9623366594314575\n",
      "\n",
      "Step 5\n",
      "loss=170.67349243164062  accuracy=0.890500009059906  w=[ 0.38890302]  b=0.9590535163879395\n",
      "\n",
      "Step 6\n",
      "loss=168.84042358398438  accuracy=0.890500009059906  w=[ 0.40065086]  b=0.9560781121253967\n",
      "\n",
      "Step 7\n",
      "loss=167.46337890625  accuracy=0.8889999985694885  w=[ 0.41092774]  b=0.953350305557251\n",
      "\n",
      "Step 8\n",
      "loss=166.39682006835938  accuracy=0.8895000219345093  w=[ 0.42002994]  b=0.95082688331604\n",
      "\n",
      "Step 9\n",
      "loss=165.5511016845703  accuracy=0.8899999856948853  w=[ 0.42817131]  b=0.9484756588935852\n",
      "\n",
      "Step 10\n",
      "loss=164.86767578125  accuracy=0.8895000219345093  w=[ 0.43551165]  b=0.9462718367576599\n",
      "\n",
      "Step 11\n",
      "loss=164.30703735351562  accuracy=0.8884999752044678  w=[ 0.44217372]  b=0.9441958665847778\n",
      "\n",
      "Step 12\n",
      "loss=163.84075927734375  accuracy=0.8884999752044678  w=[ 0.44825402]  b=0.9422319531440735\n",
      "\n",
      "Step 13\n",
      "loss=163.44888305664062  accuracy=0.8884999752044678  w=[ 0.4538298]  b=0.9403672218322754\n",
      "\n",
      "Step 14\n",
      "loss=163.1166534423828  accuracy=0.8895000219345093  w=[ 0.45896393]  b=0.9385910034179688\n",
      "\n",
      "Step 15\n",
      "loss=162.8321533203125  accuracy=0.8895000219345093  w=[ 0.46370834]  b=0.9368942379951477\n",
      "\n",
      "Step 16\n",
      "loss=162.5870361328125  accuracy=0.8895000219345093  w=[ 0.46810639]  b=0.9352692365646362\n",
      "\n",
      "Step 17\n",
      "loss=162.3748016357422  accuracy=0.8899999856948853  w=[ 0.47219464]  b=0.933709442615509\n",
      "\n",
      "Step 18\n",
      "loss=162.18943786621094  accuracy=0.8895000219345093  w=[ 0.47600433]  b=0.9322091341018677\n",
      "\n",
      "Step 19\n",
      "loss=162.02703857421875  accuracy=0.8899999856948853  w=[ 0.47956225]  b=0.9307633638381958\n",
      "\n",
      "Step 20\n",
      "loss=161.88400268554688  accuracy=0.8899999856948853  w=[ 0.48289162]  b=0.9293677806854248\n",
      "\n",
      "Step 21\n",
      "loss=161.75729370117188  accuracy=0.8899999856948853  w=[ 0.48601267]  b=0.9280185103416443\n",
      "\n",
      "Step 22\n",
      "loss=161.64517211914062  accuracy=0.8899999856948853  w=[ 0.48894313]  b=0.9267120957374573\n",
      "\n",
      "Step 23\n",
      "loss=161.54498291015625  accuracy=0.8899999856948853  w=[ 0.49169868]  b=0.925445556640625\n",
      "\n",
      "Step 24\n",
      "loss=161.45547485351562  accuracy=0.8899999856948853  w=[ 0.49429321]  b=0.924216091632843\n",
      "\n",
      "Step 25\n",
      "loss=161.37515258789062  accuracy=0.890500009059906  w=[ 0.49673906]  b=0.9230211973190308\n",
      "\n",
      "Step 26\n",
      "loss=161.30300903320312  accuracy=0.8899999856948853  w=[ 0.49904731]  b=0.9218586683273315\n",
      "\n",
      "Step 27\n",
      "loss=161.23782348632812  accuracy=0.8899999856948853  w=[ 0.50122786]  b=0.9207265377044678\n",
      "\n",
      "Step 28\n",
      "loss=161.1788787841797  accuracy=0.890500009059906  w=[ 0.5032897]  b=0.9196228981018066\n",
      "\n",
      "Step 29\n",
      "loss=161.12547302246094  accuracy=0.890500009059906  w=[ 0.50524098]  b=0.9185460209846497\n",
      "\n",
      "Step 30\n",
      "loss=161.07704162597656  accuracy=0.890500009059906  w=[ 0.50708902]  b=0.9174944162368774\n",
      "\n",
      "Step 31\n",
      "loss=161.03274536132812  accuracy=0.890500009059906  w=[ 0.5088405]  b=0.9164667129516602\n",
      "\n",
      "Step 32\n",
      "loss=160.992431640625  accuracy=0.8899999856948853  w=[ 0.51050156]  b=0.915461540222168\n",
      "\n",
      "Step 33\n",
      "loss=160.95553588867188  accuracy=0.8899999856948853  w=[ 0.51207781]  b=0.9144777059555054\n",
      "\n",
      "Step 34\n",
      "loss=160.92181396484375  accuracy=0.8899999856948853  w=[ 0.51357436]  b=0.9135140776634216\n",
      "\n",
      "Step 35\n",
      "loss=160.890869140625  accuracy=0.890500009059906  w=[ 0.51499599]  b=0.9125696420669556\n",
      "\n",
      "Step 36\n",
      "loss=160.86228942871094  accuracy=0.890500009059906  w=[ 0.51634699]  b=0.9116434454917908\n",
      "\n",
      "Step 37\n",
      "loss=160.83592224121094  accuracy=0.890500009059906  w=[ 0.51763141]  b=0.9107345938682556\n",
      "\n",
      "Step 38\n",
      "loss=160.81146240234375  accuracy=0.890500009059906  w=[ 0.51885301]  b=0.9098422527313232\n",
      "\n",
      "Step 39\n",
      "loss=160.78916931152344  accuracy=0.890999972820282  w=[ 0.52001524]  b=0.9089656472206116\n",
      "\n",
      "Step 40\n",
      "loss=160.76840209960938  accuracy=0.890999972820282  w=[ 0.52112126]  b=0.9081040620803833\n",
      "\n",
      "Step 41\n",
      "loss=160.74908447265625  accuracy=0.8914999961853027  w=[ 0.52217412]  b=0.9072567820549011\n",
      "\n",
      "Step 42\n",
      "loss=160.73117065429688  accuracy=0.8920000195503235  w=[ 0.52317655]  b=0.9064232110977173\n",
      "\n",
      "Step 43\n",
      "loss=160.71444702148438  accuracy=0.8914999961853027  w=[ 0.52413112]  b=0.905602753162384\n",
      "\n",
      "Step 44\n",
      "loss=160.69891357421875  accuracy=0.8914999961853027  w=[ 0.52504033]  b=0.9047948122024536\n",
      "\n",
      "Step 45\n",
      "loss=160.68429565429688  accuracy=0.8914999961853027  w=[ 0.52590644]  b=0.9039989113807678\n",
      "\n",
      "Step 46\n",
      "loss=160.67068481445312  accuracy=0.8914999961853027  w=[ 0.52673155]  b=0.9032145142555237\n",
      "\n",
      "Step 47\n",
      "loss=160.65792846679688  accuracy=0.8914999961853027  w=[ 0.52751768]  b=0.902441143989563\n",
      "\n",
      "Step 48\n",
      "loss=160.6458282470703  accuracy=0.8914999961853027  w=[ 0.52826667]  b=0.9016783833503723\n",
      "\n",
      "Step 49\n",
      "loss=160.63470458984375  accuracy=0.8920000195503235  w=[ 0.52898031]  b=0.9009258151054382\n",
      "\n",
      "Step 50\n",
      "loss=160.6240997314453  accuracy=0.8924999833106995  w=[ 0.52966028]  b=0.9001830220222473\n",
      "\n",
      "Step 51\n",
      "loss=160.6141357421875  accuracy=0.8924999833106995  w=[ 0.53030813]  b=0.8994496464729309\n",
      "\n",
      "Step 52\n",
      "loss=160.6046905517578  accuracy=0.8924999833106995  w=[ 0.53092533]  b=0.8987253308296204\n",
      "\n",
      "Step 53\n",
      "loss=160.5958251953125  accuracy=0.8924999833106995  w=[ 0.53151333]  b=0.898009717464447\n",
      "\n",
      "Step 54\n",
      "loss=160.5872039794922  accuracy=0.8924999833106995  w=[ 0.53207338]  b=0.8973025679588318\n",
      "\n",
      "Step 55\n",
      "loss=160.57920837402344  accuracy=0.8924999833106995  w=[ 0.53260678]  b=0.8966034650802612\n",
      "\n",
      "Step 56\n",
      "loss=160.57168579101562  accuracy=0.8924999833106995  w=[ 0.53311467]  b=0.895912230014801\n",
      "\n",
      "Step 57\n",
      "loss=160.56422424316406  accuracy=0.8924999833106995  w=[ 0.53359818]  b=0.8952285051345825\n",
      "\n",
      "Step 58\n",
      "loss=160.55731201171875  accuracy=0.8924999833106995  w=[ 0.53405839]  b=0.8945520520210266\n",
      "\n",
      "Step 59\n",
      "loss=160.55064392089844  accuracy=0.8924999833106995  w=[ 0.53449625]  b=0.8938826322555542\n",
      "\n",
      "Step 60\n",
      "loss=160.54434204101562  accuracy=0.8924999833106995  w=[ 0.53491277]  b=0.893220067024231\n",
      "\n",
      "Step 61\n",
      "loss=160.53839111328125  accuracy=0.8924999833106995  w=[ 0.53530884]  b=0.892564058303833\n",
      "\n",
      "Step 62\n",
      "loss=160.532470703125  accuracy=0.8930000066757202  w=[ 0.53568536]  b=0.891914427280426\n",
      "\n",
      "Step 63\n",
      "loss=160.52694702148438  accuracy=0.8930000066757202  w=[ 0.53604311]  b=0.8912709355354309\n",
      "\n",
      "Step 64\n",
      "loss=160.5216827392578  accuracy=0.8930000066757202  w=[ 0.53638285]  b=0.8906334042549133\n",
      "\n",
      "Step 65\n",
      "loss=160.5164031982422  accuracy=0.8934999704360962  w=[ 0.53670537]  b=0.890001654624939\n",
      "\n",
      "Step 66\n",
      "loss=160.511474609375  accuracy=0.8934999704360962  w=[ 0.53701138]  b=0.8893755078315735\n",
      "\n",
      "Step 67\n",
      "loss=160.50668334960938  accuracy=0.8934999704360962  w=[ 0.53730154]  b=0.8887547850608826\n",
      "\n",
      "Step 68\n",
      "loss=160.50198364257812  accuracy=0.8930000066757202  w=[ 0.5375765]  b=0.8881393074989319\n",
      "\n",
      "Step 69\n",
      "loss=160.49752807617188  accuracy=0.8930000066757202  w=[ 0.53783685]  b=0.8875289559364319\n",
      "\n",
      "Step 70\n",
      "loss=160.49307250976562  accuracy=0.8930000066757202  w=[ 0.53808326]  b=0.8869235515594482\n",
      "\n",
      "Step 71\n",
      "loss=160.48883056640625  accuracy=0.8930000066757202  w=[ 0.53831619]  b=0.8863229155540466\n",
      "\n",
      "Step 72\n",
      "loss=160.48471069335938  accuracy=0.8930000066757202  w=[ 0.53853625]  b=0.8857269883155823\n",
      "\n",
      "Step 73\n",
      "loss=160.48077392578125  accuracy=0.8930000066757202  w=[ 0.53874391]  b=0.8851355910301208\n",
      "\n",
      "Step 74\n",
      "loss=160.47674560546875  accuracy=0.8930000066757202  w=[ 0.53893971]  b=0.8845486044883728\n",
      "\n",
      "Step 75\n",
      "loss=160.47300720214844  accuracy=0.8924999833106995  w=[ 0.53912407]  b=0.8839659690856934\n",
      "\n",
      "Step 76\n",
      "loss=160.4693145751953  accuracy=0.8924999833106995  w=[ 0.53929746]  b=0.8833875060081482\n",
      "\n",
      "Step 77\n",
      "loss=160.46572875976562  accuracy=0.8924999833106995  w=[ 0.5394603]  b=0.8828130960464478\n",
      "\n",
      "Step 78\n",
      "loss=160.46206665039062  accuracy=0.8924999833106995  w=[ 0.53961301]  b=0.8822426795959473\n",
      "\n",
      "Step 79\n",
      "loss=160.45870971679688  accuracy=0.8924999833106995  w=[ 0.539756]  b=0.8816761374473572\n",
      "\n",
      "Step 80\n",
      "loss=160.45526123046875  accuracy=0.8924999833106995  w=[ 0.53988963]  b=0.8811133503913879\n",
      "\n",
      "Step 81\n",
      "loss=160.451904296875  accuracy=0.8924999833106995  w=[ 0.54001427]  b=0.88055419921875\n",
      "\n",
      "Step 82\n",
      "loss=160.44871520996094  accuracy=0.8924999833106995  w=[ 0.54013026]  b=0.8799986243247986\n",
      "\n",
      "Step 83\n",
      "loss=160.44544982910156  accuracy=0.8924999833106995  w=[ 0.5402379]  b=0.8794465661048889\n",
      "\n",
      "Step 84\n",
      "loss=160.4423828125  accuracy=0.8924999833106995  w=[ 0.54033756]  b=0.8788979053497314\n",
      "\n",
      "Step 85\n",
      "loss=160.43923950195312  accuracy=0.8924999833106995  w=[ 0.54042959]  b=0.8783525824546814\n",
      "\n",
      "Step 86\n",
      "loss=160.43618774414062  accuracy=0.8924999833106995  w=[ 0.54051423]  b=0.877810537815094\n",
      "\n",
      "Step 87\n",
      "loss=160.43312072753906  accuracy=0.8924999833106995  w=[ 0.54059178]  b=0.8772716522216797\n",
      "\n",
      "Step 88\n",
      "loss=160.4302215576172  accuracy=0.8924999833106995  w=[ 0.54066247]  b=0.8767358660697937\n",
      "\n",
      "Step 89\n",
      "loss=160.42721557617188  accuracy=0.8924999833106995  w=[ 0.5407266]  b=0.8762031197547913\n",
      "\n",
      "Step 90\n",
      "loss=160.42446899414062  accuracy=0.8924999833106995  w=[ 0.54078442]  b=0.8756733536720276\n",
      "\n",
      "Step 91\n",
      "loss=160.42160034179688  accuracy=0.8924999833106995  w=[ 0.54083616]  b=0.8751465082168579\n",
      "\n",
      "Step 92\n",
      "loss=160.41879272460938  accuracy=0.8920000195503235  w=[ 0.54088205]  b=0.8746224641799927\n",
      "\n",
      "Step 93\n",
      "loss=160.41604614257812  accuracy=0.8920000195503235  w=[ 0.54092234]  b=0.8741012215614319\n",
      "\n",
      "Step 94\n",
      "loss=160.41336059570312  accuracy=0.8920000195503235  w=[ 0.54095721]  b=0.8735827207565308\n",
      "\n",
      "Step 95\n",
      "loss=160.41061401367188  accuracy=0.8920000195503235  w=[ 0.5409869]  b=0.8730669021606445\n",
      "\n",
      "Step 96\n",
      "loss=160.40794372558594  accuracy=0.8920000195503235  w=[ 0.54101157]  b=0.8725537061691284\n",
      "\n",
      "Step 97\n",
      "loss=160.40533447265625  accuracy=0.8920000195503235  w=[ 0.54103142]  b=0.8720430731773376\n",
      "\n",
      "Step 98\n",
      "loss=160.4027099609375  accuracy=0.8920000195503235  w=[ 0.54104668]  b=0.8715349435806274\n",
      "\n",
      "Step 99\n",
      "loss=160.40017700195312  accuracy=0.8920000195503235  w=[ 0.54105747]  b=0.8710293173789978\n",
      "\n",
      "Step 100\n",
      "loss=160.3975830078125  accuracy=0.8920000195503235  w=[ 0.54106396]  b=0.8705260753631592\n",
      "\n",
      "Step 101\n",
      "loss=160.39501953125  accuracy=0.8920000195503235  w=[ 0.54106635]  b=0.8700252175331116\n",
      "\n",
      "Step 102\n",
      "loss=160.39254760742188  accuracy=0.8920000195503235  w=[ 0.5410648]  b=0.869526743888855\n",
      "\n",
      "Step 103\n",
      "loss=160.39016723632812  accuracy=0.8920000195503235  w=[ 0.54105943]  b=0.8690305352210999\n",
      "\n",
      "Step 104\n",
      "loss=160.38778686523438  accuracy=0.8920000195503235  w=[ 0.54105043]  b=0.8685365915298462\n",
      "\n",
      "Step 105\n",
      "loss=160.38525390625  accuracy=0.8920000195503235  w=[ 0.54103792]  b=0.8680448532104492\n",
      "\n",
      "Step 106\n",
      "loss=160.3828125  accuracy=0.8920000195503235  w=[ 0.54102206]  b=0.8675553202629089\n",
      "\n",
      "Step 107\n",
      "loss=160.38040161132812  accuracy=0.8920000195503235  w=[ 0.54100293]  b=0.8670679330825806\n",
      "\n",
      "Step 108\n",
      "loss=160.37806701660156  accuracy=0.8920000195503235  w=[ 0.5409807]  b=0.8665826320648193\n",
      "\n",
      "Step 109\n",
      "loss=160.3756866455078  accuracy=0.8920000195503235  w=[ 0.54095542]  b=0.8660994172096252\n",
      "\n",
      "Step 110\n",
      "loss=160.3733367919922  accuracy=0.8920000195503235  w=[ 0.54092729]  b=0.8656182885169983\n",
      "\n",
      "Step 111\n",
      "loss=160.37100219726562  accuracy=0.8920000195503235  w=[ 0.54089642]  b=0.8651391267776489\n",
      "\n",
      "Step 112\n",
      "loss=160.3686981201172  accuracy=0.8920000195503235  w=[ 0.54086286]  b=0.8646619915962219\n",
      "\n",
      "Step 113\n",
      "loss=160.36636352539062  accuracy=0.8920000195503235  w=[ 0.54082674]  b=0.8641867637634277\n",
      "\n",
      "Step 114\n",
      "loss=160.36407470703125  accuracy=0.8920000195503235  w=[ 0.54078817]  b=0.8637135028839111\n",
      "\n",
      "Step 115\n",
      "loss=160.36190795898438  accuracy=0.8920000195503235  w=[ 0.54074728]  b=0.8632421493530273\n",
      "\n",
      "Step 116\n",
      "loss=160.35971069335938  accuracy=0.8920000195503235  w=[ 0.54070413]  b=0.8627726435661316\n",
      "\n",
      "Step 117\n",
      "loss=160.3575439453125  accuracy=0.8920000195503235  w=[ 0.54065877]  b=0.8623049855232239\n",
      "\n",
      "Step 118\n",
      "loss=160.3553466796875  accuracy=0.8920000195503235  w=[ 0.54061133]  b=0.8618391752243042\n",
      "\n",
      "Step 119\n",
      "loss=160.3531494140625  accuracy=0.8924999833106995  w=[ 0.54056191]  b=0.8613751530647278\n",
      "\n",
      "Step 120\n",
      "loss=160.35092163085938  accuracy=0.8924999833106995  w=[ 0.54051054]  b=0.8609129190444946\n",
      "\n",
      "Step 121\n",
      "loss=160.34881591796875  accuracy=0.8924999833106995  w=[ 0.54045737]  b=0.86045241355896\n",
      "\n",
      "Step 122\n",
      "loss=160.34661865234375  accuracy=0.8924999833106995  w=[ 0.54040241]  b=0.8599936366081238\n",
      "\n",
      "Step 123\n",
      "loss=160.34442138671875  accuracy=0.8924999833106995  w=[ 0.54034579]  b=0.8595365881919861\n",
      "\n",
      "Step 124\n",
      "loss=160.3423309326172  accuracy=0.8924999833106995  w=[ 0.54028749]  b=0.8590812683105469\n",
      "\n",
      "Step 125\n",
      "loss=160.34024047851562  accuracy=0.8924999833106995  w=[ 0.54022765]  b=0.8586276173591614\n",
      "\n",
      "Step 126\n",
      "loss=160.33810424804688  accuracy=0.8924999833106995  w=[ 0.54016632]  b=0.8581755757331848\n",
      "\n",
      "Step 127\n",
      "loss=160.3360595703125  accuracy=0.8924999833106995  w=[ 0.54010355]  b=0.857725203037262\n",
      "\n",
      "Step 128\n",
      "loss=160.33396911621094  accuracy=0.8924999833106995  w=[ 0.54003948]  b=0.857276439666748\n",
      "\n",
      "Step 129\n",
      "loss=160.33200073242188  accuracy=0.8924999833106995  w=[ 0.53997409]  b=0.8568292856216431\n",
      "\n",
      "Step 130\n",
      "loss=160.32994079589844  accuracy=0.8924999833106995  w=[ 0.53990746]  b=0.8563836812973022\n",
      "\n",
      "Step 131\n",
      "loss=160.3278350830078  accuracy=0.8924999833106995  w=[ 0.53983963]  b=0.8559396862983704\n",
      "\n",
      "Step 132\n",
      "loss=160.32582092285156  accuracy=0.8924999833106995  w=[ 0.5397706]  b=0.8554972410202026\n",
      "\n",
      "Step 133\n",
      "loss=160.3238525390625  accuracy=0.8924999833106995  w=[ 0.53970051]  b=0.8550563454627991\n",
      "\n",
      "Step 134\n",
      "loss=160.32183837890625  accuracy=0.8920000195503235  w=[ 0.5396294]  b=0.8546169400215149\n",
      "\n",
      "Step 135\n",
      "loss=160.31985473632812  accuracy=0.8920000195503235  w=[ 0.53955728]  b=0.8541790843009949\n",
      "\n",
      "Step 136\n",
      "loss=160.31790161132812  accuracy=0.8920000195503235  w=[ 0.5394842]  b=0.8537427186965942\n",
      "\n",
      "Step 137\n",
      "loss=160.31605529785156  accuracy=0.8920000195503235  w=[ 0.53941023]  b=0.8533077836036682\n",
      "\n",
      "Step 138\n",
      "loss=160.31393432617188  accuracy=0.8920000195503235  w=[ 0.53933537]  b=0.8528743386268616\n",
      "\n",
      "Step 139\n",
      "loss=160.31207275390625  accuracy=0.8920000195503235  w=[ 0.53925973]  b=0.8524423837661743\n",
      "\n",
      "Step 140\n",
      "loss=160.31024169921875  accuracy=0.8920000195503235  w=[ 0.53918326]  b=0.8520118594169617\n",
      "\n",
      "Step 141\n",
      "loss=160.3082275390625  accuracy=0.8920000195503235  w=[ 0.53910607]  b=0.8515827655792236\n",
      "\n",
      "Step 142\n",
      "loss=160.306396484375  accuracy=0.8920000195503235  w=[ 0.53902817]  b=0.8511551022529602\n",
      "\n",
      "Step 143\n",
      "loss=160.304443359375  accuracy=0.8920000195503235  w=[ 0.53894961]  b=0.8507288098335266\n",
      "\n",
      "Step 144\n",
      "loss=160.30252075195312  accuracy=0.8920000195503235  w=[ 0.53887039]  b=0.8503039479255676\n",
      "\n",
      "Step 145\n",
      "loss=160.3006591796875  accuracy=0.8920000195503235  w=[ 0.53879058]  b=0.8498804569244385\n",
      "\n",
      "Step 146\n",
      "loss=160.2988739013672  accuracy=0.8920000195503235  w=[ 0.53871018]  b=0.8494583368301392\n",
      "\n",
      "Step 147\n",
      "loss=160.296875  accuracy=0.8920000195503235  w=[ 0.53862923]  b=0.8490375876426697\n",
      "\n",
      "Step 148\n",
      "loss=160.29522705078125  accuracy=0.8920000195503235  w=[ 0.53854781]  b=0.84861820936203\n",
      "\n",
      "Step 149\n",
      "loss=160.29336547851562  accuracy=0.8920000195503235  w=[ 0.53846586]  b=0.8482001423835754\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "with tf.Session() as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    init_vals = session.run([loss, w, b])\n",
    "    print('Initial values: loss={}  w={}  b={}'.format(*init_vals))\n",
    "    \n",
    "    for step in range(150):\n",
    "        print('Step {}'.format(step))\n",
    "        vals = session.run([train, loss, accuracy, w, b])\n",
    "        print('loss={}  accuracy={}  w={}  b={}'.format(*vals[1:]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
