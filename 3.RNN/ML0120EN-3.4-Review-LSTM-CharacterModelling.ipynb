{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Big Data University](https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png)\n",
    "# <center> Text generation using RNN/LSTM (Character-level)</center>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<font size = 3><strong>In this notebook you will learn the How to use TensorFlow for create a Recurrent Neural Network</strong></font>\n",
    "<br>    \n",
    "- <a href=\"#intro\">Introduction</a>\n",
    "<br>\n",
    "- <p><a href=\"#arch\">Architectures</a></p>\n",
    "    - <a href=\"#lstm\">Long Short-Term Memory Model (LSTM)</a>\n",
    "\n",
    "- <p><a href=\"#build\">Building a LSTM with TensorFlow</a></p>\n",
    "</div>\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements a Recurrent Neural Network with LSTM/RNN units for training/sampling from character-level language models. In other words the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.  \n",
    "The RNN can then be used to generate text character by character that will look like the original training data. \n",
    "\n",
    "This code is based on this [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), and the code is an step-by-step implimentation of the [character-level implimentation](https://github.com/crazydonkey200/tensorflow-char-rnn).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader\n",
    "The following cell is a class that help to read data from input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "\n",
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size *\n",
    "                                                   self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "#### Batch, number_of_batch, batch_size and seq_length\n",
    "what is batch, number_of_batch, batch_size and seq_length in the charcter level example?  \n",
    "\n",
    "Lets assume the input is 'here is an example'. Then:\n",
    "- txt_length = 18  \n",
    "- seq_length = 3  \n",
    "- batch_size = 2  \n",
    "- number_of_batch = 18/3*2 = 3\n",
    "- batch = array (['h','e','r'],['e',' ','i'])\n",
    "- sample Seq = 'her'  \n",
    "\n",
    "\n",
    "So, what are our actual parameters?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 60 #minibatch size, i.e. size of dataset in each epoch\n",
    "seq_length = 50 #RNN sequence length\n",
    "num_epochs = 25 # you should change it to 50 if you want to see a relatively good results\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128 #size of RNN hidden state\n",
    "num_layers = 2 #number of layers in the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Architecture\n",
    "- each LSTM cell has an input layre, which its size is 128 units. \n",
    "- 128 is dimensionality of embedding vector.\n",
    "\n",
    "\n",
    "\n",
    "#### rnn_size = num_units = num_hidden_units:   = LSTM size\n",
    "\n",
    "\n",
    "- Each LSTM cell has a hidden layer, where there are some hidden units.\n",
    "- The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A).\n",
    "- Each LSTM cell keeps a vector, called __hidden state__ vector, of size n_hidden=128.\n",
    "- A __hidden state__ vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \n",
    "- For each LSTM cell that we initialise, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell. \n",
    "- \"num_units\" is equivalant to \"size of RNN hidden state\"\n",
    "- rnn_size= 128, is also the dimension size of W2V/embedding, for each character/word.\n",
    "- An LSTM keeps two pieces of information as it propagates through time: \n",
    "    - A __hidden state__ vector\n",
    "    - A __previous time-step output__\n",
    "- To make the name num_units more intuitive, you can think of it as the number of hidden units in the LSTM cell, or the number of memory units in the cell.\n",
    "- number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.\n",
    "\n",
    "#### num_layers = 2 \n",
    "- number of layers in the RNN\n",
    "- An input of MultiRNNCell is __cells__ which is list of RNNCells that will be composed in this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-06 18:47:01 URL:https://public.boxcloud.com/d/1/byc_Ao0uctAPEKFl_SiDkFsv_-GQILLDYaMOFMFX7tURITWFXoSXrNes0uKB-9XfRPcj4xqssV48m-AorsAazPBcKQY6V79pUqAGKAwPbORETERAV_6SyloabyBR-1kS1HKMc-U-0mo68EZoEoFn-tCXDfeV9LMt6-_8CaiIwlv9b5dwQa354bhPppN9xBHkyEldjYzWmYr4OZw4hQqZdSaVR-23a9jhJIGNH3I4iCRDpxkW8O9l7LljyNI01sqXriyU2H1_qCNA02e038XL3aQCA9XsbiEQHrMXPOLyFuERMQCPgqa2ixmaC0O4g_FY0yBY5vjYY52Hv79aC11PhEq21EHPXS79mmCoB-Rj5lAnIIvUIP7TVDRkKi7a5GqdqyfHquuntM7p7dvlOX8eVG1W0QF-2jUAQmy8kHV1Lt8NoeKaCGiGxgDofrNfWxgPM8Ca9GxMg3vHO06PrUt5bdenKvQrlRF6y_KKvoHvx2dBrpKwPN6fA-gTyGV-OEHTjfDY6JiiIuCZwrFNuoPxXZSqUSBSl7A1jH6fObHhXvuxS6wZ2AmPk9DEo6iHlbMyW55izQzYIH0YAVOeAZwfQRxeVIldhVeb2uO6zDnXpKS0vRcZZy0bNjN-dREI8HwYmr-hu1xUVdtaY15599DRE87_iVnq6_aDG1kD3vpPIIx6kPPBH6bT8z_JoFxGL6mNzPcDixrk-w04q7glnXaEcCVMRT0gb_-F00jnS_QG-k0fxsBCIZzAHy_nu5Hl-pAZ2TBps2U8DAu7f2DJZcUKIY4Ail6u3ywk5tww_HMVJWAK-n7AqzYY3G16txjbffoDVba6JtAl9BvuReY0GpWc_3gUWM3n0V3LnxEB2IL9EqvQaUx4IHUrSZwsD6P-V3TPOFWtbmtKcmTPKNqgvn8g7AGVD1utVrvhLtlecrMsovNo-jwVXbhHuMQhaDODXmVMcnwoSUx6p2-od-fcC6uDxbBWDq70Se3E8Jewrl-4EU4225JK74Ib2TSY4yFsKoj_rKRAxe1z4doB0p45QJGoQcoUIanbu1b1p2TTVsdQfBzEmdB5-W9GecuGzzc2Qr3jxrGAEuftJ4K7Xmbd-DqfvtvYTg_a6XTyi7ncbh6CStEqOAIYwAiDUM42rZpYOmDXCZVGg3WH0z6IDK4SLbe1U9qEER9V14w./download [1115393/1115393] -> \"../data/character_model/input.txt\" [1]\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p ../data/character_model\n",
    "!wget -nv -O ../data/character_model/input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = TextLoader('../data/character_model/', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "data_loader.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = data_loader.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ..., \n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 50)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size=60, seq_length=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 4, 14, 22, ...,  9, 20,  5],\n",
       "       [20, 10, 29, ..., 10, 18,  4],\n",
       "       ..., \n",
       "       [ 2,  0,  6, ..., 21,  0,  6],\n",
       "       [ 7,  7,  4, ...,  2,  3,  0],\n",
       "       [ 7,  0, 33, ...,  9, 23,  0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 65\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', data_loader.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ",  , !, $, &, ', ,, -, ., 3, :, ;, ?, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p, q, r, s, t, u, v, w, x, y, z\n"
     ]
    }
   ],
   "source": [
    "print(\", \".join(sorted(list(data_loader.chars))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.vocab['t']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining stacked RNN Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__BasicRNNCell__ is the most basic RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a two layer cell\n",
    "with tf.variable_scope('multi_rnn_cell'):\n",
    "    stacked_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicRNNCell(rnn_size) for _ in range(num_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden state size\n",
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 60x50\n",
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 60x50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
    "\n",
    "__BasicRNNCell.zero_state(batch_size, dtype)__ Return zero-filled state tensor(s).\n",
    "\n",
    "Args:\n",
    "\n",
    "batch_size: int, float, or unit Tensor representing the batch size.  \n",
    "dtype: the data type to use for the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size ? 60x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(60, 50) dtype=int32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict={input_data:x, targets:y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [19,  4, 14, ..., 14,  9, 20],\n",
       "       [ 8, 20, 10, ...,  8, 10, 18],\n",
       "       ..., \n",
       "       [21,  2,  0, ...,  0, 21,  0],\n",
       "       [ 9,  7,  7, ...,  0,  2,  3],\n",
       "       [ 3,  7,  0, ...,  5,  9, 23]], dtype=int32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm',reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "        #input_data is a matrix of 60x50 and embedding is dictionary of 65x128 for all 65 characters\n",
    "        # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
    "        # it creates a 60*50*[1*128] matrix\n",
    "        # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
    "        em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
    "        # split: Splits a tensor into sub tensors.\n",
    "        # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
    "        # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
    "        inputs = tf.split(em, seq_length, 1)\n",
    "        # It will convert the list to 50 matrix of [60x128]\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15802948,  0.12496774,  0.16846476, ..., -0.11029783,\n",
       "         0.10446949, -0.10728429],\n",
       "       [ 0.03655826,  0.05616073, -0.11711955, ..., -0.08184877,\n",
       "        -0.0203063 , -0.11410461],\n",
       "       [ 0.1367863 , -0.02998486, -0.11551569, ...,  0.13584189,\n",
       "        -0.10375368,  0.15036653],\n",
       "       ..., \n",
       "       [-0.0543799 , -0.1526051 , -0.1581445 , ...,  0.16187204,\n",
       "         0.06553376,  0.06251667],\n",
       "       [ 0.05188106,  0.03752673,  0.17359488, ...,  0.06455946,\n",
       "        -0.06128889, -0.096526  ],\n",
       "       [ 0.01784262,  0.11226384,  0.17503782, ..., -0.05319381,\n",
       "         0.15254994, -0.07819793]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(60, 50, 128) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 50, 128)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp = session.run(em,feed_dict={input_data:x})\n",
    "emp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0278158 ,  0.03496425, -0.06117989, ...,  0.15364163,\n",
       "         0.17234583, -0.14929119],\n",
       "       [-0.13231784,  0.02223931,  0.08811866, ..., -0.12554383,\n",
       "         0.0763071 , -0.11975383],\n",
       "       [-0.0894724 ,  0.07967277, -0.07505681, ...,  0.11697145,\n",
       "        -0.07735235, -0.16337179],\n",
       "       ..., \n",
       "       [ 0.03655826,  0.05616073, -0.11711955, ..., -0.08184877,\n",
       "        -0.0203063 , -0.11410461],\n",
       "       [-0.16884759,  0.10350727, -0.16924985, ...,  0.13376759,\n",
       "         0.09569244, -0.0209478 ],\n",
       "       [-0.0894724 ,  0.07967277, -0.07505681, ...,  0.11697145,\n",
       "        -0.07735235, -0.16337179]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'split:0' shape=(60, 1, 128) dtype=float32>,\n",
       " <tf.Tensor 'split:1' shape=(60, 1, 128) dtype=float32>,\n",
       " <tf.Tensor 'split:2' shape=(60, 1, 128) dtype=float32>,\n",
       " <tf.Tensor 'split:3' shape=(60, 1, 128) dtype=float32>,\n",
       " <tf.Tensor 'split:4' shape=(60, 1, 128) dtype=float32>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding a batch of 50 sequence to a RNN:\n",
    "- Step 1:  first character of each of the 50 sentences (in a batch) is input in parallel.  \n",
    "- Step 2:  second character of each of the 50 sentences is input in parallel. \n",
    "- Step n: nth character of each of the 50 sentences is input in parallel.  \n",
    "\n",
    "The parallelism is only for efficiency.  Each character in a batch is handled in parallel,  but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0278158 ,  0.03496425, -0.06117989, ...,  0.15364163,\n",
       "         0.17234583, -0.14929119],\n",
       "       [-0.175194  , -0.13956085,  0.05114289, ...,  0.02340321,\n",
       "        -0.14861377,  0.01708506],\n",
       "       [-0.16209128, -0.06615045,  0.01765828, ..., -0.07865245,\n",
       "         0.00767247,  0.17622088],\n",
       "       ..., \n",
       "       [ 0.00171761,  0.02305156, -0.15469344, ..., -0.09677739,\n",
       "         0.00373751, -0.02744491],\n",
       "       [-0.13231784,  0.02223931,  0.08811866, ..., -0.12554383,\n",
       "         0.0763071 , -0.11975383],\n",
       "       [-0.11929911, -0.00828248, -0.10003815, ..., -0.05853085,\n",
       "        -0.13604917, -0.16325609]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0],feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputs is 50x[60*128]\n",
    "outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/multi_rnn_cell/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/multi_rnn_cell_1/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/multi_rnn_cell_2/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/multi_rnn_cell_3/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/multi_rnn_cell_4/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnnlm_1/multi_rnn_cell/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = outputs[0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05824382,  0.02748017,  0.08114314, ...,  0.05969861,\n",
       "        -0.00070989,  0.10022673],\n",
       "       [ 0.0702572 , -0.02185716, -0.01672457, ..., -0.08219045,\n",
       "        -0.0184204 ,  0.03360308],\n",
       "       [ 0.02013035,  0.00143867,  0.04153982, ..., -0.01863404,\n",
       "        -0.09647927,  0.02204642],\n",
       "       ..., \n",
       "       [ 0.03273392, -0.02835438,  0.12730832, ..., -0.03715776,\n",
       "         0.00983012, -0.09039374],\n",
       "       [-0.01099309,  0.17606096,  0.08424649, ..., -0.14397293,\n",
       "        -0.04705437,  0.02811577],\n",
       "       [-0.0236993 ,  0.00772038,  0.00658628, ..., -0.01797423,\n",
       "         0.03807817, -0.07155046]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(test,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs is 50x[60*128]. We need to reshape it to [60x50x128]. Then we can calculate the softmax:\n",
    "\n",
    "softmax_w is [rnn_size, vocab_size], [128x65]\n",
    "\n",
    "[60x50x128]x[128x65]+[60x50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(3000, 128) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat(outputs, 1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(3000, 65) dtype=float32>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01222247,  0.01239169,  0.01315727, ...,  0.01299655,\n",
       "         0.01246927,  0.01485467],\n",
       "       [ 0.01300001,  0.0150417 ,  0.01482361, ...,  0.01177573,\n",
       "         0.01591454,  0.01351067],\n",
       "       [ 0.01423361,  0.01429424,  0.01438452, ...,  0.01387808,\n",
       "         0.01175493,  0.01666627],\n",
       "       ..., \n",
       "       [ 0.01343981,  0.01711948,  0.01274421, ...,  0.01058192,\n",
       "         0.0162006 ,  0.01851727],\n",
       "       [ 0.00842934,  0.01167001,  0.01234786, ...,  0.01293614,\n",
       "         0.01714483,  0.01246518],\n",
       "       [ 0.01316325,  0.01136755,  0.01680106, ...,  0.01283109,\n",
       "         0.01323074,  0.01329494]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits],\n",
    "                [tf.reshape(targets, [-1])],\n",
    "                [tf.ones([batch_size * seq_length])],\n",
    "                vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truediv_1:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "cost\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/multi_rnn_cell_49/cell_0/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/multi_rnn_cell_49/cell_1/basic_rnn_cell/Tanh:0' shape=(60, 128) dtype=float32>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state = last_state\n",
    "final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = tf.Variable(0.0, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_clip =5.\n",
    "tvars = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/weights:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_1/basic_rnn_cell/weights:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_1/basic_rnn_cell/biases:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rnnlm/softmax_w:0',\n",
       " 'rnnlm/softmax_b:0',\n",
       " 'rnnlm/embedding:0',\n",
       " 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/weights:0',\n",
       " 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/biases:0',\n",
       " 'rnnlm/multi_rnn_cell/cell_1/basic_rnn_cell/weights:0',\n",
       " 'rnnlm/multi_rnn_cell/cell_1/basic_rnn_cell/biases:0',\n",
       " 'Variable:0']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_0:0' shape=(128, 65) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(65,) dtype=float32>,\n",
       " <tensorflow.python.framework.ops.IndexedSlices at 0x7f7a15a3b7f0>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(256, 128) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_4:0' shape=(128,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_5:0' shape=(256, 128) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_6:0' shape=(128,) dtype=float32>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.09968632e-02,  -5.91168646e-03,  -4.88472986e-04, ...,\n",
       "          1.23671023e-03,   8.99837760e-04,   9.71516303e-04],\n",
       "       [ -1.02021801e-03,  -1.97622250e-03,  -1.39479572e-03, ...,\n",
       "         -1.61420525e-04,   1.18729095e-05,  -4.82227551e-05],\n",
       "       [  3.01676599e-04,  -3.85406148e-03,  -4.42664605e-03, ...,\n",
       "          5.55628678e-04,   4.58189053e-04,   5.04673808e-04],\n",
       "       ..., \n",
       "       [  2.70265481e-03,   3.95567995e-03,   1.72807439e-03, ...,\n",
       "         -7.03196973e-04,  -5.23120281e-04,  -6.10215415e-04],\n",
       "       [ -8.60623829e-03,  -3.62226507e-03,  -6.48075994e-03, ...,\n",
       "          1.22241932e-03,   8.51926336e-04,   1.07544393e-03],\n",
       "       [ -8.31407309e-03,  -5.29507268e-03,  -3.01819318e-03, ...,\n",
       "          1.02002721e-03,   7.05382670e-04,   9.10283183e-04]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(grads, feed_dict)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using classes\n",
    "Now that we have learned how the networks work, we can put all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self,sample=False):\n",
    "        rnn_size = 128 # size of RNN hidden state vector\n",
    "        batch_size = 60 # minibatch size, i.e. size of dataset in each epoch\n",
    "        seq_length = 50 # RNN sequence length\n",
    "        num_layers = 2 # number of layers in the RNN\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            print(\"sample mode\")\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        # model.cell.state_size is (128, 128)\n",
    "        with tf.variable_scope('lstm_model_cell'):\n",
    "            reuse = tf.get_variable_scope().reuse\n",
    "            self.stacked_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicRNNCell(rnn_size, reuse=reuse) \n",
    "                                                         for _ in range(num_layers)])\n",
    "\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "        # Initial state of the LSTM memory.\n",
    "        # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
    "        self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
    "\n",
    "        with tf.variable_scope('rnnlm_class1'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "            with tf.device(\"/cpu:0\"):\n",
    "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "                #inputs = tf.split(em, seq_length, 1)\n",
    "\n",
    "        # The value of state is updated after processing each batch of chars.\n",
    "        outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([batch_size * seq_length])],\n",
    "                vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "        \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "the input is always a matrix of of shape [n x m]. Where n is the batch size, m is the feature size. \n",
    "In our case, the input shape will be [60 x ??]. \n",
    "\n",
    " \n",
    "size of data is 1113000, number of batches are 371, batch size is 60 and sequence length is 50. so, 50*60*371= 1113000\n",
    "\n",
    "we have 50 epochs. \n",
    "each input matrix will represent 1 update per epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Creating the LSTM object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"rnn\"):\n",
    "    model = LSTMModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "e=1\n",
    "sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "data_loader.reset_batch_pointer()\n",
    "state = sess.run(model.initial_state)\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x, y = data_loader.next_batch()\n",
    "feed = {model.input_data: x, model.targets: y, model.initial_state:state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2207117"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.03888231, -0.03096804,  0.04803599, ..., -0.15671399,\n",
       "          0.11745685,  0.01139924],\n",
       "        [ 0.28310701,  0.01702675, -0.3102383 , ...,  0.22127898,\n",
       "         -0.21215115, -0.25415346],\n",
       "        [-0.17626575,  0.08315198, -0.24285816, ..., -0.18454163,\n",
       "          0.1577169 ,  0.09897756],\n",
       "        ..., \n",
       "        [ 0.14232227,  0.22863497,  0.00102451, ...,  0.04010071,\n",
       "         -0.0842349 , -0.13486421],\n",
       "        [-0.0808434 ,  0.08186   , -0.08142276, ..., -0.1167951 ,\n",
       "         -0.02547013, -0.19040173],\n",
       "        [-0.23224947,  0.17555398, -0.23806581, ..., -0.05937954,\n",
       "         -0.21333382,  0.00371905]], dtype=float32),\n",
       " array([[ 0.15038921, -0.20966865, -0.10630569, ...,  0.08369136,\n",
       "         -0.25725228, -0.02398055],\n",
       "        [-0.17314234,  0.12231099, -0.08771722, ..., -0.19649088,\n",
       "         -0.1640528 ,  0.06887512],\n",
       "        [ 0.08831589, -0.17550877, -0.17147467, ...,  0.10027228,\n",
       "         -0.1832839 , -0.14662866],\n",
       "        ..., \n",
       "        [ 0.16485128,  0.05310654, -0.1109726 , ...,  0.26228723,\n",
       "         -0.02531261,  0.00233954],\n",
       "        [-0.04972073, -0.12295981,  0.02013682, ...,  0.13064176,\n",
       "         -0.01059116, -0.07805133],\n",
       "        [-0.1392131 ,  0.31443673,  0.00215041, ..., -0.12960409,\n",
       "         -0.32908654, -0.04215321]], dtype=float32))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train usinng LSTMModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (371 / 18550 batches, lr=0.0100)\n",
      "Train_loss=1.738   Time/Batch=21.019 ms\n",
      "\n",
      "Epoch 2 (742 / 18550 batches, lr=0.0097)\n",
      "Train_loss=1.651   Time/Batch=21.111 ms\n",
      "\n",
      "Epoch 3 (1113 / 18550 batches, lr=0.0094)\n",
      "Train_loss=1.610   Time/Batch=24.332 ms\n",
      "\n",
      "Epoch 4 (1484 / 18550 batches, lr=0.0091)\n",
      "Train_loss=1.596   Time/Batch=20.391 ms\n",
      "\n",
      "Epoch 5 (1855 / 18550 batches, lr=0.0089)\n",
      "Train_loss=1.583   Time/Batch=21.730 ms\n",
      "\n",
      "Epoch 6 (2226 / 18550 batches, lr=0.0086)\n",
      "Train_loss=1.581   Time/Batch=27.467 ms\n",
      "\n",
      "Epoch 7 (2597 / 18550 batches, lr=0.0083)\n",
      "Train_loss=1.581   Time/Batch=25.622 ms\n",
      "\n",
      "Epoch 8 (2968 / 18550 batches, lr=0.0081)\n",
      "Train_loss=1.572   Time/Batch=21.569 ms\n",
      "\n",
      "Epoch 9 (3339 / 18550 batches, lr=0.0078)\n",
      "Train_loss=1.567   Time/Batch=21.797 ms\n",
      "\n",
      "Epoch 10 (3710 / 18550 batches, lr=0.0076)\n",
      "Train_loss=1.571   Time/Batch=20.609 ms\n",
      "\n",
      "Epoch 11 (4081 / 18550 batches, lr=0.0074)\n",
      "Train_loss=1.574   Time/Batch=23.571 ms\n",
      "\n",
      "Epoch 12 (4452 / 18550 batches, lr=0.0072)\n",
      "Train_loss=1.580   Time/Batch=23.896 ms\n",
      "\n",
      "Epoch 13 (4823 / 18550 batches, lr=0.0069)\n",
      "Train_loss=1.574   Time/Batch=24.477 ms\n",
      "\n",
      "Epoch 14 (5194 / 18550 batches, lr=0.0067)\n",
      "Train_loss=1.584   Time/Batch=24.223 ms\n",
      "\n",
      "Epoch 15 (5565 / 18550 batches, lr=0.0065)\n",
      "Train_loss=1.566   Time/Batch=23.789 ms\n",
      "\n",
      "Epoch 16 (5936 / 18550 batches, lr=0.0063)\n",
      "Train_loss=1.550   Time/Batch=22.209 ms\n",
      "\n",
      "Epoch 17 (6307 / 18550 batches, lr=0.0061)\n",
      "Train_loss=1.546   Time/Batch=21.574 ms\n",
      "\n",
      "Epoch 18 (6678 / 18550 batches, lr=0.0060)\n",
      "Train_loss=1.543   Time/Batch=23.474 ms\n",
      "\n",
      "Epoch 19 (7049 / 18550 batches, lr=0.0058)\n",
      "Train_loss=1.546   Time/Batch=28.259 ms\n",
      "\n",
      "Epoch 20 (7420 / 18550 batches, lr=0.0056)\n",
      "Train_loss=1.544   Time/Batch=26.989 ms\n",
      "\n",
      "Epoch 21 (7791 / 18550 batches, lr=0.0054)\n",
      "Train_loss=1.548   Time/Batch=23.103 ms\n",
      "\n",
      "Epoch 22 (8162 / 18550 batches, lr=0.0053)\n",
      "Train_loss=1.528   Time/Batch=32.728 ms\n",
      "\n",
      "Epoch 23 (8533 / 18550 batches, lr=0.0051)\n",
      "Train_loss=1.533   Time/Batch=22.531 ms\n",
      "\n",
      "Epoch 24 (8904 / 18550 batches, lr=0.0050)\n",
      "Train_loss=1.536   Time/Batch=21.102 ms\n",
      "\n",
      "Epoch 25 (9275 / 18550 batches, lr=0.0048)\n",
      "Train_loss=1.533   Time/Batch=21.267 ms\n",
      "\n",
      "Epoch 26 (9646 / 18550 batches, lr=0.0047)\n",
      "Train_loss=1.510   Time/Batch=21.004 ms\n",
      "\n",
      "Epoch 27 (10017 / 18550 batches, lr=0.0045)\n",
      "Train_loss=1.510   Time/Batch=21.842 ms\n",
      "\n",
      "Epoch 28 (10388 / 18550 batches, lr=0.0044)\n",
      "Train_loss=1.507   Time/Batch=20.926 ms\n",
      "\n",
      "Epoch 29 (10759 / 18550 batches, lr=0.0043)\n",
      "Train_loss=1.501   Time/Batch=31.265 ms\n",
      "\n",
      "Epoch 30 (11130 / 18550 batches, lr=0.0041)\n",
      "Train_loss=1.503   Time/Batch=27.283 ms\n",
      "\n",
      "Epoch 31 (11501 / 18550 batches, lr=0.0040)\n",
      "Train_loss=1.502   Time/Batch=23.837 ms\n",
      "\n",
      "Epoch 32 (11872 / 18550 batches, lr=0.0039)\n",
      "Train_loss=1.494   Time/Batch=22.840 ms\n",
      "\n",
      "Epoch 33 (12243 / 18550 batches, lr=0.0038)\n",
      "Train_loss=1.498   Time/Batch=20.927 ms\n",
      "\n",
      "Epoch 34 (12614 / 18550 batches, lr=0.0037)\n",
      "Train_loss=1.481   Time/Batch=20.739 ms\n",
      "\n",
      "Epoch 35 (12985 / 18550 batches, lr=0.0036)\n",
      "Train_loss=1.488   Time/Batch=23.424 ms\n",
      "\n",
      "Epoch 36 (13356 / 18550 batches, lr=0.0034)\n",
      "Train_loss=1.485   Time/Batch=35.723 ms\n",
      "\n",
      "Epoch 37 (13727 / 18550 batches, lr=0.0033)\n",
      "Train_loss=1.463   Time/Batch=21.494 ms\n",
      "\n",
      "Epoch 38 (14098 / 18550 batches, lr=0.0032)\n",
      "Train_loss=1.470   Time/Batch=21.276 ms\n",
      "\n",
      "Epoch 39 (14469 / 18550 batches, lr=0.0031)\n",
      "Train_loss=1.466   Time/Batch=21.926 ms\n",
      "\n",
      "Epoch 40 (14840 / 18550 batches, lr=0.0030)\n",
      "Train_loss=1.469   Time/Batch=25.202 ms\n",
      "\n",
      "Epoch 41 (15211 / 18550 batches, lr=0.0030)\n",
      "Train_loss=1.461   Time/Batch=36.844 ms\n",
      "\n",
      "Epoch 42 (15582 / 18550 batches, lr=0.0029)\n",
      "Train_loss=1.459   Time/Batch=21.394 ms\n",
      "\n",
      "Epoch 43 (15953 / 18550 batches, lr=0.0028)\n",
      "Train_loss=1.454   Time/Batch=25.923 ms\n",
      "\n",
      "Epoch 44 (16324 / 18550 batches, lr=0.0027)\n",
      "Train_loss=1.453   Time/Batch=20.543 ms\n",
      "\n",
      "Epoch 45 (16695 / 18550 batches, lr=0.0026)\n",
      "Train_loss=1.448   Time/Batch=20.614 ms\n",
      "\n",
      "Epoch 46 (17066 / 18550 batches, lr=0.0025)\n",
      "Train_loss=1.447   Time/Batch=43.739 ms\n",
      "\n",
      "Epoch 47 (17437 / 18550 batches, lr=0.0025)\n",
      "Train_loss=1.447   Time/Batch=20.452 ms\n",
      "\n",
      "Epoch 48 (17808 / 18550 batches, lr=0.0024)\n",
      "Train_loss=1.446   Time/Batch=24.357 ms\n",
      "\n",
      "Epoch 49 (18179 / 18550 batches, lr=0.0023)\n",
      "Train_loss=1.437   Time/Batch=21.503 ms\n",
      "\n",
      "Epoch 50 (18550 / 18550 batches, lr=0.0022)\n",
      "Train_loss=1.438   Time/Batch=21.252 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_lr = 0.01\n",
    "num_epochs = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 5 for test, but should be higher\n",
    "        current_lr = initial_lr * (decay_rate ** e)\n",
    "        sess.run(tf.assign(model.lr, current_lr))\n",
    "        print('Epoch {} ({} / {} batches, lr={:.4f})'.format(\n",
    "            e+1,\n",
    "            (e+1) * data_loader.num_batches, \n",
    "            num_epochs * data_loader.num_batches,\n",
    "            current_lr\n",
    "        ))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        state = sess.run(model.initial_state) # (2x[60x128])\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}\n",
    "            train_loss, state, _ = sess.run([model.cost, model.final_state, model.train_op], feed)\n",
    "            end = time.time()\n",
    "        print(\"Train_loss={:.3f}   Time/Batch={:.3f} ms\".format(\n",
    "            train_loss, \n",
    "            (end - start) * 1000\n",
    "        ))\n",
    "        print()\n",
    "        #model.sample(sess, data_loader.chars , data_loader.vocab, num=200, prime='The ', sampling_type=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample mode\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope(\"sample_test\"):\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    m = LSTMModel(sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prime='The '\n",
    "num=200\n",
    "sampling_type=1\n",
    "vocab=data_loader.vocab\n",
    "chars=data_loader.chars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32),\n",
       " array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]], dtype=float32))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(m.initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print state\n",
    "sess.run(tf.global_variables_initializer())\n",
    "state=sess.run(m.initial_state)\n",
    "for char in prime[:-1]:\n",
    "    x = np.zeros((1, 1))\n",
    "    x[0, 0] = vocab[char]\n",
    "    feed = {m.input_data: x, m.initial_state:state}\n",
    "    [state] = sess.run([m.final_state], feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.10137668, -0.08077186, -0.10750609,  0.19757108, -0.00856418,\n",
       "         -0.08256163,  0.05348716,  0.14192471, -0.17346245,  0.22914389,\n",
       "         -0.02725136,  0.18006587, -0.04484245,  0.08095631, -0.14055172,\n",
       "          0.04505174,  0.03672248,  0.26555255, -0.05147567, -0.1171177 ,\n",
       "         -0.11120223, -0.15657797,  0.11259519, -0.09635724, -0.03603043,\n",
       "          0.1233211 , -0.10203333,  0.04217954,  0.05316259, -0.11408927,\n",
       "         -0.06685334, -0.15804359, -0.0777316 ,  0.00410224, -0.41452974,\n",
       "          0.05459766, -0.09610467, -0.04428494, -0.12285881, -0.13246138,\n",
       "          0.10123338,  0.02276151, -0.03532519,  0.01705571, -0.20884265,\n",
       "         -0.11671029, -0.01063112,  0.07384278,  0.14188001, -0.05980527,\n",
       "          0.04676616, -0.07080595, -0.20595013,  0.09804212, -0.0092013 ,\n",
       "         -0.113457  , -0.08490174,  0.06722686, -0.17515771,  0.1102583 ,\n",
       "          0.13089943,  0.07752761, -0.08167016,  0.06896663, -0.2031312 ,\n",
       "         -0.0597404 ,  0.13616781, -0.04560331,  0.13034494,  0.20417435,\n",
       "          0.04325606,  0.00573507, -0.22928818, -0.03442378,  0.18944466,\n",
       "         -0.26987237, -0.06087158, -0.27903813,  0.08870605,  0.07701644,\n",
       "          0.07907534, -0.0812204 ,  0.02220624,  0.01609818, -0.07551125,\n",
       "          0.04339306,  0.21259697,  0.04101811, -0.11451589,  0.04190128,\n",
       "          0.02117959, -0.1832577 , -0.14810011,  0.05010875, -0.0102651 ,\n",
       "          0.04474885,  0.13458964,  0.03243554, -0.17707799, -0.0689575 ,\n",
       "         -0.1326219 , -0.18451893, -0.07774367, -0.1763258 , -0.27977592,\n",
       "         -0.03119628, -0.16914873,  0.05513806, -0.01554437,  0.02857593,\n",
       "          0.02717626, -0.12470067,  0.12777159, -0.04399659, -0.12828831,\n",
       "         -0.16408128,  0.08113669,  0.10253198, -0.15607378,  0.07909797,\n",
       "         -0.0021287 ,  0.14629997, -0.07415134, -0.07934821,  0.19297239,\n",
       "          0.10837644,  0.04147336, -0.01824292]], dtype=float32),\n",
       " array([[ 0.07738093,  0.22042066,  0.05165083,  0.30390278,  0.00871414,\n",
       "         -0.14318338,  0.07483237,  0.20528719,  0.2251171 , -0.02318639,\n",
       "          0.28030419, -0.12808535,  0.06734945,  0.08093263, -0.01277104,\n",
       "          0.06185715,  0.10275829,  0.04980355,  0.19966951,  0.0525166 ,\n",
       "         -0.24898754, -0.02813834,  0.21692942, -0.0500885 ,  0.03983887,\n",
       "          0.03889215,  0.07217672,  0.04774928, -0.07274768,  0.03875766,\n",
       "          0.14622089, -0.09669452,  0.12846969, -0.20952448, -0.29123586,\n",
       "         -0.27231941, -0.00943131,  0.02967752,  0.04326952, -0.04527986,\n",
       "          0.14584225,  0.05892554,  0.11702228, -0.02320902, -0.05764144,\n",
       "         -0.11155446, -0.14566803, -0.08758251, -0.13283403,  0.27522418,\n",
       "         -0.43099296, -0.16802563,  0.1184596 ,  0.03716612, -0.16540919,\n",
       "          0.01076523,  0.02430989,  0.07016046, -0.00366621,  0.04100511,\n",
       "          0.24408653,  0.16368438, -0.08049961, -0.17646037, -0.01520931,\n",
       "         -0.06785739,  0.12322748,  0.14283063,  0.18414274,  0.01402618,\n",
       "         -0.04684905, -0.38175866,  0.20015089, -0.01066009, -0.05620852,\n",
       "          0.15168373, -0.09835045, -0.08095843, -0.01322879,  0.05104158,\n",
       "          0.09952886, -0.12505604,  0.13366103,  0.01276724, -0.17052272,\n",
       "          0.02160972,  0.18143211,  0.03839583,  0.15305941,  0.03249511,\n",
       "         -0.12118045, -0.08675839, -0.06038449, -0.09182753, -0.26647931,\n",
       "         -0.07934324, -0.12587942,  0.05177328, -0.12380558, -0.11977122,\n",
       "         -0.14755502, -0.09102193,  0.05424568, -0.10295357, -0.14681754,\n",
       "         -0.13618389,  0.26718181, -0.12232522, -0.00061985,  0.03617959,\n",
       "         -0.00300557,  0.30095863,  0.10897404,  0.03429548, -0.06828348,\n",
       "          0.11467907,  0.04400025,  0.28167987, -0.04302198, -0.16568822,\n",
       "         -0.01484874, -0.04803999,  0.09997033, -0.19863886,  0.1481382 ,\n",
       "         -0.11139414, -0.24466559,  0.03575931]], dtype=float32))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_pick(weights):\n",
    "    t = np.cumsum(weights)\n",
    "    s = np.sum(weights)\n",
    "    return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "ret = prime\n",
    "char = prime[-1]\n",
    "for n in range(num):\n",
    "    x = np.zeros((1, 1))\n",
    "    x[0, 0] = vocab[char]\n",
    "    feed = {m.input_data: x, m.initial_state:state}\n",
    "    [probs, state] = sess.run([m.probs, m.final_state], feed)\n",
    "    p = probs[0]\n",
    "\n",
    "    if sampling_type == 0:\n",
    "        sample = np.argmax(p)\n",
    "    elif sampling_type == 2:\n",
    "        if char == ' ':\n",
    "            sample = weighted_pick(p)\n",
    "        else:\n",
    "            sample = np.argmax(p)\n",
    "    else: # sampling_type == 1 default:\n",
    "        sample = weighted_pick(p)\n",
    "\n",
    "    pred = chars[sample]\n",
    "    ret += pred\n",
    "    char = pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The ExuedB,kjB?Q!hBCjd$JGjQUITK kpU.cXB'dZhV\\nTx,Ap,Tu;QYfx vtweUSuqa UygPBQfCWypqn3Ub!jSSXfAA;JdiUOZqRR?3aZr$Vb\\ny'U;PVF.$&GGaF?H:&!EDPiim &GAl'xYxxA&zIlG'ajq,,pv&zAUTrUIRu ?luAmv-tOTL$h:EKZ,kZgl?noLE$wtTc\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample using function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The 3ESYEnEyh.3seImxt.k\\nU;Ss\\n?k?am$KGASlvrd-PoXvX:CyDNDDXOHF ?Hclt?oFG-u?rRaob'yU&KwNW3QdNHO3 WzsAISCQl?wlca$AfV&awKe\\niw&w'J'Gz!&h'uKM,uRzJpN:yv?jNUHHlqCDYnjhHScjKHs?q'mkp\\nK$YpdksOTGjztDsrs$K-W&McaY$LD!VQ\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "state=sess.run(m.initial_state)\n",
    "m.sample(sess, data_loader.chars , data_loader.vocab, num=200, prime='The ', sampling_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
