{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/jvcqp2iy2jlx2b32rmzdt0tx8lvxgzkp.png\" width = 300, align = \"center\"></a>\n",
    "\n",
    "<h1 align=center><font size = 5>RECURRENT NETWORKS and LSTM IN DEEP LEARNING</font></h1>\n",
    "\n",
    "\n",
    "## Applying Recurrent Neural Networks/LSTM for Language Modelling\n",
    "Hello and welcome to this part. In this notebook, we will go over the topic of what Language Modelling is and create a Recurrent Neural Network model based on the Long Short-Term Memory unit to train and be benchmarked by the Penn Treebank. By the end of this notebook, you should be able to understand how TensorFlow builds and executes a RNN model for Language Modelling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Objective\n",
    "By now, you should have an understanding of how Recurrent Networks work -- a specialized model to process sequential data by keeping track of the \"state\" or context. In this notebook, we go over a TensorFlow code snippet for creating a model focused on **Language Modelling** -- a very relevant task that is the cornerstone of many different linguistic problems such as **Speech Recognition, Machine Translation and Image Captioning**. For this, we will be using the Penn Treebank, which is an often-used dataset for benchmarking Language Modelling models.\n",
    "\n",
    "## What exactly is Language Modelling?\n",
    "Language Modelling, to put it simply, **is the task of assigning probabilities to sequences of words**. This means that, given a context of one or a few words in the language the model was trained on, the model should have a knowledge of what are the most probable words or sequence of words for the sentence. Language Modelling is one of the tasks under Natural Language Processing, and one of the most important.\n",
    "\n",
    "<img src=https://ibm.box.com/shared/static/1d1i5gub6wljby2vani2vzxp0xsph702.png width=\"768\"/>\n",
    "<center>*Example of a sentence being predicted*</center>\n",
    "\n",
    "In this example, one can see the predictions for the next word of a sentence, given the context \"This is an\". As you can see, this boils down to a sequential data analysis task -- you are given a word or a sequence of words (the input data), and, given the context (the state), you need to find out what is the next word (the prediction). This kind of analysis is very important for language-related tasks such as **Speech Recognition, Machine Translation, Image Captioning, Text Correction** and many other very relevant problems. \n",
    "\n",
    "<img src=https://ibm.box.com/shared/static/az39idf9ipfdpc5ugifpgxnydelhyf3i.png width=\"1080\"/>\n",
    "<center>*The above example schematized as an RNN in execution*</center>\n",
    "\n",
    "As the above image shows, Recurrent Network models fit this problem like a glove. Alongside LSTM and its capacity to maintain the model's state for over one thousand time steps, we have all the tools we need to undertake this problem. The goal for this notebook is to create a model that can reach **low levels of perplexity** on our desired dataset.\n",
    "\n",
    "For Language Modelling problems, **perplexity** is the way to gauge efficiency. Perplexity is simply a measure of how well a probabilistic model is able to predict its sample. A higher-level way to explain this would be saying that **low perplexity means a higher degree of trust in the predictions the model makes**. Therefore, the lower perplexity is, the better.\n",
    "\n",
    "## The Penn Treebank dataset\n",
    "Historically, datasets big enough for Natural Language Processing are hard to come by. This is in part due to the necessity of the sentences to be broken down and tagged with a certain degree of correctness -- or else the models trained on it won't be able to be correct at all. This means that we need a **large amount of data, annotated by or at least corrected by humans**. This is, of course, not an easy task at all.\n",
    "\n",
    "The Penn Treebank, or PTB for short, is a dataset maintained by the University of Pennsylvania. It is *huge* -- there are over **four million and eight hundred thousand** annotated words in it, all corrected by humans. It is composed of many different sources, from abstracts of Department of Energy papers to texts from the Library of America. Since it is verifiably correct and of such a huge size, the Penn Treebank has been used time and time again as a benchmark dataset for Language Modelling.\n",
    "\n",
    "The dataset is divided in different kinds of annotations, such as Piece-of-Speech, Syntactic and Semantic skeletons. For this example, we will simply use a sample of clean, non-annotated words (with the exception of one tag -- `<unk>`, which is used for rare words such as uncommon proper nouns) for our model. This means that we just want to predict what the next words would be, not what they mean in context or their classes on a given sentence. \n",
    "<br/>\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<center>the percentage of lung cancer deaths among the workers at the west `<unk>` mass. paper factory appears to be the highest for any asbestos workers studied in western industrialized countries he said \n",
    " the plant which is owned by `<unk>` & `<unk>` co. was under contract with `<unk>` to make the cigarette filters \n",
    " the finding probably will support those who argue that the u.s. should regulate the class of asbestos including `<unk>` more `<unk>` than the common kind of asbestos `<unk>` found in most schools and other buildings dr. `<unk>` said</center>\n",
    "</div>\n",
    " <center>*Example of text from the dataset we are going to use, `ptb.train`*</center>\n",
    " <br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Word Embeddings</h2><br/>\n",
    "\n",
    "For better processing, in this example, we will make use of [**word embeddings**]( [https://www.tensorflow.org/tutorials/word2vec/), which are **a way of representing sentence structures or words as n-dimensional vectors (where n is a reasonably high number, such as 200 or 500) of real numbers**. Basically, we will assign each word a randomly-initialized vector, and input those into the network to be processed. After a number of iterations, these vectors are expected to assume values that help the network to correctly predict what it needs to -- in our case, the probable next word in the sentence. This is shown to be very effective in Natural Language Processing tasks, and is a commonplace practice.\n",
    "<br/><br/>\n",
    "<font size = 4><strong>\n",
    "$$Vec(\"Example\") = [0.02, 0.00, 0.00, 0.92, 0.30,...]$$\n",
    "</font></strong>\n",
    "<br/>\n",
    "Word Embedding tends to group up similarly used words *reasonably* together in the vectorial space. For example, if we use T-SNE (a dimensional reduction visualization algorithm) to flatten the dimensions of our vectors into a 2-dimensional space and use the words these vectors represent as their labels, we might see something like this:\n",
    "\n",
    "<img src=https://ibm.box.com/shared/static/bqhc5dg879gcoabzhxra1w8rkg3od1cu.png width=\"800\"/>\n",
    "<center>*T-SNE Mockup with clusters marked for easier visualization*</center>\n",
    "\n",
    "As you can see, words that are frequently used together, in place of each other, or in the same places as them tend to be grouped together -- being closer together the higher these correlations are. For example, \"None\" is pretty semantically close to \"Zero\", while a phrase that uses \"Italy\" can probably also fit \"Germany\" in it, with little damage to the sentence structure. A vectorial \"closeness\" for similar words like this is a great indicator of a well-built model.\n",
    "\n",
    "---\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to import the necessary modules for our code. We need **`numpy` and `tensorflow`**, obviously. Additionally, we can import directly the **`tensorflow.models.rnn.rnn`** model, which includes the function for building RNNs, and **`tensorflow.models.rnn.ptb.reader`** which is the helper module for getting the input data from the dataset we just downloaded.\n",
    "\n",
    "If you want to learm more take a look at https://www.tensorflow.org/versions/r0.11/api_docs/python/rnn_cell/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  1.1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print('TensorFlow version: ', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local penn_treebank_reader.py...\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('./penn_treebank_reader.py'):\n",
    "    print('Downloading penn_treebank_reader.py...')\n",
    "    !wget -q -O ../data/Penn_Treebank/ptb.zip https://ibm.box.com/shared/static/z2yvmhbskc45xd2a9a4kkn6hg4g4kj5r.zip\n",
    "    !unzip -o ../data/Penn_Treebank/ptb.zip -d ../data/Penn_Treebank\n",
    "    !cp ../data/Penn_Treebank/ptb/reader.py ./penn_treebank_reader.py\n",
    "else:\n",
    "    print('Using local penn_treebank_reader.py...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import penn_treebank_reader as reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM model for Language Modeling\n",
    "Now that we know exactly what we are doing, we can start building our model using TensorFlow. The very first thing we need to do is download and extract the `simple-examples` dataset, which can be done by executing the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('../data/Penn_Treebank/simple_examples.tgz'):\n",
    "    !wget -O ../data/Penn_Treebank/simple_examples.tgz http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz \n",
    "    !tar xzf ../data/Penn_Treebank/simple_examples.tgz -C ../data/Penn_Treebank/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, for the sake of making it easy to play around with the model's hyperparameters, we can declare them beforehand. Feel free to change these -- you will see a difference in performance each time you change those!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initial weight scale\n",
    "init_scale = 0.1\n",
    "#Initial learning rate\n",
    "learning_rate = 1.0\n",
    "#Maximum permissible norm for the gradient (For gradient clipping -- another measure against Exploding Gradients)\n",
    "max_grad_norm = 5\n",
    "#The number of layers in our model\n",
    "num_layers = 2\n",
    "#The total number of recurrence steps, also known as the number of layers when our RNN is \"unfolded\"\n",
    "num_steps = 20\n",
    "#The number of processing units (neurons) in the hidden layers\n",
    "hidden_size = 200\n",
    "#The maximum number of epochs trained with the initial learning rate\n",
    "max_epoch = 4\n",
    "#The total number of epochs in training\n",
    "max_max_epoch = 13\n",
    "#The probability for keeping data in the Dropout Layer (This is an optimization, but is outside our scope for this notebook!)\n",
    "#At 1, we ignore the Dropout Layer wrapping.\n",
    "keep_prob = 1\n",
    "#The decay for the learning rate\n",
    "decay = 0.5\n",
    "#The size for each batch of data\n",
    "batch_size = 30\n",
    "#The size of our vocabulary\n",
    "vocab_size = 10000\n",
    "#Training flag to separate training from testing\n",
    "is_training = 1\n",
    "#Data directory for our dataset\n",
    "data_dir = \"../data/Penn_Treebank/simple-examples/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some clarifications for LSTM architecture based on the argumants:\n",
    "\n",
    "Network structure:\n",
    "- In this network, the number of LSTM cells are 2. To give the model more expressive power, we can add multiple layers of LSTMs to process the data. The output of the first layer will become the input of the second and so on.\n",
    "- The recurrence steps is 20, that is, when our RNN is \"Unfolded\", the recurrence step is 20.   \n",
    "- the structure is like: \n",
    "     - 200 input units -> [200x200] Weight -> 200 Hidden units (first layer) -> [200x200] Weight matrix  -> 200 Hidden units (second layer) ->  [200] weight Matrix -> 200 unit output\n",
    "\n",
    "Hidden layer:\n",
    "- Each LSTM has 200 hidden units which is equivalant to the dimensianality of the embedding words and output. \n",
    "\n",
    "Input layer: \n",
    "- The network has 200 input units. \n",
    "- Suppose each word is represented by an embedding vector of dimensionality e=200. The input layer of each cell will have 200 linear units. These e=200 linear units are connected to each of the h=200 LSTM units in the hidden layer (assuming there is only one hidden layer, though our case has 2 layers).\n",
    "- The input shape is [batch_size, num_steps], that is [30x20]. It will turn into [30x20x200] after embedding, and then 20x[30x200]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot to be done and a ton of information to process at the same time, so go over this code slowly. It may seem complex at first, but if you try to ally what you just learned about language modelling to the code you see, you should be able to understand it.\n",
    "\n",
    "This code is adapted from the [PTBModel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py) example bundled with the TensorFlow source code.\n",
    "\n",
    "\n",
    "#### Train data\n",
    "The story starts from data: \n",
    "- Train data is a list of words, represented by numbers - N=929589 numbers, e.g. [9971, 9972, 9974, 9975,...]\n",
    "- We read data as mini-batch of size b=30. Assume the size of each sentence is 20 words (num_steps = 20). Then it will take int(N/b*h)+1=1548 iterations for the learner to go through all sentences once. So, the number of iterators is 1548\n",
    "- Each batch data is read from train dataset of size 600, and shape of [30x20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start an interactive session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session=tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _ = raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Lets just read one mini-batch now and feed our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "itera = reader.ptb_iterator(train_data, batch_size, num_steps)\n",
    "first_touple=next(itera)\n",
    "x=first_touple[0]\n",
    "y=first_touple[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at 3 sentences of our input x:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605],\n",
       "       [   0, 1071,    4,    0,  185,   24,  368,   20,   31, 3109,  954,\n",
       "          12,    3,   21,    2, 2915,    2,   12,    3,   21]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define 2 place holders to feed them with mini-batchs, that is x and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "_targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets defin a dictionary, and use it later to feed the placeholders with our first mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict={_input_data:x, _targets:y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can use it to feed _input_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9970, 9971, 9972, 9974, 9975, 9976, 9980, 9981, 9982, 9983, 9984,\n",
       "        9986, 9987, 9988, 9989, 9991, 9992, 9993, 9994, 9995],\n",
       "       [2654,    6,  334, 2886,    4,    1,  233,  711,  834,   11,  130,\n",
       "         123,    7,  514,    2,   63,   10,  514,    8,  605],\n",
       "       [   0, 1071,    4,    0,  185,   24,  368,   20,   31, 3109,  954,\n",
       "          12,    3,   21,    2, 2915,    2,   12,    3,   21],\n",
       "       [   3,   71,    4,   27,  246,   60,   11,  215,    4,    1, 1846,\n",
       "           9,    3,   71,  546,    2, 6505,  162,    6,  104],\n",
       "       [  93,   25,    6,  261,  681,  251,    0,  278, 3246,   13,  200,\n",
       "           1,    8,  105, 3360,    1,    4,    0,  536,    4],\n",
       "       [  20,    6,  954,   12,    3,   21,   78,   14,  977,  726,    0,\n",
       "          37,   42,   34,    5,  437,  116,  206,  927,    2],\n",
       "       [  18,  296,    7,  201,   76,    4,  182,  560, 3836,   17,  974,\n",
       "         975,    6,  942,    4,  156,   77, 1570,  288,  644],\n",
       "       [  23, 1238,  899,    5,   25,  201,    4,    0,  434,  642,   55,\n",
       "         201,    4,    0, 2423,    2,    1,    1,    1,  483],\n",
       "       [ 379,  706,    9,  413, 8219,   96,   15,    0, 2185, 1758,    1,\n",
       "           1,   37,   13,  834,    5,  852,  222,    7, 1785],\n",
       "       [   2,  179,  940,  117,   38,   59,  677,   14,    1,   10, 1016,\n",
       "         309,   13, 1077, 6360,   16,   23, 4490,    9,  355],\n",
       "       [3572,    4, 3015, 1347,  536,   13,    6, 3949,    5,  438, 9643,\n",
       "           2,   64,   87,   32,  358, 3672, 4103, 1082,   11],\n",
       "       [  71,  178,    3,    8,    3,    2,    0, 1008,  234,   30, 6400,\n",
       "          10,    0,   98,    9,    1,  338,   13,    5,   25],\n",
       "       [1473,   88,   19, 2578, 6591,    8,  629,  563,    8,  223,  184,\n",
       "         127,   18,    6,  828,    1,    2,    0,  324,  158],\n",
       "       [   1,    1,    2,   18,    0, 1844,    4,   73,   39, 2694,    6,\n",
       "        1709,    2,    7,    0, 6509, 1116,   27,    1,    1],\n",
       "       [1055,    5,   25, 8582,   10,  353,  645,   24,    6,  287,    2,\n",
       "        1006,    0, 8861, 2369,   44,    7,    0,    1,  180],\n",
       "       [  36,  501,    5,    6, 1969,    0,   98,   89, 2254,    0,  312,\n",
       "        1641,    4, 1063,    8,  713,    0,  264,  820,    2],\n",
       "       [  32, 2599,  762, 1875,   26, 1402,   45,  516,    2, 2937,   16,\n",
       "        3355, 2062,  251,    0,  529,   24, 1625,  122,   18],\n",
       "       [ 677,  127,    2,   19,   23, 7800, 3592,   14,   64,   87,   32,\n",
       "         350,    0, 3968,    2,   38,   26,  114,   38,   26],\n",
       "       [  25,   45,  769,    2,   23, 2634, 1096, 1175,   19,    6,    1,\n",
       "         154,   23, 1890,   30,    6,    1,    1,    2,  198],\n",
       "       [7736,  391,    5, 5173,  838,    2,  840,    9, 8716,  537, 4132,\n",
       "        2915,    9,    1,    1,   10, 1268,  175,   32,  184],\n",
       "       [   3,   21,    4,    1,  308,  458,   11,   41,   14, 5718,  102,\n",
       "         824,    1,    2,   14,   59,   50,   12,    3,   21],\n",
       "       [   8,    1,   22,   73,   10,  863,   11,  898,  653,  270,    8,\n",
       "         500,  273, 1559,    2,   14, 3019,    5,  585,   84],\n",
       "       [ 483,  762,   87,  108, 1119,    0,    1,   67,    0, 3296,   26,\n",
       "         591,  174,  127,    2,  108,   26, 9821,   11,    6],\n",
       "       [3885,  582,   81,   17, 1834,    2, 1256,   98,  162,  582,  441,\n",
       "         125,   22, 1652,  172,    4,    3,    3,    8,  206],\n",
       "       [  44,   23,    1,    0, 1704,    4,    1,    2,   22,  373,   38,\n",
       "         275,    1, 8017,    2, 2785, 3659, 4359,   80,  634],\n",
       "       [1896,    8,   13, 9468,   17,  752, 4622,    2,   29, 2221,    0,\n",
       "         446, 3552,    4,    0, 2495,  431,  134,  284,  152],\n",
       "       [  48,    7, 1741,  193,    8,  446,  165,  301, 6521, 5122,   15,\n",
       "          12,    3,   21,    4,   10,  161,  783,    8,   79],\n",
       "       [  47, 4447, 1431,    4, 6967, 2121,   24,  452,   18,   43,    3,\n",
       "          48, 1076,   12,    3,   21,   69,   40,    2, 1323],\n",
       "       [  31, 3374,    4, 2108,    1,  134,    8, 6967, 1825, 3306,   14,\n",
       "          13, 3581,    5, 2424, 1583, 6495,    5,    6, 1136],\n",
       "       [  59, 2070, 2433,   28,  517,   20,   23, 4306,    6,   40,  195,\n",
       "           2, 9398,  400, 4908,  673, 1572,  400,    1, 1173]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_input_data,feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we create the stacked LSTM, which is a 2 layer LSTM network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_lstm = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.BasicLSTMCell(hidden_size, forget_bias=0.0) \n",
    "                                            for _ in range(num_layers)]\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we initialize the states of the nework:\n",
    "\n",
    "#### _initial_state\n",
    "\n",
    "For each LCTM, there are 2 state matrics, c_state and m_state.  c_state and m_state represent \"Memory State\" and \"Cell State\". Each hidden layer, has a vector of size 30, which keeps the states. so, for 200 hidden units in each LSTM, we have a matrix of size [30x200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros:0' shape=(30, 200) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(30, 200) dtype=float32>),\n",
       " LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros:0' shape=(30, 200) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/BasicLSTMCellZeroState_1/zeros_1:0' shape=(30, 200) dtype=float32>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "_initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets look at the states, though they are all zero for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LSTMStateTuple(c=array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), h=array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)),\n",
       " LSTMStateTuple(c=array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32), h=array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(_initial_state,feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "We create the embeddings for our input data. embedding is dictionary of [10000x200] for all 10000 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10000, 200]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_size])  #[10000x200]\n",
    "except ValueError:\n",
    "    pass\n",
    "embedding.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01209606, -0.00610176,  0.0101538 , ...,  0.0226196 ,\n",
       "         0.00843582,  0.02089332],\n",
       "       [-0.00645194,  0.01049244,  0.02086505, ...,  0.00153445,\n",
       "         0.02180625, -0.00136558],\n",
       "       [ 0.00315042,  0.02083927,  0.02305393, ..., -0.01826529,\n",
       "        -0.00958337, -0.01164767],\n",
       "       ..., \n",
       "       [ 0.01118658, -0.02081927,  0.01142085, ...,  0.02072189,\n",
       "        -0.01370281, -0.01318221],\n",
       "       [ 0.01070726,  0.0197452 , -0.01347473, ..., -0.0122654 ,\n",
       "        -0.00089327,  0.01057745],\n",
       "       [ 0.0126599 ,  0.01401849, -0.01375013, ..., -0.01807761,\n",
       "         0.01567172, -0.01887284]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(embedding, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding_lookup goes to each row of input_data, and for each word in the row/sentence, finds the correspond vector in embedding.\n",
    "It creates a [30*20*200] matrix, so, the first elemnt of __inputs__ (the first sentence), is a matrix of 20x200, which each row of it is vector representing a word in the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define where to get the data for our embeddings from\n",
    "inputs = tf.nn.embedding_lookup(embedding, _input_data)  #shape=(30, 20, 200) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'embedding_lookup:0' shape=(30, 20, 200) dtype=float32>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00346966,  0.0180132 , -0.0067113 , ..., -0.02043601,\n",
       "         0.01268665, -0.01841335],\n",
       "       [ 0.01141287, -0.01209113,  0.00185063, ..., -0.00534211,\n",
       "         0.00288254, -0.00211206],\n",
       "       [-0.00873637,  0.01713076,  0.0013929 , ...,  0.00410479,\n",
       "         0.01605235, -0.00925095],\n",
       "       ..., \n",
       "       [ 0.02404756,  0.00544174, -0.02302063, ...,  0.02057038,\n",
       "         0.01565341,  0.00142729],\n",
       "       [-0.01898457, -0.01439425,  0.01303636, ...,  0.01108175,\n",
       "        -0.0069335 ,  0.01917717],\n",
       "       [-0.02178566,  0.00721733, -0.00448469, ..., -0.00281648,\n",
       "        -0.01761998,  0.01708188]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Recurrent Neural Networks\n",
    "__tf.nn.dynamicrnn()__ creates a recurrent neural network using __stacked_lstm__ which is an instance of RNNCell. \n",
    "\n",
    "The input should be a Tensor of shape: [batch_size, max_time, ...], in our case it would be (30, 20, 200)\n",
    "\n",
    "This method, returns a pair (outputs, new_state) where:\n",
    "- outputs is a length T list of outputs (one for each input), or a nested tuple of such elements.\n",
    "- new_state is the final state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs, new_state =  tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=_initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, lets look at the outputs. The output of the stackedLSTM comes from 200 hidden_layer, and in each time step(=20), one of them get activated. we use the linear activation to map the 200 hidden layer to a [?x10 matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'rnn/transpose:0' shape=(30, 20, 200) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.20562829e-04,   6.02060099e-05,   2.73827434e-04, ...,\n",
       "          1.54950991e-04,   2.65133771e-04,   4.74843808e-04],\n",
       "       [  8.81994274e-05,  -1.02157450e-04,   3.84475599e-04, ...,\n",
       "          2.33782237e-04,   4.26791579e-04,   3.77696066e-04],\n",
       "       [  1.94964101e-04,   3.63061205e-04,   3.34488461e-04, ...,\n",
       "          2.31641243e-04,   5.73628291e-04,   2.21637674e-04],\n",
       "       ..., \n",
       "       [  4.45521146e-04,  -1.39987678e-03,  -3.23443557e-04, ...,\n",
       "          2.83046276e-04,  -6.50624046e-04,  -6.72494993e-04],\n",
       "       [  1.38641539e-04,  -1.15363649e-03,  -3.51849681e-04, ...,\n",
       "          6.35754317e-04,  -6.62986364e-04,   1.25905761e-04],\n",
       "       [  1.03857907e-04,  -3.09593888e-04,  -4.67124657e-04, ...,\n",
       "         -1.76438043e-05,  -7.08129141e-04,   6.73512812e-04]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(outputs[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets reshape the output tensor from  [30 x 20 x 200] to [600 x 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(600, 200) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(outputs, [-1, size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.20562829e-04,   6.02060099e-05,   2.73827434e-04,\n",
       "         2.16999479e-06,  -3.37485872e-05,   9.76368319e-05,\n",
       "        -7.20815151e-05,   3.59538681e-04,   9.69108951e-05,\n",
       "        -4.51735919e-04,  -1.10498120e-04,  -2.26888660e-04,\n",
       "        -1.21247562e-04,   4.21536941e-04,   4.02809412e-04,\n",
       "        -4.73341133e-05,   2.61181180e-04,  -1.46595950e-04,\n",
       "         4.20109078e-04,   8.05012300e-04,  -2.60946726e-05,\n",
       "         1.66126789e-04,  -2.78372201e-04,   3.64558015e-04,\n",
       "         2.12733750e-04,  -9.33654446e-05,   3.72408860e-04,\n",
       "        -1.01597579e-04,  -1.61287506e-04,  -2.98267434e-04,\n",
       "         2.69479002e-04,   3.75117015e-05,   8.22367219e-05,\n",
       "        -2.49352306e-04,  -4.92326973e-04,   2.37040411e-04,\n",
       "         2.16158223e-04,  -2.16564767e-05,  -2.29183483e-04,\n",
       "        -6.79773861e-04,  -2.80815468e-04,  -5.21420094e-04,\n",
       "         5.34663341e-05,  -8.46006849e-04,  -2.81927551e-05,\n",
       "         1.32858346e-04,   6.61207287e-06,   9.46656437e-05,\n",
       "         4.75724839e-04,  -2.19036330e-04,   2.14921369e-04,\n",
       "        -3.55625496e-04,   4.85746772e-04,  -3.16515012e-04,\n",
       "        -7.98016990e-05,  -7.77806781e-05,  -5.62109228e-04,\n",
       "        -3.14483012e-04,  -2.04579977e-04,   3.85578722e-04,\n",
       "         1.77653128e-04,  -2.78248917e-04,  -2.04773969e-04,\n",
       "         1.52032317e-05,   1.50767868e-04,  -1.32426168e-04,\n",
       "         2.06605910e-04,   1.53338347e-04,   1.05724335e-04,\n",
       "        -5.58146043e-04,  -1.78084592e-04,  -1.77661583e-04,\n",
       "        -2.26431457e-05,  -4.69228806e-04,  -8.33506856e-05,\n",
       "        -2.91046308e-04,   2.19443988e-04,   2.79427040e-04,\n",
       "         1.97194036e-04,   1.15247363e-04,   2.53688195e-05,\n",
       "        -3.42745683e-04,  -2.90101656e-04,   8.53204328e-05,\n",
       "         1.10675479e-04,   3.31407791e-05,   1.91103463e-05,\n",
       "         4.96569482e-05,   4.49832442e-04,   5.47406788e-04,\n",
       "         6.91933616e-04,   2.81041284e-04,  -2.22474337e-04,\n",
       "         2.48439785e-04,   2.95085338e-04,  -1.10090652e-04,\n",
       "         4.12037858e-04,   9.84593207e-05,   6.32954027e-07,\n",
       "         9.16868667e-05,   1.97269605e-04,   2.60832108e-04,\n",
       "         3.74845877e-05,   9.00534651e-05,   3.78664088e-04,\n",
       "        -2.24651216e-04,   7.46054066e-05,  -2.19594920e-04,\n",
       "         7.10310414e-04,   1.31212990e-04,  -3.77165910e-04,\n",
       "         2.38851106e-04,  -1.40853488e-04,   5.64574555e-04,\n",
       "         7.78881295e-05,   1.41360448e-04,   2.83571126e-06,\n",
       "        -1.63196411e-04,  -5.55582810e-04,  -3.08083865e-04,\n",
       "        -8.87182250e-05,  -6.20935825e-05,   1.46531980e-04,\n",
       "         1.35623821e-04,  -2.91907403e-04,  -9.21082028e-05,\n",
       "        -4.13190392e-05,   1.68245329e-04,  -2.27148237e-04,\n",
       "        -2.51982619e-05,   3.16208927e-04,   2.05992561e-04,\n",
       "        -9.15143246e-05,  -5.22338203e-04,   3.65160231e-04,\n",
       "         2.16590619e-04,   1.42644378e-04,   3.01022119e-05,\n",
       "         5.60859007e-05,   1.71347841e-04,  -1.26923303e-04,\n",
       "         8.21772410e-05,   7.84278905e-04,   4.94246255e-04,\n",
       "        -3.36166035e-04,  -2.46913318e-04,  -1.22437996e-04,\n",
       "        -2.31315527e-04,   4.05458530e-04,   4.34839167e-05,\n",
       "        -4.13256872e-04,  -1.21541525e-04,   1.38487565e-04,\n",
       "        -5.99312589e-05,  -3.58589932e-05,  -1.23114978e-05,\n",
       "        -4.15198127e-04,   3.44390253e-04,   5.16607775e-04,\n",
       "         3.08709743e-04,   3.12184333e-04,  -4.13496455e-04,\n",
       "         2.73876867e-05,  -2.12382118e-04,  -2.35983942e-04,\n",
       "        -1.47426865e-04,   7.16049835e-05,  -5.42793015e-04,\n",
       "        -5.14733023e-04,   5.74991573e-05,   4.13695780e-05,\n",
       "         2.04252050e-04,  -1.80461167e-04,  -9.46455384e-06,\n",
       "        -1.89910366e-04,   2.84890499e-04,  -6.22284497e-05,\n",
       "        -1.06815358e-04,   4.38074130e-05,  -3.75487376e-04,\n",
       "         7.62734635e-05,   3.33969016e-04,   2.42191498e-04,\n",
       "         2.09021702e-04,   2.53796588e-05,  -1.99992901e-05,\n",
       "        -1.73613706e-04,  -1.56874215e-04,   2.79962289e-04,\n",
       "        -3.92320042e-04,  -8.01395508e-05,  -3.45308683e-04,\n",
       "        -2.24729767e-04,  -2.26491669e-04,   1.55433867e-04,\n",
       "         2.23089315e-04,   8.29890414e-05,   1.54950991e-04,\n",
       "         2.65133771e-04,   4.74843808e-04], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(output[0], feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logistic unit\n",
    "Now, we create a logistic unit to return the probability of the output word. That is, mapping the 600\n",
    "\n",
    "Softmax = [600 x 200]* [200 x 1000]+ [1 x 1000] -> [600 x 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size]) #[200x1000]\n",
    "softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "logits = tf.matmul(output, softmax_w) + softmax_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 10000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "logi = session.run(logits, feed_dict)\n",
    "logi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "First_word_output_probablity = logi[0]\n",
    "First_word_output_probablity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "The maximum probablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6212"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_array= session.run(embedding, feed_dict)\n",
    "np.argmax(First_word_output_probablity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what is the ground truth for the first word of first sentence? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9971"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, you can get it from target tensor, if you want to find the embedding vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(30, 20) dtype=int32>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to compare logit with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targ = session.run(tf.reshape(_targets, [-1]), feed_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9971"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word_target_code= targ[0]\n",
    "first_word_target_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.04733184e-03,   2.42270827e-02,  -1.72268525e-02,\n",
       "         1.20069720e-02,  -1.92369372e-02,   2.35040113e-02,\n",
       "         1.19325295e-02,  -3.65709513e-03,   6.30673021e-03,\n",
       "        -1.87882669e-02,   4.45487909e-03,   4.59974073e-03,\n",
       "        -9.57092177e-03,   7.65682012e-03,   1.23129040e-03,\n",
       "         1.18641928e-03,  -1.36597222e-02,  -1.60192177e-02,\n",
       "        -2.32345592e-02,   6.34717755e-03,   1.94436572e-02,\n",
       "        -8.06851871e-03,   2.55010463e-03,   8.79669562e-03,\n",
       "        -1.95105486e-02,   1.68616176e-02,   2.16872953e-02,\n",
       "         1.09831989e-02,  -2.17179991e-02,   8.18478316e-03,\n",
       "         8.41740519e-04,   1.16804801e-03,  -9.29802656e-04,\n",
       "         3.97964939e-03,   1.46834441e-02,  -1.84606202e-02,\n",
       "         1.63465701e-02,   1.12980455e-02,  -6.35626912e-03,\n",
       "         1.32077560e-02,  -2.02165917e-02,  -1.04783345e-02,\n",
       "         3.29232961e-03,   9.09314305e-03,   3.32598388e-03,\n",
       "         1.09447390e-02,  -1.85148045e-03,  -1.80765912e-02,\n",
       "         5.67870215e-04,   1.42210126e-02,   4.04428691e-03,\n",
       "         3.17817740e-03,   1.88552924e-02,   9.40334052e-03,\n",
       "        -1.44602684e-02,   2.29660496e-02,   1.85355544e-02,\n",
       "        -1.84890926e-02,   2.30777822e-03,   1.17346570e-02,\n",
       "         1.47714131e-02,  -1.12818535e-02,  -1.84135325e-02,\n",
       "         6.33428991e-03,  -1.26117365e-02,  -1.53896669e-02,\n",
       "        -2.21581869e-02,  -1.13660991e-02,  -2.12191790e-03,\n",
       "         1.78920589e-02,  -1.79871358e-02,   1.07330419e-02,\n",
       "         4.19841893e-03,  -7.62545690e-03,   2.32332684e-02,\n",
       "         5.91765344e-03,   1.32474750e-02,   7.89166242e-03,\n",
       "        -1.93813816e-03,  -5.89379482e-03,  -1.76311322e-02,\n",
       "        -1.81506891e-02,   1.70686804e-02,   1.57786570e-02,\n",
       "         3.57769430e-04,  -1.55545836e-02,  -1.96538325e-02,\n",
       "        -1.40840299e-02,  -2.37362590e-02,  -3.94169800e-03,\n",
       "         3.84494662e-03,  -2.12112274e-02,  -9.60841589e-03,\n",
       "        -1.85838845e-02,   1.26916952e-02,  -9.62446816e-03,\n",
       "        -1.02567030e-02,  -1.95426811e-02,  -7.35466368e-03,\n",
       "         9.34616849e-03,   9.41869244e-03,   1.43930763e-02,\n",
       "         3.17574851e-03,  -1.10670459e-02,   1.40415058e-02,\n",
       "         1.65748969e-03,  -2.39923093e-02,  -8.78809299e-03,\n",
       "        -1.98645182e-02,   9.23290104e-03,  -4.45857830e-03,\n",
       "        -1.98066924e-02,  -7.87263736e-04,  -2.15746053e-02,\n",
       "         2.42599659e-03,   1.89184956e-02,   7.48199224e-03,\n",
       "         1.58509836e-02,   1.64753608e-02,   8.10435042e-04,\n",
       "         1.64675117e-02,  -1.92528907e-02,  -2.22958066e-03,\n",
       "        -6.19266368e-03,   1.28916688e-02,  -1.17929466e-03,\n",
       "        -1.19578913e-02,   1.66556463e-02,  -4.93622571e-03,\n",
       "         1.43524520e-02,   6.59996085e-03,   1.98040158e-02,\n",
       "         1.72386318e-02,  -4.62767668e-03,   2.08887085e-02,\n",
       "         1.61281154e-02,  -5.00751100e-03,  -3.73673253e-03,\n",
       "         1.73324980e-02,  -4.04841453e-03,  -1.31510701e-02,\n",
       "        -2.31439807e-02,   7.39731640e-03,  -1.41780134e-02,\n",
       "         1.59540214e-02,   7.73763284e-03,  -1.42910089e-02,\n",
       "         1.87940896e-02,   2.60119885e-03,   6.40035421e-03,\n",
       "         1.24752931e-02,   1.69389211e-02,   3.20627540e-03,\n",
       "         1.75408274e-02,   2.10497528e-03,   7.14695454e-03,\n",
       "         2.17209421e-02,   1.09754577e-02,   2.00112164e-03,\n",
       "         2.13144571e-02,   6.05482049e-03,  -1.72326993e-02,\n",
       "        -2.36124210e-02,  -1.62378326e-03,   1.56494044e-03,\n",
       "         2.33907849e-02,   1.70802884e-03,   5.68195432e-03,\n",
       "         7.70640001e-03,  -2.18711495e-02,  -1.19827101e-02,\n",
       "         2.28087455e-02,   2.23006457e-02,  -2.00082697e-02,\n",
       "        -1.14337886e-02,  -1.98250115e-02,   1.17598474e-02,\n",
       "         1.19432658e-02,  -5.40530682e-03,  -1.06516657e-02,\n",
       "         2.36079618e-02,   4.55011614e-03,   7.56038725e-03,\n",
       "         2.27345116e-02,  -9.51826107e-03,   1.25247836e-02,\n",
       "        -1.11990776e-02,   1.42791681e-02,  -1.48644708e-02,\n",
       "         1.74560137e-02,   1.80639327e-05,   1.23802759e-02,\n",
       "         1.85266361e-02,   7.59956986e-03,  -2.31570676e-02,\n",
       "         1.15530081e-02,  -1.83208566e-02,   1.71539076e-02,\n",
       "        -2.66799331e-03,   1.04738027e-02], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_word_target_vec = session.run( tf.nn.embedding_lookup(embedding, targ[0]))\n",
    "first_word_target_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective function\n",
    "\n",
    "Now we want to define our objective function. Our objective is to minimize loss function, that is, to minimize the average negative log probability of the target words:\n",
    "\n",
    "loss=−1N∑i=1Nln⁡ptargeti  \n",
    "This function is already implimented and available in TensorFlow through __sequence_loss_by_example__ so we can just use it here. __sequence_loss_by_example__ is weighted cross-entropy loss for a sequence of logits (per example).  \n",
    "\n",
    "Its arguments:  \n",
    "\n",
    "logits: List of 2D Tensors of shape [batch_size x num_decoder_symbols].  \n",
    "targets: List of 1D batch-sized int32 Tensors of the same length as logits.  \n",
    "weights: List of 1D batch-sized float-Tensors of the same length as logits.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(_targets, [-1])],[tf.ones([batch_size * num_steps])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss is a 1D batch-sized float Tensor [600x1]: The log-perplexity for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.19458389,  9.2089529 ,  9.20383358,  9.19668484,  9.22284603,\n",
       "        9.22323704,  9.21939754,  9.20577526,  9.21760178,  9.20278835,\n",
       "        9.22381783,  9.22282028,  9.21599388,  9.20778465,  9.22204113,\n",
       "        9.20389938,  9.20590591,  9.19645882,  9.20643234,  9.22712135,\n",
       "        9.22742367,  9.20594978,  9.21986485,  9.22117615,  9.21954632,\n",
       "        9.22490978,  9.22328377,  9.2225771 ,  9.20987511,  9.21405888,\n",
       "        9.22658634,  9.22763443,  9.19816017,  9.20090866,  9.21199799,\n",
       "        9.19970322,  9.19820786,  9.20146179,  9.19972229,  9.21308231,\n",
       "        9.20467091,  9.22111702,  9.21110916,  9.22421551,  9.20310879,\n",
       "        9.21938229,  9.1954031 ,  9.21675587,  9.21378136,  9.19936275,\n",
       "        9.1998291 ,  9.21116543,  9.21734905,  9.20096397,  9.20203018,\n",
       "        9.20098114,  9.19969749,  9.21083355,  9.21749401,  9.22101593,\n",
       "        9.22098637,  9.22110558,  9.22119617,  9.22270203,  9.20559311,\n",
       "        9.20961571,  9.21391582,  9.22113323,  9.2199192 ,  9.22478867,\n",
       "        9.21913624,  9.2109623 ,  9.2210474 ,  9.21877098,  9.20079231,\n",
       "        9.21832752,  9.20171642,  9.22754669,  9.20522022,  9.21554279,\n",
       "        9.21104431,  9.22733402,  9.22569275,  9.19489384,  9.19589424,\n",
       "        9.21127987,  9.20207214,  9.2006731 ,  9.208004  ,  9.22615814,\n",
       "        9.21981812,  9.20137501,  9.22488117,  9.21454239,  9.21994209,\n",
       "        9.22130394,  9.21116734,  9.21119118,  9.22112751,  9.21569633,\n",
       "        9.22737598,  9.19950676,  9.19992161,  9.2112093 ,  9.21733189,\n",
       "        9.20236588,  9.22301102,  9.19443226,  9.22005653,  9.21138   ,\n",
       "        9.20467758,  9.20273495,  9.19869518,  9.19419193,  9.20052814,\n",
       "        9.21327686,  9.19950294,  9.19834042,  9.20095158,  9.2273922 ,\n",
       "        9.19392204,  9.22753239,  9.20440102,  9.22753334,  9.2211132 ,\n",
       "        9.21653461,  9.1979475 ,  9.22450924,  9.22526264,  9.22766304,\n",
       "        9.19617176,  9.22761059,  9.19727325,  9.22123814,  9.21100616,\n",
       "        9.19370556,  9.19816208,  9.21841335,  9.21506977,  9.20081615,\n",
       "        9.21863365,  9.20991325,  9.19420433,  9.21111298,  9.20411873,\n",
       "        9.22114468,  9.21126842,  9.19810581,  9.20414066,  9.20186138,\n",
       "        9.20437527,  9.22117043,  9.21112728,  9.20907402,  9.20090771,\n",
       "        9.21966743,  9.21972656,  9.21978283,  9.20385361,  9.20077324,\n",
       "        9.20591545,  9.21946621,  9.22204685,  9.21192741,  9.21075249,\n",
       "        9.20801163,  9.21129417,  9.19966888,  9.20381737,  9.21988487,\n",
       "        9.21987534,  9.20452023,  9.20790005,  9.22300339,  9.19431114,\n",
       "        9.20105648,  9.21591949,  9.22749424,  9.21219444,  9.21115208,\n",
       "        9.20654011,  9.19545841,  9.19681454,  9.22633362,  9.21445179,\n",
       "        9.21032143,  9.22298527,  9.21982574,  9.19972134,  9.21599293,\n",
       "        9.22113037,  9.20797348,  9.22168827,  9.2048893 ,  9.21552467,\n",
       "        9.22714138,  9.2218008 ,  9.21912098,  9.20434761,  9.22331238,\n",
       "        9.22114182,  9.20737267,  9.21666145,  9.21134472,  9.20783615,\n",
       "        9.22748089,  9.20197964,  9.19447041,  9.22549629,  9.20953274,\n",
       "        9.20096207,  9.21709728,  9.22320175,  9.22397709,  9.20388412,\n",
       "        9.21893406,  9.20245552,  9.22492313,  9.20961189,  9.20227337,\n",
       "        9.19684219,  9.21102142,  9.20148373,  9.21116924,  9.20074368,\n",
       "        9.21125221,  9.20865154,  9.20254421,  9.2193737 ,  9.19433498,\n",
       "        9.199646  ,  9.21136189,  9.22292519,  9.2192812 ,  9.21978569,\n",
       "        9.19951248,  9.20790768,  9.19425964,  9.21094322,  9.19634914,\n",
       "        9.20347786,  9.2230587 ,  9.19774914,  9.22729874,  9.20138931,\n",
       "        9.21223164,  9.21168423,  9.20146084,  9.19462395,  9.22378826,\n",
       "        9.20726776,  9.21819401,  9.22748756,  9.20797443,  9.21981335,\n",
       "        9.20075035,  9.2112112 ,  9.220438  ,  9.20470715,  9.2099123 ,\n",
       "        9.21984005,  9.20079231,  9.21824169,  9.21127224,  9.2018652 ,\n",
       "        9.22103119,  9.21762943,  9.20031834,  9.21535397,  9.22747803,\n",
       "        9.21204948,  9.20080948,  9.22737598,  9.21131802,  9.21308231,\n",
       "        9.21011162,  9.2211771 ,  9.21980572,  9.21984196,  9.21987057,\n",
       "        9.19437695,  9.21106243,  9.20569229,  9.19958305,  9.2156992 ,\n",
       "        9.21695805,  9.20302391,  9.22742748,  9.22074413,  9.20061016,\n",
       "        9.21821308,  9.21132565,  9.21764469,  9.2059803 ,  9.20974636,\n",
       "        9.22735119,  9.21116543,  9.21956253,  9.20647049,  9.22497654,\n",
       "        9.2258997 ,  9.19438744,  9.22740841,  9.20066738,  9.21138954,\n",
       "        9.22252941,  9.20004082,  9.20952415,  9.21138573,  9.19791603,\n",
       "        9.20509911,  9.22105217,  9.21754646,  9.2014904 ,  9.21990681,\n",
       "        9.21115017,  9.2177515 ,  9.19574833,  9.20079613,  9.2092762 ,\n",
       "        9.19927216,  9.19900513,  9.20588589,  9.20179272,  9.21667957,\n",
       "        9.21843529,  9.22429371,  9.20075321,  9.21068382,  9.21548748,\n",
       "        9.21729469,  9.19919777,  9.19610882,  9.21128368,  9.21965313,\n",
       "        9.2031002 ,  9.2035017 ,  9.21651554,  9.21831131,  9.227458  ,\n",
       "        9.20740509,  9.20080376,  9.22315216,  9.2273674 ,  9.21475792,\n",
       "        9.19741154,  9.22296238,  9.21710491,  9.22320557,  9.22393322,\n",
       "        9.21645927,  9.21145725,  9.21008778,  9.20095062,  9.22626877,\n",
       "        9.20166397,  9.21135616,  9.22608662,  9.20170784,  9.20081139,\n",
       "        9.21840858,  9.20709515,  9.20089149,  9.22719669,  9.22254848,\n",
       "        9.20324421,  9.21870422,  9.2230835 ,  9.22730827,  9.21991253,\n",
       "        9.21422195,  9.22727203,  9.19752407,  9.21949959,  9.22747517,\n",
       "        9.21991062,  9.21989536,  9.20076084,  9.20073509,  9.2113409 ,\n",
       "        9.20113564,  9.19432163,  9.20891666,  9.21350288,  9.20084   ,\n",
       "        9.21007347,  9.2191906 ,  9.21560669,  9.20106316,  9.21588326,\n",
       "        9.20192719,  9.21925735,  9.21998119,  9.21994114,  9.19972229,\n",
       "        9.21760559,  9.22337914,  9.22407722,  9.22388172,  9.20790291,\n",
       "        9.21752453,  9.22104645,  9.21974945,  9.20085621,  9.20235538,\n",
       "        9.20986652,  9.20108223,  9.22308922,  9.19910717,  9.2194128 ,\n",
       "        9.21508884,  9.21980572,  9.20075417,  9.22312927,  9.21472263,\n",
       "        9.21128368,  9.19995022,  9.21108723,  9.21754837,  9.22106266,\n",
       "        9.21986485,  9.19826889,  9.21760941,  9.19963837,  9.20437813,\n",
       "        9.20980835,  9.22235584,  9.22615528,  9.22142029,  9.20148659,\n",
       "        9.22271156,  9.20929909,  9.21351624,  9.20072651,  9.22295761,\n",
       "        9.19428921,  9.19420052,  9.21593189,  9.21690178,  9.2121315 ,\n",
       "        9.19893456,  9.22327232,  9.21247482,  9.20000935,  9.21142101,\n",
       "        9.21987724,  9.20199776,  9.21113968,  9.22594452,  9.20161819,\n",
       "        9.1985321 ,  9.22123528,  9.20746136,  9.20076847,  9.21265411,\n",
       "        9.20178795,  9.22009659,  9.20976925,  9.22741032,  9.20747662,\n",
       "        9.19977188,  9.21157169,  9.22504044,  9.22412491,  9.20084953,\n",
       "        9.20026493,  9.22308254,  9.20196342,  9.19987202,  9.1999445 ,\n",
       "        9.21762466,  9.19832897,  9.22541809,  9.19734097,  9.22139645,\n",
       "        9.21106052,  9.21109009,  9.20158291,  9.19936466,  9.21347427,\n",
       "        9.2272768 ,  9.21982956,  9.21111012,  9.21249771,  9.22117424,\n",
       "        9.21978569,  9.20088863,  9.19834423,  9.21186924,  9.2262373 ,\n",
       "        9.20971394,  9.21965694,  9.21355724,  9.20091343,  9.19662571,\n",
       "        9.20784283,  9.20161533,  9.2254715 ,  9.19615555,  9.22132111,\n",
       "        9.20149899,  9.20795727,  9.19764042,  9.22518635,  9.2171011 ,\n",
       "        9.20395756,  9.20065117,  9.21604252,  9.20906258,  9.21125984,\n",
       "        9.19700432,  9.21499825,  9.22100449,  9.21113586,  9.19649696,\n",
       "        9.20185184,  9.22298336,  9.19973564,  9.21071339,  9.2095623 ,\n",
       "        9.22750092,  9.19964314,  9.19707584,  9.20141506,  9.19692802,\n",
       "        9.20329094,  9.20371056,  9.19642067,  9.20926285,  9.20799541,\n",
       "        9.19984722,  9.21098423,  9.217556  ,  9.22114563,  9.19976425,\n",
       "        9.22694302,  9.2112112 ,  9.20154858,  9.22711849,  9.21098423,\n",
       "        9.21725368,  9.21111679,  9.22110939,  9.22000599,  9.20437622,\n",
       "        9.20296001,  9.19855595,  9.218297  ,  9.20977974,  9.2110548 ,\n",
       "        9.2130022 ,  9.19345093,  9.19995689,  9.21098328,  9.21745396,\n",
       "        9.20705128,  9.22454929,  9.20100498,  9.22185516,  9.21631145,\n",
       "        9.19514561,  9.22129536,  9.21392536,  9.21971893,  9.22305298,\n",
       "        9.20132446,  9.21998119,  9.22605991,  9.20123482,  9.22311687,\n",
       "        9.2078371 ,  9.20657825,  9.19441795,  9.20897961,  9.21051311,\n",
       "        9.21751881,  9.19436646,  9.22737694,  9.21564865,  9.22446728,\n",
       "        9.19634533,  9.2207222 ,  9.21838093,  9.21696663,  9.19512463,\n",
       "        9.22713089,  9.21349049,  9.22736835,  9.22453403,  9.19775486,\n",
       "        9.20064545,  9.22101784,  9.20186234,  9.21731853,  9.20454502,\n",
       "        9.21609116,  9.20199585,  9.21988869,  9.22114658,  9.21051216], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(loss, feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.21652"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(cost, feed_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets store the new state as final state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# \n",
    "final_state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To do gradient clipping in TensorFlow we have to take the following steps:\n",
    "\n",
    "1. Define the optimizer.\n",
    "2. Extract variables that are trainable.\n",
    "3. Calculate the gradients based on the loss function.\n",
    "4. Apply the optimizer to the variables / gradients tuple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define Optimizer\n",
    "\n",
    "__GradientDescentOptimizer__ constructs a new gradient descent optimizer. Later, we use constructed __optimizer__ to compute gradients for a loss and apply gradients to variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a variable for the learning rate\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "# Create the gradient descent optimizer with our learning rate\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2. Trainable Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definining a variable, if you passed _trainable=True_, the _Variable()_ constructor automatically adds new variables to the graph collection __GraphKeys.TRAINABLE_VARIABLES__. Now, using _tf.trainable_variables()_ you can get all variables created with __trainable=True__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding:0' shape=(10000, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0' shape=(400, 800) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases:0' shape=(800,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0' shape=(400, 800) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0' shape=(800,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_w:0' shape=(200, 10000) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_b:0' shape=(10000,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can find the name and scope of all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvars=tvars[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0',\n",
       " 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0',\n",
       " 'softmax_w:0',\n",
       " 'softmax_b:0']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tvars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Calculate the gradients based on the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'truediv:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights:0' shape=(400, 800) dtype=float32_ref>,\n",
       " <tf.Variable 'rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases:0' shape=(800,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_w:0' shape=(200, 10000) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax_b:0' shape=(10000,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient:\n",
    "The gradient of a function (line) is the slope of the line, or the rate of change of a function. It's a vector (a direction to move) that points in the direction of greatest increase of the function, and calculated by __derivative__ operation.\n",
    "\n",
    "First lets recall the gradient function using an toy example:\n",
    "$$ z=\\left(2x^2+3xy\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_x = tf.placeholder(tf.float32)\n",
    "var_y = tf.placeholder(tf.float32) \n",
    "func_test = 2.0*var_x*var_x + 3.0*var_x*var_y\n",
    "session.run(tf.global_variables_initializer())\n",
    "feed={var_x:1.0,var_y:2.0}\n",
    "session.run(func_test, feed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __tf.gradients()__ function allows you to compute the symbolic gradient of one tensor with respect to one or more other tensors—including variables. __tf.gradients(func,xs)__ constructs symbolic partial derivatives of sum of __func__ w.r.t. _x_ in __xs__. \n",
    "\n",
    "Now, lets look at the derivitive w.r.t. __var_x__:\n",
    "$$ \\frac{\\partial \\:}{\\partial \\:x}\\left(2x^2+3xy\\right)=4x+3y $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_x])\n",
    "session.run(var_grad,feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the derivitive w.r.t. __var_y__:\n",
    "$$ \\frac{\\partial \\:}{\\partial \\:x}\\left(2x^2+3xy\\right)=3x $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_grad = tf.gradients(func_test, [var_y])\n",
    "session.run(var_grad,feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can look at gradients w.r.t all variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'gradients_2/rnn/while/multi_rnn_cell/cell_1/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/b_acc_3:0' shape=(400, 800) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/rnn/while/multi_rnn_cell/cell_1/basic_lstm_cell/basic_lstm_cell_1/BiasAdd/Enter_grad/b_acc_3:0' shape=(800,) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/MatMul_grad/MatMul_1:0' shape=(200, 10000) dtype=float32>,\n",
       " <tf.Tensor 'gradients_2/add_grad/Reshape_1:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gradients(cost, tvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grad_t_list = tf.gradients(cost, tvars)\n",
    "#sess.run(grad_t_list,feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "now, we have a list of tensors, t-list. We can use it to find clipped tensors. __clip_by_global_norm__ clips values of multiple tensors by the ratio of the sum of their norms.\n",
    "\n",
    "__clip_by_global_norm__ get _t-list_ as input and returns 2 things:\n",
    " - a list of clipped tensors, so called _list_clipped_ \n",
    " - the global norm (global_norm) of all tensors in t_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_0:0' shape=(400, 800) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_1:0' shape=(800,) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_2:0' shape=(200, 10000) dtype=float32>,\n",
       " <tf.Tensor 'clip_by_global_norm/clip_by_global_norm/_3:0' shape=(10000,) dtype=float32>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the gradient clipping threshold\n",
    "grads, _ = tf.clip_by_global_norm(grad_t_list, max_grad_norm)\n",
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[  4.00099776e-09,   2.60287625e-10,  -8.35133740e-09, ...,\n",
       "          -6.60036426e-09,  -1.29708000e-09,   2.41258902e-09],\n",
       "        [  3.20189830e-09,   4.92644869e-10,   6.87790003e-09, ...,\n",
       "          -2.46679965e-09,  -3.85124732e-09,   2.13621587e-09],\n",
       "        [ -1.34912259e-09,  -8.43564596e-09,  -1.68764220e-08, ...,\n",
       "           1.46968970e-08,   1.26722366e-08,  -1.04613935e-08],\n",
       "        ..., \n",
       "        [  1.67160441e-09,  -3.30584032e-10,  -8.06503420e-10, ...,\n",
       "           1.06465237e-09,  -2.67157629e-09,   2.57879551e-11],\n",
       "        [ -1.00444875e-09,   1.27610078e-09,   1.13445708e-09, ...,\n",
       "           1.78999804e-09,   1.98703298e-09,  -1.87881954e-09],\n",
       "        [  3.00192149e-10,   1.10290344e-09,   9.15292120e-10, ...,\n",
       "          -4.65849359e-09,  -4.55704141e-10,   1.35898803e-09]], dtype=float32),\n",
       " array([ -1.84668181e-06,  -4.37627887e-06,  -8.41022540e-07,\n",
       "         -6.67336622e-07,   5.15306374e-06,  -5.64383890e-06,\n",
       "         -1.77421919e-06,  -4.13259954e-07,  -2.40099848e-06,\n",
       "          4.33005607e-06,  -2.55442330e-07,   8.24100425e-07,\n",
       "         -2.57666841e-07,  -1.23878658e-06,  -1.01122278e-05,\n",
       "         -3.88686067e-06,  -2.76105357e-06,   4.34212416e-06,\n",
       "          9.60091347e-06,   1.43859575e-06,   1.60447371e-07,\n",
       "          6.61924423e-06,   3.52451821e-06,  -1.60276088e-07,\n",
       "         -6.25878829e-06,   1.66799055e-06,  -4.46641616e-06,\n",
       "          5.74917749e-06,   3.35982122e-06,  -3.11368524e-07,\n",
       "          2.20775121e-07,  -6.68640610e-07,  -6.53258439e-06,\n",
       "         -8.97189966e-06,   7.11727921e-08,  -4.34961703e-06,\n",
       "          2.16178319e-06,   1.50417037e-07,  -2.73584919e-06,\n",
       "         -1.92734274e-06,   4.49492245e-06,   7.35979825e-07,\n",
       "          7.21473634e-06,   2.48828815e-06,  -7.48625439e-07,\n",
       "         -3.07980770e-07,  -4.55125064e-06,   1.23109783e-06,\n",
       "          1.00935677e-06,  -1.47362061e-06,  -6.92461708e-06,\n",
       "         -1.34661161e-06,   2.44401917e-06,   6.60790738e-06,\n",
       "         -7.04268814e-06,   7.08844823e-08,  -2.94240590e-06,\n",
       "          3.07755818e-06,   3.38295877e-06,  -4.02520027e-06,\n",
       "          6.77633307e-06,   7.57129328e-06,  -6.07585935e-06,\n",
       "         -3.38467771e-06,   3.76412095e-06,  -3.42629755e-06,\n",
       "          6.74344847e-06,   1.17129002e-06,   4.46745980e-06,\n",
       "          1.74953016e-06,  -3.44956516e-06,   3.98761017e-07,\n",
       "          1.05332367e-06,   2.46580385e-06,  -3.74742194e-06,\n",
       "          1.61796697e-06,   1.78213349e-06,  -4.60164529e-08,\n",
       "         -9.75810281e-07,  -1.93253527e-06,  -6.20937726e-06,\n",
       "         -2.63292304e-06,  -2.23288203e-06,  -5.71224427e-06,\n",
       "          2.05662832e-06,   5.71483770e-06,  -5.63322465e-06,\n",
       "         -6.71076748e-07,   2.11766246e-06,  -1.83352688e-06,\n",
       "         -4.60738374e-06,   5.05054595e-06,   1.92784046e-06,\n",
       "         -6.57880810e-06,  -4.35273796e-06,  -2.03403306e-06,\n",
       "         -4.10863368e-06,   2.93826292e-06,  -2.63541779e-06,\n",
       "          6.78358401e-06,   2.33050741e-06,   1.33721733e-06,\n",
       "         -5.43706960e-07,  -1.67486621e-06,   4.51848837e-06,\n",
       "          3.40977340e-06,  -5.36467769e-06,   7.99423219e-07,\n",
       "          1.28243801e-06,   1.91204390e-06,  -5.38089762e-06,\n",
       "          3.07175696e-06,  -4.25328972e-06,   1.12373300e-07,\n",
       "         -1.23377140e-06,   2.75867205e-06,   8.58213753e-07,\n",
       "         -1.71298734e-06,   5.02376452e-07,   3.63618210e-06,\n",
       "          1.99879469e-06,   4.49312392e-06,   1.19777371e-06,\n",
       "         -2.12215673e-06,   1.59101819e-06,  -7.99044358e-07,\n",
       "         -1.49706409e-07,  -6.94020287e-07,   2.64874689e-06,\n",
       "         -2.12399686e-06,   8.68947893e-07,   3.78903883e-06,\n",
       "         -3.55185648e-06,   1.37856762e-06,  -2.79245933e-06,\n",
       "          2.41755038e-06,  -1.71620832e-06,  -4.25964481e-06,\n",
       "          2.30731985e-06,  -2.24292512e-06,  -5.78088839e-06,\n",
       "         -4.81642655e-06,   3.30055241e-06,   1.98995076e-06,\n",
       "         -2.96640474e-06,   6.93280754e-06,   1.18373327e-06,\n",
       "          1.80784093e-06,   3.30375826e-07,   5.78247318e-06,\n",
       "         -6.33656964e-06,  -7.83064593e-07,  -5.67991765e-06,\n",
       "         -1.64189748e-06,   3.18204047e-06,  -1.18811831e-05,\n",
       "         -9.73678198e-07,   8.40580651e-06,   1.57763930e-06,\n",
       "          4.55865211e-06,  -5.07079449e-06,   3.11919075e-06,\n",
       "          5.90031732e-06,  -4.84539305e-06,  -1.88678132e-06,\n",
       "          1.30139426e-07,   1.39686085e-06,  -7.66669132e-07,\n",
       "          4.69039151e-06,   2.23360473e-07,   4.06182608e-06,\n",
       "          4.79503342e-06,   1.85320380e-06,   5.80608230e-06,\n",
       "          5.41319923e-06,   2.40551481e-06,   3.39464532e-06,\n",
       "          2.23047346e-06,   7.87277168e-06,  -9.64012315e-07,\n",
       "          1.46052218e-06,  -3.00866850e-06,   1.33353535e-06,\n",
       "         -5.26215160e-09,   4.70121813e-06,   2.31090962e-06,\n",
       "         -4.18055788e-06,  -5.40023893e-06,  -8.68692314e-06,\n",
       "         -1.80959580e-06,   3.52524921e-06,   8.65294805e-06,\n",
       "         -3.02447370e-06,   2.84666248e-06,  -2.66396540e-07,\n",
       "         -1.02408676e-05,   2.45973388e-06,   2.99324881e-07,\n",
       "          5.04938407e-06,   2.55789200e-06,   5.32771926e-03,\n",
       "         -1.05977915e-02,   1.34620061e-02,   5.43348142e-05,\n",
       "         -1.79872122e-02,  -7.77648296e-03,   1.08451294e-02,\n",
       "          4.50828066e-03,  -2.04311498e-03,  -1.39657520e-02,\n",
       "          1.63033307e-02,   1.14634205e-02,   9.79490392e-03,\n",
       "          2.33906624e-03,  -1.75846517e-02,  -1.27230666e-03,\n",
       "          1.62696349e-04,  -9.98058706e-04,  -2.65897438e-02,\n",
       "         -7.02808797e-03,  -1.30858913e-03,  -1.36459572e-03,\n",
       "          8.82193074e-03,   2.51005613e-03,  -3.19930688e-02,\n",
       "          1.68680120e-02,  -1.70753226e-02,   2.03465056e-02,\n",
       "          1.38668474e-02,  -9.46434494e-03,  -6.88911602e-03,\n",
       "         -2.40008496e-02,   1.14282658e-02,  -2.33347118e-02,\n",
       "         -3.36803757e-02,  -1.45555045e-02,   3.54932388e-03,\n",
       "         -9.44022788e-04,   3.28249447e-02,   3.22579145e-02,\n",
       "          1.11919176e-02,   5.81473392e-03,   4.61832667e-03,\n",
       "          3.98152471e-02,  -1.79459399e-03,  -2.39799637e-03,\n",
       "          1.03814462e-02,   4.33394965e-03,   4.57972381e-03,\n",
       "         -2.82727294e-02,   9.11009870e-03,   6.11011731e-03,\n",
       "          1.52669232e-02,   2.67628338e-02,   5.88271627e-03,\n",
       "         -1.74864251e-02,  -3.01669235e-04,  -2.52158903e-02,\n",
       "         -6.02432806e-03,   1.67607982e-02,   1.41731426e-02,\n",
       "          1.97503828e-02,  -1.14375260e-03,  -7.37042632e-03,\n",
       "          1.90856643e-02,  -1.87157486e-02,   2.31351964e-02,\n",
       "          9.92621109e-03,  -2.02966807e-03,  -6.94499258e-03,\n",
       "         -1.06670498e-03,  -7.03459419e-03,   1.43145933e-03,\n",
       "         -1.30834812e-02,   2.09667645e-02,  -7.95019045e-03,\n",
       "          4.53776382e-02,  -2.81512141e-02,   2.41465177e-02,\n",
       "         -7.29434751e-03,  -1.73310228e-02,   3.98810580e-02,\n",
       "          1.16871474e-02,  -7.02588167e-03,  -9.41040274e-03,\n",
       "          1.78904980e-02,  -1.65349115e-02,   2.44801342e-02,\n",
       "          1.17414081e-02,   2.75169704e-02,  -1.01540731e-02,\n",
       "         -6.26183022e-03,   7.54366163e-03,  -1.49270352e-02,\n",
       "         -6.00740919e-03,   1.45495031e-02,  -1.02086319e-02,\n",
       "         -3.57986148e-03,   2.59967148e-02,   4.55198959e-02,\n",
       "          2.30842624e-02,  -6.05245400e-03,  -9.63537197e-04,\n",
       "          2.97665643e-03,  -2.52353530e-02,  -4.22212109e-03,\n",
       "         -2.77755447e-02,   1.07303532e-02,   1.18234763e-02,\n",
       "         -1.61137059e-02,  -4.15841956e-03,  -1.65899675e-02,\n",
       "          1.04536209e-02,   1.28840115e-02,  -1.57269109e-02,\n",
       "         -1.61961801e-02,   4.40815743e-03,   7.43403041e-04,\n",
       "         -9.36788332e-04,  -2.16678064e-02,   1.73947774e-06,\n",
       "         -7.18424376e-03,  -2.95226946e-02,   2.18251199e-02,\n",
       "         -8.49888753e-03,  -3.84345055e-02,   1.61896106e-02,\n",
       "         -4.08721110e-03,  -5.45431022e-03,  -6.31324667e-03,\n",
       "         -2.23100781e-02,  -9.59886052e-03,  -8.65585078e-03,\n",
       "          5.23895072e-03,  -3.28668356e-02,   7.59766623e-03,\n",
       "         -7.01414049e-03,   1.82457194e-02,   2.76305992e-03,\n",
       "          1.14256730e-02,   7.39618717e-03,   5.69781102e-03,\n",
       "          3.25123183e-02,   2.17724647e-02,   3.19745764e-03,\n",
       "          1.85185892e-03,   6.91895979e-03,  -6.64077001e-03,\n",
       "          7.69138522e-03,   5.43854944e-03,   7.22485268e-03,\n",
       "          1.58468988e-02,   3.89818065e-02,  -3.00260950e-02,\n",
       "         -4.73650172e-03,  -1.07163992e-02,   2.52307463e-03,\n",
       "         -3.24204168e-03,  -8.47247057e-03,  -4.05535195e-03,\n",
       "          4.45776014e-03,   2.21380708e-03,   6.41876971e-03,\n",
       "         -2.87538823e-02,   1.45886522e-02,   1.20918127e-02,\n",
       "         -8.49836506e-03,   5.14674350e-04,   6.30940031e-03,\n",
       "          2.33987626e-02,  -2.53915694e-03,   8.16639699e-03,\n",
       "          8.28759745e-03,   2.63626338e-03,   8.27256683e-03,\n",
       "         -1.18494397e-02,   1.31238215e-02,  -1.83562506e-02,\n",
       "         -1.05900066e-02,  -1.75036602e-02,   3.53426440e-03,\n",
       "          1.28997769e-02,   9.72758606e-03,  -1.75476645e-03,\n",
       "         -9.38031916e-03,   7.10319681e-03,  -6.88659819e-03,\n",
       "          8.88084993e-03,   1.62427723e-02,   7.39275059e-03,\n",
       "         -1.83362532e-02,  -1.28278546e-02,   2.33568763e-03,\n",
       "          1.05979973e-02,   1.14992764e-02,  -1.53183844e-02,\n",
       "         -6.50895108e-03,   2.85140733e-04,   6.13144180e-03,\n",
       "          2.04073451e-02,  -2.29545253e-06,  -2.03242985e-06,\n",
       "         -1.93872120e-06,  -1.11014549e-06,   3.07870278e-06,\n",
       "         -6.45550290e-06,  -5.21124605e-07,   2.80506106e-06,\n",
       "         -1.91899721e-06,   1.58942123e-06,   3.93466763e-07,\n",
       "          2.19825756e-06,  -1.65447409e-06,   1.88755200e-06,\n",
       "         -5.48077878e-06,  -2.45736442e-06,  -2.20538345e-06,\n",
       "          6.96660607e-07,   4.85575129e-06,  -2.13153908e-06,\n",
       "         -5.99795214e-07,   5.65727896e-06,   5.59227658e-07,\n",
       "         -5.03287367e-07,  -4.08202777e-06,   3.03693969e-06,\n",
       "         -1.47186825e-06,   5.60333683e-06,   2.61467358e-06,\n",
       "         -1.33715366e-06,   4.30171076e-06,   5.28809949e-07,\n",
       "         -5.31344995e-06,  -3.49754123e-06,   2.32592174e-06,\n",
       "         -6.10134475e-06,   3.97548183e-06,  -4.65587931e-07,\n",
       "         -3.16491560e-06,  -2.84656630e-06,  -7.42003579e-07,\n",
       "         -1.79611732e-07,   2.86185696e-06,   4.29104148e-06,\n",
       "         -1.65085157e-06,  -9.98841870e-07,  -1.27568330e-06,\n",
       "          1.59856242e-07,  -2.04576077e-06,   6.53391908e-07,\n",
       "         -5.04087575e-06,   8.56888732e-07,   1.63869788e-06,\n",
       "          7.66060293e-06,  -5.88011790e-06,   1.85800275e-06,\n",
       "          3.02375861e-06,   2.20568586e-06,  -1.52169287e-06,\n",
       "         -2.99716362e-06,   3.19809465e-06,   7.62687796e-06,\n",
       "         -2.21359096e-06,  -1.75706134e-07,   4.76414516e-06,\n",
       "         -1.91988420e-07,   4.29056763e-06,  -1.19191077e-06,\n",
       "          3.67011626e-06,   3.27924590e-06,  -2.25691269e-06,\n",
       "         -5.84277245e-07,  -1.37893721e-06,  -2.21911890e-07,\n",
       "         -2.19316667e-06,   2.82970109e-06,   4.82513428e-07,\n",
       "         -2.35386779e-06,   9.89339696e-07,   1.71575141e-06,\n",
       "         -5.40541259e-06,  -8.74298109e-07,  -2.69521411e-06,\n",
       "         -2.89372701e-06,   3.36114522e-06,   3.26654731e-06,\n",
       "         -3.71622627e-06,  -1.74255467e-06,   7.18288675e-07,\n",
       "         -2.39012161e-06,  -5.67219377e-06,  -1.74673289e-08,\n",
       "          2.33719788e-06,  -1.34542518e-06,  -5.38628910e-06,\n",
       "         -2.71107456e-06,  -1.53630242e-06,   8.31827606e-07,\n",
       "         -5.59751243e-06,   5.94639778e-06,   1.31168258e-06,\n",
       "          1.07174083e-06,  -5.16894886e-07,   2.33512623e-08,\n",
       "          3.96578525e-06,  -7.47311674e-07,  -3.05814683e-06,\n",
       "          2.75083153e-06,   2.59169860e-06,  -1.21879452e-06,\n",
       "         -4.04568345e-06,   2.91188439e-06,  -3.62608171e-06,\n",
       "          1.71789929e-06,  -8.13163581e-07,   2.88922024e-06,\n",
       "         -1.50617907e-07,  -6.94512778e-07,   3.24831717e-07,\n",
       "          1.59139256e-06,   3.77253713e-07,   2.01210355e-06,\n",
       "          1.90467665e-06,   2.44492298e-06,   3.38010494e-07,\n",
       "         -3.40096221e-06,   1.94887093e-06,   1.86953031e-07,\n",
       "          2.29948910e-06,  -4.53533994e-06,   4.54129690e-07,\n",
       "          3.36605285e-06,  -1.79209621e-06,   3.22569349e-06,\n",
       "          1.13574731e-06,   1.25840813e-07,   6.90401748e-07,\n",
       "         -2.89731838e-06,   2.58905015e-06,  -2.23924872e-06,\n",
       "         -1.53463623e-06,  -2.98818077e-06,   4.07761718e-06,\n",
       "         -6.12095960e-07,   1.13702413e-06,   3.23083259e-06,\n",
       "          2.81455641e-06,   1.47979267e-06,  -1.30923610e-07,\n",
       "         -7.89588512e-07,  -6.10941424e-06,   4.24314919e-07,\n",
       "         -7.38656718e-06,  -4.40594789e-07,   3.09545362e-06,\n",
       "         -8.69805262e-06,   8.86543830e-07,   5.76654520e-06,\n",
       "          1.46160403e-06,   1.99544570e-06,  -6.40509370e-07,\n",
       "          6.41743043e-07,   2.08888036e-06,  -2.50654602e-06,\n",
       "         -1.76089316e-07,   9.97390316e-07,   3.07565006e-06,\n",
       "         -2.84941643e-06,   4.88804244e-06,  -1.15226260e-06,\n",
       "          2.23116513e-06,   3.49327479e-06,   2.94085498e-06,\n",
       "          9.24517906e-07,   4.28083285e-06,  -1.42529018e-07,\n",
       "          2.00707564e-06,   1.82992687e-06,   6.93028051e-06,\n",
       "         -2.57958141e-06,  -1.03104014e-08,  -4.47833281e-06,\n",
       "         -4.25101348e-07,   1.92095104e-06,   3.66817812e-06,\n",
       "         -1.73686362e-06,  -2.14261263e-06,  -2.94342931e-06,\n",
       "         -5.41299596e-06,  -1.41387943e-06,  -1.38264227e-06,\n",
       "          4.10512303e-06,  -3.30923217e-06,   3.95016559e-06,\n",
       "          6.98288318e-07,  -4.54987776e-06,   1.38132395e-06,\n",
       "         -1.56115141e-06,   2.96181793e-06,  -3.56935516e-07,\n",
       "         -1.84241355e-06,  -4.37637937e-06,  -8.39952406e-07,\n",
       "         -6.67599977e-07,   5.14184330e-06,  -5.64151969e-06,\n",
       "         -1.77000197e-06,  -4.14548055e-07,  -2.40314694e-06,\n",
       "          4.33106607e-06,  -2.56154323e-07,   8.24851782e-07,\n",
       "         -2.51785735e-07,  -1.23917539e-06,  -1.01107307e-05,\n",
       "         -3.88889021e-06,  -2.76279593e-06,   4.34295498e-06,\n",
       "          9.60357011e-06,   1.43755597e-06,   1.59967954e-07,\n",
       "          6.61440936e-06,   3.52564075e-06,  -1.61608028e-07,\n",
       "         -6.26055544e-06,   1.66838947e-06,  -4.46676586e-06,\n",
       "          5.75170088e-06,   3.35377263e-06,  -3.11274448e-07,\n",
       "          2.20147058e-07,  -6.70120244e-07,  -6.53497500e-06,\n",
       "         -8.97855261e-06,   7.15908257e-08,  -4.34694903e-06,\n",
       "          2.16234230e-06,   1.55058103e-07,  -2.73709702e-06,\n",
       "         -1.93082769e-06,   4.49703066e-06,   7.38709105e-07,\n",
       "          7.21561992e-06,   2.48759170e-06,  -7.52181109e-07,\n",
       "         -3.04629424e-07,  -4.55154213e-06,   1.22547260e-06,\n",
       "          1.01131559e-06,  -1.47762694e-06,  -6.92983076e-06,\n",
       "         -1.34558627e-06,   2.44673652e-06,   6.60359092e-06,\n",
       "         -7.04180002e-06,   7.14607040e-08,  -2.94216056e-06,\n",
       "          3.08118956e-06,   3.38395307e-06,  -4.03063586e-06,\n",
       "          6.77107846e-06,   7.57012003e-06,  -6.08032633e-06,\n",
       "         -3.38841278e-06,   3.76278399e-06,  -3.42562976e-06,\n",
       "          6.75533875e-06,   1.17073807e-06,   4.46390732e-06,\n",
       "          1.75057346e-06,  -3.45126159e-06,   3.97660159e-07,\n",
       "          1.04873925e-06,   2.46456557e-06,  -3.74897536e-06,\n",
       "          1.61827711e-06,   1.79116194e-06,  -4.45713511e-08,\n",
       "         -9.76446131e-07,  -1.92943708e-06,  -6.20800392e-06,\n",
       "         -2.64031041e-06,  -2.23478310e-06,  -5.71304554e-06,\n",
       "          2.05747710e-06,   5.71404371e-06,  -5.63205731e-06,\n",
       "         -6.68401071e-07,   2.11295514e-06,  -1.82789654e-06,\n",
       "         -4.60865749e-06,   5.05590060e-06,   1.92464108e-06,\n",
       "         -6.57672172e-06,  -4.35927495e-06,  -2.03363743e-06,\n",
       "         -4.11258588e-06,   2.93814674e-06,  -2.64533855e-06,\n",
       "          6.78559354e-06,   2.33240075e-06,   1.33430876e-06,\n",
       "         -5.48399157e-07,  -1.67806115e-06,   4.51917595e-06,\n",
       "          3.40403176e-06,  -5.37562073e-06,   8.01870897e-07,\n",
       "          1.28360762e-06,   1.91166009e-06,  -5.38226686e-06,\n",
       "          3.07163759e-06,  -4.25892495e-06,   1.15308296e-07,\n",
       "         -1.23401333e-06,   2.75385310e-06,   8.57965858e-07,\n",
       "         -1.71339695e-06,   5.03840681e-07,   3.63602680e-06,\n",
       "          2.00135764e-06,   4.49146501e-06,   1.19818264e-06,\n",
       "         -2.11942915e-06,   1.58973489e-06,  -8.01679164e-07,\n",
       "         -1.46217474e-07,  -6.98103008e-07,   2.64734877e-06,\n",
       "         -2.12212399e-06,   8.67751226e-07,   3.79232165e-06,\n",
       "         -3.55187376e-06,   1.37525149e-06,  -2.79596316e-06,\n",
       "          2.41726980e-06,  -1.71591978e-06,  -4.26705492e-06,\n",
       "          2.30671026e-06,  -2.24787823e-06,  -5.77505352e-06,\n",
       "         -4.81624966e-06,   3.29534419e-06,   1.99643068e-06,\n",
       "         -2.96102144e-06,   6.93083894e-06,   1.17913237e-06,\n",
       "          1.81325765e-06,   3.33584950e-07,   5.78132540e-06,\n",
       "         -6.34614480e-06,  -7.76405159e-07,  -5.68664655e-06,\n",
       "         -1.64032349e-06,   3.18932189e-06,  -1.18830712e-05,\n",
       "         -9.75719445e-07,   8.40430494e-06,   1.58089824e-06,\n",
       "          4.55727786e-06,  -5.07389950e-06,   3.11916915e-06,\n",
       "          5.89946922e-06,  -4.83521899e-06,  -1.88432227e-06,\n",
       "          1.28325723e-07,   1.39907945e-06,  -7.71240707e-07,\n",
       "          4.69018914e-06,   2.27516324e-07,   4.06110803e-06,\n",
       "          4.78971515e-06,   1.85333238e-06,   5.80768665e-06,\n",
       "          5.41117060e-06,   2.40367467e-06,   3.40086626e-06,\n",
       "          2.23326651e-06,   7.87180034e-06,  -9.65788217e-07,\n",
       "          1.46042339e-06,  -3.00959368e-06,   1.32563548e-06,\n",
       "         -5.83347060e-09,   4.70296891e-06,   2.30956766e-06,\n",
       "         -4.18066338e-06,  -5.39967868e-06,  -8.69085852e-06,\n",
       "         -1.81161988e-06,   3.53025621e-06,   8.64748472e-06,\n",
       "         -3.02455283e-06,   2.84632165e-06,  -2.67010648e-07,\n",
       "         -1.02381164e-05,   2.45918682e-06,   2.98674394e-07,\n",
       "          5.04760055e-06,   2.56066005e-06], dtype=float32),\n",
       " array([[ -1.12843754e-05,   4.77957074e-05,   8.35624360e-06, ...,\n",
       "          -8.73999895e-09,  -8.58363691e-09,  -8.67450467e-09],\n",
       "        [  1.23593520e-04,   1.07883105e-04,  -2.53455801e-05, ...,\n",
       "          -9.32906872e-08,  -9.13919109e-08,  -9.25155419e-08],\n",
       "        [  1.94266773e-04,   1.12471847e-04,   8.35241080e-05, ...,\n",
       "          -3.86179892e-07,  -3.78318930e-07,  -3.83039719e-07],\n",
       "        ..., \n",
       "        [  2.63220918e-05,   8.59921856e-05,   2.40990819e-04, ...,\n",
       "          -2.28150043e-07,  -2.23512330e-07,  -2.26291078e-07],\n",
       "        [ -4.88790138e-05,  -1.80576171e-04,  -2.04131080e-04, ...,\n",
       "           2.76921980e-07,   2.71334017e-07,   2.74700199e-07],\n",
       "        [  4.57282695e-05,  -5.33332714e-05,  -3.79182566e-05, ...,\n",
       "          -6.70265052e-08,  -6.56592931e-08,  -6.65034179e-08]], dtype=float32),\n",
       " array([-0.79802436, -1.03136182, -1.0313406 , ...,  0.00202244,\n",
       "         0.00198134,  0.00200608], dtype=float32)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(grads,feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Apply the optimizer to the variables / gradients tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the training TensorFlow Operation through our optimizer\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(train_op,feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learned how the model is build step by step. Noe, let's then create a Class that represents our model. This class needs a few things:\n",
    "- We have to create the model in accordance with our defined hyperparameters\n",
    "- We have to create the placeholders for our input data and expected outputs (the real data)\n",
    "- We have to create the LSTM cell structure and connect them with our RNN structure\n",
    "- We have to create the word embeddings and point them to the input data\n",
    "- We have to create the input structure for our RNN\n",
    "- We have to instanciate our RNN model and retrieve the variable in which we should expect our outputs to appear\n",
    "- We need to create a logistic structure to return the probability of our words\n",
    "- We need to create the loss and cost functions for our optimizer to work, and then create the optimizer\n",
    "- And finally, we need to create a training operation that can be run to actually train our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PTBModel(object):\n",
    "\n",
    "    def __init__(self, is_training):\n",
    "        ######################################\n",
    "        # Setting parameters for ease of use #\n",
    "        ######################################\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        ###############################################################################\n",
    "        # Creating placeholders for our input data and expected outputs (target data) #\n",
    "        ###############################################################################\n",
    "        self._input_data = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "        self._targets = tf.placeholder(tf.int32, [batch_size, num_steps]) #[30#20]\n",
    "\n",
    "        ##########################################################################\n",
    "        # Creating the LSTM cell structure and connect it with the RNN structure #\n",
    "        ##########################################################################\n",
    "        # Create the LSTM unit. \n",
    "        # This creates only the structure for the LSTM and has to be associated with a RNN unit still.\n",
    "        # The argument n_hidden(size=200) of BasicLSTMCell is size of hidden layer, that is, the number of hidden units of the LSTM (inside A).\n",
    "        # Size is the same as the size of our hidden layer, and no bias is added to the Forget Gate. \n",
    "        # LSTM cell processes one word at a time and computes probabilities of the possible continuations of the sentence.\n",
    "        lstm_cells = []\n",
    "        reuse = tf.get_variable_scope().reuse\n",
    "        for _ in range(num_layers):\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0, reuse=reuse)\n",
    "            if is_training and keep_prob < 1:\n",
    "                # Unless you changed keep_prob, this won't actually execute -- this is a dropout wrapper for our LSTM unit\n",
    "                # This is an optimization of the LSTM output, but is not needed at all\n",
    "                cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "            lstm_cells.append(cell)\n",
    "           \n",
    "        # By taking in the LSTM cells as parameters, the MultiRNNCell function junctions the LSTM units to the RNN units.\n",
    "        # RNN cell composed sequentially of multiple simple cells.\n",
    "        stacked_lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "\n",
    "        # Define the initial state, i.e., the model state for the very first data point\n",
    "        # It initialize the state of the LSTM memory. The memory state of the network is initialized with a vector of zeros and gets updated after reading each word.\n",
    "        self._initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "\n",
    "        ####################################################################\n",
    "        # Creating the word embeddings and pointing them to the input data #\n",
    "        ####################################################################\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            # Create the embeddings for our input data. Size is hidden size.\n",
    "            # Uses default variable initializer\n",
    "            embedding = tf.get_variable(\"embedding\", [vocab_size, size])  #[10000x200]\n",
    "            # Define where to get the data for our embeddings from\n",
    "            inputs = tf.nn.embedding_lookup(embedding, self._input_data)\n",
    "\n",
    "        # Unless you changed keep_prob, this won't actually execute -- this is a dropout addition for our inputs\n",
    "        # This is an optimization of the input processing and is not needed at all\n",
    "        if is_training and keep_prob < 1:\n",
    "            inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "\n",
    "        ############################################\n",
    "        # Creating the input structure for our RNN #\n",
    "        ############################################\n",
    "        # Input structure is 20x[30x200]\n",
    "        # Considering each word is represended by a 200 dimentional vector, and we have 30 batchs, we create 30 word-vectors of size [30xx2000]\n",
    "        #inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "        # The input structure is fed from the embeddings, which are filled in by the input data\n",
    "        # Feeding a batch of b sentences to a RNN:\n",
    "        # In step 1,  first word of each of the b sentences (in a batch) is input in parallel.  \n",
    "        # In step 2,  second word of each of the b sentences is input in parallel. \n",
    "        # The parallelism is only for efficiency.  \n",
    "        # Each sentence in a batch is handled in parallel, but the network sees one word of a sentence at a time and does the computations accordingly. \n",
    "        # All the computations involving the words of all sentences in a batch at a given time step are done in parallel. \n",
    "\n",
    "        ####################################################################################################\n",
    "        # Instanciating our RNN model and retrieving the structure for returning the outputs and the state #\n",
    "        ####################################################################################################\n",
    "        outputs, state = tf.nn.dynamic_rnn(stacked_lstm, inputs, initial_state=self._initial_state)\n",
    "\n",
    "        #########################################################################\n",
    "        # Creating a logistic unit to return the probability of the output word #\n",
    "        #########################################################################\n",
    "        output = tf.reshape(outputs, [-1, size])\n",
    "        softmax_w = tf.get_variable(\"softmax_w\", [size, vocab_size]) #[200x1000]\n",
    "        softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) #[1x1000]\n",
    "        logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "\n",
    "        #########################################################################\n",
    "        # Defining the loss and cost functions for the model's learning to work #\n",
    "        #########################################################################\n",
    "        loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [tf.reshape(self._targets, [-1])],\n",
    "                                                      [tf.ones([batch_size * num_steps])])\n",
    "        self._cost = cost = tf.reduce_sum(loss) / batch_size\n",
    "\n",
    "        # Store the final state\n",
    "        self._final_state = state\n",
    "\n",
    "        #Everything after this point is relevant only for training\n",
    "        if not is_training:\n",
    "            return\n",
    "\n",
    "        #################################################\n",
    "        # Creating the Training Operation for our Model #\n",
    "        #################################################\n",
    "        # Create a variable for the learning rate\n",
    "        self._lr = tf.Variable(0.0, trainable=False)\n",
    "        # Get all TensorFlow variables marked as \"trainable\" (i.e. all of them except _lr, which we just created)\n",
    "        tvars = tf.trainable_variables()\n",
    "        # Define the gradient clipping threshold\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\n",
    "        # Create the gradient descent optimizer with our learning rate\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
    "        # Create the training TensorFlow Operation through our optimizer\n",
    "        self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    # Helper functions for our LSTM RNN class\n",
    "\n",
    "    # Assign the learning rate for this model\n",
    "    def assign_lr(self, session, lr_value):\n",
    "        session.run(tf.assign(self.lr, lr_value))\n",
    "\n",
    "    # Returns the input data for this model at a point in time\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "    # Returns the targets for this model at a point in time\n",
    "    @property\n",
    "    def targets(self):\n",
    "        return self._targets\n",
    "\n",
    "    # Returns the initial state for this model\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    # Returns the defined Cost\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    # Returns the final state for this model\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state\n",
    "\n",
    "    # Returns the current learning rate for this model\n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "\n",
    "    # Returns the training operation defined for this model\n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, the actual structure of our Recurrent Neural Network with Long Short-Term Memory is finished. What remains for us to do is to actually create the methods to run through time -- that is, the `run_epoch` method to be run at each epoch and a `main` script which ties all of this together.\n",
    "\n",
    "What our `run_epoch` method should do is take our input data and feed it to the relevant operations. This will return at the very least the current result for the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "# run_epoch takes as parameters the current session, the model instance, the data to be fed, and the operation to be run #\n",
    "##########################################################################################################################\n",
    "def run_epoch(session, m, data, eval_op, verbose=False):\n",
    "\n",
    "    #Define the epoch size based on the length of the data, batch size and the number of steps\n",
    "    epoch_size = ((len(data) // m.batch_size) - 1) // m.num_steps\n",
    "    start_time = time.time()\n",
    "    costs = 0.0\n",
    "    iters = 0\n",
    "    #state = m.initial_state.eval()\n",
    "    #m.initial_state = tf.convert_to_tensor(m.initial_state) \n",
    "    #state = m.initial_state.eval()\n",
    "    state = session.run(m.initial_state)\n",
    "    \n",
    "    #For each step and data point\n",
    "    for step, (x, y) in enumerate(reader.ptb_iterator(data, m.batch_size, m.num_steps)):\n",
    " \n",
    "        #Evaluate and return cost, state by running cost, final_state and the function passed as parameter\n",
    "        cost, state, _ = session.run([m.cost, m.final_state, eval_op],\n",
    "                                     {m.input_data: x,\n",
    "                                      m.targets: y,\n",
    "                                      m.initial_state: state})\n",
    "        \n",
    "        #Add returned cost to costs (which keeps track of the total costs for this epoch)\n",
    "        costs += cost\n",
    "        \n",
    "        #Add number of steps to iteration counter\n",
    "        iters += m.num_steps\n",
    "\n",
    "        if verbose and (step % 10) == 0:\n",
    "            print(\"({:.2%})    Perplexity={:.3f}    Speed={:.0f} wps\".format(\n",
    "                    step * 1.0 / epoch_size, \n",
    "                    np.exp(costs / iters), \n",
    "                    iters * m.batch_size / (time.time() - start_time))\n",
    "                 )\n",
    "\n",
    "    # Returns the Perplexity rating for us to keep track of how the model is evolving\n",
    "    return np.exp(costs / iters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create the `main` method to tie everything together. The code here reads the data from the directory, using the `reader` helper module, and then trains and evaluates the model on both a testing and a validating subset of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reads the data and separates it into training data, validation data and testing data\n",
    "raw_data = reader.ptb_raw_data(data_dir)\n",
    "train_data, valid_data, test_data, _ = raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Learning rate: 1.000\n",
      "(0.00%)    Perplexity=10015.681    Speed=2745 wps\n",
      "(0.65%)    Perplexity=5668.811    Speed=4017 wps\n",
      "(1.29%)    Perplexity=3186.960    Speed=4048 wps\n",
      "(1.94%)    Perplexity=2392.133    Speed=4084 wps\n",
      "(2.58%)    Perplexity=2035.965    Speed=4104 wps\n",
      "(3.23%)    Perplexity=1799.039    Speed=4122 wps\n",
      "(3.87%)    Perplexity=1649.212    Speed=4137 wps\n",
      "(4.52%)    Perplexity=1520.943    Speed=4148 wps\n",
      "(5.16%)    Perplexity=1424.213    Speed=4157 wps\n",
      "(5.81%)    Perplexity=1362.012    Speed=4163 wps\n",
      "(6.46%)    Perplexity=1305.276    Speed=4166 wps\n",
      "(7.10%)    Perplexity=1253.879    Speed=4171 wps\n",
      "(7.75%)    Perplexity=1214.834    Speed=4174 wps\n",
      "(8.39%)    Perplexity=1175.636    Speed=4177 wps\n",
      "(9.04%)    Perplexity=1144.073    Speed=4179 wps\n",
      "(9.68%)    Perplexity=1118.170    Speed=4181 wps\n",
      "(10.33%)    Perplexity=1082.403    Speed=4183 wps\n",
      "(10.97%)    Perplexity=1059.396    Speed=4168 wps\n",
      "(11.62%)    Perplexity=1033.964    Speed=4170 wps\n",
      "(12.27%)    Perplexity=1010.451    Speed=4171 wps\n",
      "(12.91%)    Perplexity=986.985    Speed=4073 wps\n",
      "(13.56%)    Perplexity=964.142    Speed=3986 wps\n",
      "(14.20%)    Perplexity=939.922    Speed=3995 wps\n",
      "(14.85%)    Perplexity=920.399    Speed=4004 wps\n",
      "(15.49%)    Perplexity=899.670    Speed=4012 wps\n",
      "(16.14%)    Perplexity=882.779    Speed=4007 wps\n",
      "(16.79%)    Perplexity=864.732    Speed=4006 wps\n",
      "(17.43%)    Perplexity=847.756    Speed=3966 wps\n",
      "(18.08%)    Perplexity=831.864    Speed=3902 wps\n",
      "(18.72%)    Perplexity=818.798    Speed=3898 wps\n",
      "(19.37%)    Perplexity=805.682    Speed=3896 wps\n",
      "(20.01%)    Perplexity=792.612    Speed=3896 wps\n",
      "(20.66%)    Perplexity=777.063    Speed=3902 wps\n",
      "(21.30%)    Perplexity=764.965    Speed=3902 wps\n",
      "(21.95%)    Perplexity=752.133    Speed=3877 wps\n",
      "(22.60%)    Perplexity=738.727    Speed=3826 wps\n",
      "(23.24%)    Perplexity=728.372    Speed=3808 wps\n",
      "(23.89%)    Perplexity=719.388    Speed=3811 wps\n",
      "(24.53%)    Perplexity=710.350    Speed=3821 wps\n",
      "(25.18%)    Perplexity=699.711    Speed=3830 wps\n",
      "(25.82%)    Perplexity=688.025    Speed=3839 wps\n",
      "(26.47%)    Perplexity=678.833    Speed=3833 wps\n",
      "(27.11%)    Perplexity=668.675    Speed=3797 wps\n",
      "(27.76%)    Perplexity=659.758    Speed=3783 wps\n",
      "(28.41%)    Perplexity=650.086    Speed=3792 wps\n",
      "(29.05%)    Perplexity=643.245    Speed=3801 wps\n",
      "(29.70%)    Perplexity=635.219    Speed=3810 wps\n",
      "(30.34%)    Perplexity=627.498    Speed=3818 wps\n",
      "(30.99%)    Perplexity=620.630    Speed=3826 wps\n",
      "(31.63%)    Perplexity=612.635    Speed=3833 wps\n",
      "(32.28%)    Perplexity=604.492    Speed=3840 wps\n",
      "(32.92%)    Perplexity=597.775    Speed=3847 wps\n",
      "(33.57%)    Perplexity=589.488    Speed=3817 wps\n",
      "(34.22%)    Perplexity=582.568    Speed=3795 wps\n",
      "(34.86%)    Perplexity=576.054    Speed=3802 wps\n",
      "(35.51%)    Perplexity=568.496    Speed=3809 wps\n",
      "(36.15%)    Perplexity=559.843    Speed=3816 wps\n",
      "(36.80%)    Perplexity=552.518    Speed=3822 wps\n",
      "(37.44%)    Perplexity=547.008    Speed=3829 wps\n",
      "(38.09%)    Perplexity=541.404    Speed=3816 wps\n",
      "(38.73%)    Perplexity=535.909    Speed=3791 wps\n",
      "(39.38%)    Perplexity=531.305    Speed=3789 wps\n",
      "(40.03%)    Perplexity=526.864    Speed=3796 wps\n",
      "(40.67%)    Perplexity=521.217    Speed=3802 wps\n",
      "(41.32%)    Perplexity=516.263    Speed=3808 wps\n",
      "(41.96%)    Perplexity=510.877    Speed=3813 wps\n",
      "(42.61%)    Perplexity=506.571    Speed=3815 wps\n",
      "(43.25%)    Perplexity=500.801    Speed=3793 wps\n",
      "(43.90%)    Perplexity=496.589    Speed=3779 wps\n",
      "(44.54%)    Perplexity=494.252    Speed=3785 wps\n",
      "(45.19%)    Perplexity=490.850    Speed=3790 wps\n",
      "(45.84%)    Perplexity=487.461    Speed=3795 wps\n",
      "(46.48%)    Perplexity=483.829    Speed=3800 wps\n",
      "(47.13%)    Perplexity=480.258    Speed=3799 wps\n",
      "(47.77%)    Perplexity=475.957    Speed=3783 wps\n",
      "(48.42%)    Perplexity=471.859    Speed=3763 wps\n",
      "(49.06%)    Perplexity=467.541    Speed=3769 wps\n",
      "(49.71%)    Perplexity=463.869    Speed=3774 wps\n",
      "(50.36%)    Perplexity=460.905    Speed=3777 wps\n",
      "(51.00%)    Perplexity=458.799    Speed=3781 wps\n",
      "(51.65%)    Perplexity=455.877    Speed=3786 wps\n",
      "(52.29%)    Perplexity=452.585    Speed=3779 wps\n",
      "(52.94%)    Perplexity=449.791    Speed=3761 wps\n",
      "(53.58%)    Perplexity=446.310    Speed=3758 wps\n",
      "(54.23%)    Perplexity=442.768    Speed=3763 wps\n",
      "(54.87%)    Perplexity=439.614    Speed=3768 wps\n",
      "(55.52%)    Perplexity=437.087    Speed=3772 wps\n",
      "(56.17%)    Perplexity=434.569    Speed=3777 wps\n",
      "(56.81%)    Perplexity=432.130    Speed=3780 wps\n",
      "(57.46%)    Perplexity=428.969    Speed=3763 wps\n",
      "(58.10%)    Perplexity=425.853    Speed=3752 wps\n",
      "(58.75%)    Perplexity=422.406    Speed=3756 wps\n",
      "(59.39%)    Perplexity=418.722    Speed=3761 wps\n",
      "(60.04%)    Perplexity=415.947    Speed=3765 wps\n",
      "(60.68%)    Perplexity=413.245    Speed=3769 wps\n",
      "(61.33%)    Perplexity=410.039    Speed=3773 wps\n",
      "(61.98%)    Perplexity=406.908    Speed=3764 wps\n",
      "(62.62%)    Perplexity=403.973    Speed=3750 wps\n",
      "(63.27%)    Perplexity=401.434    Speed=3750 wps\n",
      "(63.91%)    Perplexity=398.994    Speed=3754 wps\n",
      "(64.56%)    Perplexity=396.550    Speed=3758 wps\n",
      "(65.20%)    Perplexity=394.233    Speed=3762 wps\n",
      "(65.85%)    Perplexity=392.950    Speed=3766 wps\n",
      "(66.49%)    Perplexity=391.255    Speed=3765 wps\n",
      "(67.14%)    Perplexity=389.461    Speed=3751 wps\n",
      "(67.79%)    Perplexity=387.901    Speed=3745 wps\n",
      "(68.43%)    Perplexity=385.816    Speed=3749 wps\n",
      "(69.08%)    Perplexity=383.979    Speed=3752 wps\n",
      "(69.72%)    Perplexity=382.431    Speed=3756 wps\n",
      "(70.37%)    Perplexity=380.565    Speed=3759 wps\n",
      "(71.01%)    Perplexity=378.639    Speed=3763 wps\n",
      "(71.66%)    Perplexity=377.092    Speed=3751 wps\n",
      "(72.30%)    Perplexity=374.972    Speed=3739 wps\n",
      "(72.95%)    Perplexity=373.082    Speed=3743 wps\n",
      "(73.60%)    Perplexity=371.346    Speed=3746 wps\n",
      "(74.24%)    Perplexity=369.532    Speed=3750 wps\n",
      "(74.89%)    Perplexity=367.541    Speed=3753 wps\n",
      "(75.53%)    Perplexity=365.968    Speed=3757 wps\n",
      "(76.18%)    Perplexity=364.150    Speed=3752 wps\n",
      "(76.82%)    Perplexity=362.773    Speed=3740 wps\n",
      "(77.47%)    Perplexity=361.368    Speed=3738 wps\n",
      "(78.11%)    Perplexity=359.848    Speed=3742 wps\n",
      "(78.76%)    Perplexity=358.324    Speed=3745 wps\n",
      "(79.41%)    Perplexity=357.082    Speed=3748 wps\n",
      "(80.05%)    Perplexity=355.525    Speed=3751 wps\n",
      "(80.70%)    Perplexity=354.346    Speed=3753 wps\n",
      "(81.34%)    Perplexity=352.844    Speed=3742 wps\n",
      "(81.99%)    Perplexity=350.772    Speed=3731 wps\n",
      "(82.63%)    Perplexity=349.267    Speed=3731 wps\n",
      "(83.28%)    Perplexity=347.829    Speed=3732 wps\n",
      "(83.93%)    Perplexity=345.822    Speed=3735 wps\n",
      "(84.57%)    Perplexity=344.144    Speed=3738 wps\n",
      "(85.22%)    Perplexity=342.163    Speed=3741 wps\n",
      "(85.86%)    Perplexity=340.264    Speed=3743 wps\n",
      "(86.51%)    Perplexity=338.442    Speed=3744 wps\n",
      "(87.15%)    Perplexity=336.606    Speed=3746 wps\n",
      "(87.80%)    Perplexity=335.325    Speed=3749 wps\n",
      "(88.44%)    Perplexity=333.897    Speed=3752 wps\n",
      "(89.09%)    Perplexity=332.889    Speed=3753 wps\n",
      "(89.74%)    Perplexity=332.080    Speed=3754 wps\n",
      "(90.38%)    Perplexity=330.796    Speed=3755 wps\n",
      "(91.03%)    Perplexity=329.548    Speed=3758 wps\n",
      "(91.67%)    Perplexity=327.735    Speed=3760 wps\n",
      "(92.32%)    Perplexity=326.374    Speed=3762 wps\n",
      "(92.96%)    Perplexity=325.084    Speed=3763 wps\n",
      "(93.61%)    Perplexity=323.469    Speed=3763 wps\n",
      "(94.25%)    Perplexity=322.158    Speed=3766 wps\n",
      "(94.90%)    Perplexity=320.737    Speed=3768 wps\n",
      "(95.55%)    Perplexity=319.450    Speed=3770 wps\n",
      "(96.19%)    Perplexity=318.212    Speed=3771 wps\n",
      "(96.84%)    Perplexity=317.332    Speed=3772 wps\n",
      "(97.48%)    Perplexity=316.370    Speed=3768 wps\n",
      "(98.13%)    Perplexity=315.369    Speed=3751 wps\n",
      "(98.77%)    Perplexity=314.355    Speed=3749 wps\n",
      "(99.42%)    Perplexity=313.427    Speed=3752 wps\n",
      "Epoch 1 : Train Perplexity: 312.663\n",
      "Epoch 1 : Valid Perplexity: 190.744\n",
      "Epoch 2 : Learning rate: 0.500\n",
      "(0.00%)    Perplexity=253.566    Speed=1781 wps\n",
      "(0.65%)    Perplexity=205.115    Speed=3362 wps\n",
      "(1.29%)    Perplexity=188.936    Speed=3724 wps\n",
      "(1.94%)    Perplexity=178.739    Speed=3871 wps\n",
      "(2.58%)    Perplexity=177.039    Speed=3950 wps\n",
      "(3.23%)    Perplexity=178.496    Speed=4000 wps\n",
      "(3.87%)    Perplexity=177.138    Speed=3986 wps\n",
      "(4.52%)    Perplexity=173.094    Speed=3742 wps\n",
      "(5.16%)    Perplexity=173.307    Speed=3636 wps\n",
      "(5.81%)    Perplexity=175.757    Speed=3692 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6.46%)    Perplexity=175.068    Speed=3733 wps\n",
      "(7.10%)    Perplexity=176.832    Speed=3774 wps\n",
      "(7.75%)    Perplexity=178.631    Speed=3807 wps\n",
      "(8.39%)    Perplexity=178.417    Speed=3836 wps\n",
      "(9.04%)    Perplexity=178.469    Speed=3766 wps\n",
      "(9.68%)    Perplexity=179.058    Speed=3674 wps\n",
      "(10.33%)    Perplexity=178.738    Speed=3678 wps\n",
      "(10.97%)    Perplexity=179.885    Speed=3690 wps\n",
      "(11.62%)    Perplexity=179.845    Speed=3705 wps\n",
      "(12.27%)    Perplexity=179.013    Speed=3729 wps\n",
      "(12.91%)    Perplexity=177.816    Speed=3751 wps\n",
      "(13.56%)    Perplexity=176.914    Speed=3765 wps\n",
      "(14.20%)    Perplexity=175.427    Speed=3772 wps\n",
      "(14.85%)    Perplexity=175.368    Speed=3778 wps\n",
      "(15.49%)    Perplexity=174.418    Speed=3793 wps\n",
      "(16.14%)    Perplexity=174.443    Speed=3808 wps\n",
      "(16.79%)    Perplexity=174.490    Speed=3822 wps\n",
      "(17.43%)    Perplexity=174.097    Speed=3826 wps\n",
      "(18.08%)    Perplexity=173.453    Speed=3830 wps\n",
      "(18.72%)    Perplexity=173.827    Speed=3837 wps\n",
      "(19.37%)    Perplexity=173.622    Speed=3850 wps\n",
      "(20.01%)    Perplexity=173.230    Speed=3860 wps\n",
      "(20.66%)    Perplexity=171.820    Speed=3864 wps\n",
      "(21.30%)    Perplexity=171.128    Speed=3866 wps\n",
      "(21.95%)    Perplexity=170.344    Speed=3868 wps\n",
      "(22.60%)    Perplexity=169.310    Speed=3877 wps\n",
      "(23.24%)    Perplexity=168.954    Speed=3886 wps\n",
      "(23.89%)    Perplexity=169.133    Speed=3892 wps\n",
      "(24.53%)    Perplexity=169.234    Speed=3894 wps\n",
      "(25.18%)    Perplexity=168.603    Speed=3895 wps\n",
      "(25.82%)    Perplexity=167.797    Speed=3899 wps\n",
      "(26.47%)    Perplexity=167.498    Speed=3907 wps\n",
      "(27.11%)    Perplexity=166.857    Speed=3914 wps\n",
      "(27.76%)    Perplexity=166.358    Speed=3916 wps\n",
      "(28.41%)    Perplexity=165.794    Speed=3917 wps\n",
      "(29.05%)    Perplexity=165.741    Speed=3911 wps\n",
      "(29.70%)    Perplexity=165.313    Speed=3852 wps\n",
      "(30.34%)    Perplexity=164.694    Speed=3831 wps\n",
      "(30.99%)    Perplexity=164.335    Speed=3839 wps\n",
      "(31.63%)    Perplexity=163.923    Speed=3846 wps\n",
      "(32.28%)    Perplexity=163.245    Speed=3853 wps\n",
      "(32.92%)    Perplexity=163.007    Speed=3859 wps\n",
      "(33.57%)    Perplexity=162.197    Speed=3866 wps\n",
      "(34.22%)    Perplexity=161.722    Speed=3837 wps\n",
      "(34.86%)    Perplexity=161.082    Speed=3812 wps\n",
      "(35.51%)    Perplexity=160.186    Speed=3818 wps\n",
      "(36.15%)    Perplexity=158.931    Speed=3825 wps\n",
      "(36.80%)    Perplexity=157.893    Speed=3830 wps\n",
      "(37.44%)    Perplexity=157.722    Speed=3836 wps\n",
      "(38.09%)    Perplexity=157.337    Speed=3841 wps\n",
      "(38.73%)    Perplexity=157.073    Speed=3829 wps\n",
      "(39.38%)    Perplexity=156.988    Speed=3803 wps\n",
      "(40.03%)    Perplexity=156.934    Speed=3800 wps\n",
      "(40.67%)    Perplexity=156.525    Speed=3807 wps\n",
      "(41.32%)    Perplexity=156.235    Speed=3812 wps\n",
      "(41.96%)    Perplexity=155.760    Speed=3817 wps\n",
      "(42.61%)    Perplexity=155.445    Speed=3823 wps\n",
      "(43.25%)    Perplexity=154.885    Speed=3823 wps\n",
      "(43.90%)    Perplexity=154.656    Speed=3800 wps\n",
      "(44.54%)    Perplexity=155.058    Speed=3784 wps\n",
      "(45.19%)    Perplexity=155.181    Speed=3785 wps\n",
      "(45.84%)    Perplexity=155.188    Speed=3787 wps\n",
      "(46.48%)    Perplexity=155.022    Speed=3791 wps\n",
      "(47.13%)    Perplexity=154.834    Speed=3797 wps\n",
      "(47.77%)    Perplexity=154.463    Speed=3802 wps\n",
      "(48.42%)    Perplexity=153.950    Speed=3802 wps\n",
      "(49.06%)    Perplexity=153.634    Speed=3803 wps\n",
      "(49.71%)    Perplexity=153.487    Speed=3791 wps\n",
      "(50.36%)    Perplexity=153.436    Speed=3759 wps\n",
      "(51.00%)    Perplexity=153.691    Speed=3759 wps\n",
      "(51.65%)    Perplexity=153.661    Speed=3764 wps\n",
      "(52.29%)    Perplexity=153.459    Speed=3769 wps\n",
      "(52.94%)    Perplexity=153.299    Speed=3773 wps\n",
      "(53.58%)    Perplexity=152.978    Speed=3778 wps\n",
      "(54.23%)    Perplexity=152.687    Speed=3775 wps\n",
      "(54.87%)    Perplexity=152.493    Speed=3758 wps\n",
      "(55.52%)    Perplexity=152.471    Speed=3750 wps\n",
      "(56.17%)    Perplexity=152.342    Speed=3752 wps\n",
      "(56.81%)    Perplexity=152.271    Speed=3754 wps\n",
      "(57.46%)    Perplexity=151.988    Speed=3758 wps\n",
      "(58.10%)    Perplexity=151.741    Speed=3762 wps\n",
      "(58.75%)    Perplexity=151.159    Speed=3767 wps\n",
      "(59.39%)    Perplexity=150.544    Speed=3768 wps\n",
      "(60.04%)    Perplexity=150.160    Speed=3769 wps\n",
      "(60.68%)    Perplexity=149.815    Speed=3772 wps\n",
      "(61.33%)    Perplexity=149.420    Speed=3776 wps\n",
      "(61.98%)    Perplexity=148.893    Speed=3779 wps\n",
      "(62.62%)    Perplexity=148.420    Speed=3781 wps\n",
      "(63.27%)    Perplexity=148.182    Speed=3782 wps\n",
      "(63.91%)    Perplexity=147.969    Speed=3784 wps\n",
      "(64.56%)    Perplexity=147.672    Speed=3788 wps\n",
      "(65.20%)    Perplexity=147.370    Speed=3792 wps\n",
      "(65.85%)    Perplexity=147.506    Speed=3794 wps\n",
      "(66.49%)    Perplexity=147.478    Speed=3796 wps\n",
      "(67.14%)    Perplexity=147.450    Speed=3797 wps\n",
      "(67.79%)    Perplexity=147.430    Speed=3800 wps\n",
      "(68.43%)    Perplexity=147.255    Speed=3804 wps\n",
      "(69.08%)    Perplexity=147.141    Speed=3807 wps\n",
      "(69.72%)    Perplexity=147.123    Speed=3809 wps\n",
      "(70.37%)    Perplexity=146.931    Speed=3810 wps\n",
      "(71.01%)    Perplexity=146.788    Speed=3806 wps\n",
      "(71.66%)    Perplexity=146.785    Speed=3783 wps\n",
      "(72.30%)    Perplexity=146.540    Speed=3778 wps\n",
      "(72.95%)    Perplexity=146.301    Speed=3782 wps\n",
      "(73.60%)    Perplexity=146.038    Speed=3785 wps\n",
      "(74.24%)    Perplexity=145.761    Speed=3789 wps\n",
      "(74.89%)    Perplexity=145.445    Speed=3792 wps\n",
      "(75.53%)    Perplexity=145.309    Speed=3795 wps\n",
      "(76.18%)    Perplexity=145.060    Speed=3782 wps\n",
      "(76.82%)    Perplexity=145.088    Speed=3773 wps\n",
      "(77.47%)    Perplexity=145.043    Speed=3774 wps\n",
      "(78.11%)    Perplexity=144.988    Speed=3775 wps\n",
      "(78.76%)    Perplexity=144.879    Speed=3778 wps\n",
      "(79.41%)    Perplexity=144.914    Speed=3781 wps\n",
      "(80.05%)    Perplexity=144.811    Speed=3784 wps\n",
      "(80.70%)    Perplexity=144.805    Speed=3785 wps\n",
      "(81.34%)    Perplexity=144.708    Speed=3786 wps\n",
      "(81.99%)    Perplexity=144.395    Speed=3783 wps\n",
      "(82.63%)    Perplexity=144.250    Speed=3764 wps\n",
      "(83.28%)    Perplexity=144.080    Speed=3760 wps\n",
      "(83.93%)    Perplexity=143.666    Speed=3763 wps\n",
      "(84.57%)    Perplexity=143.305    Speed=3766 wps\n",
      "(85.22%)    Perplexity=142.734    Speed=3769 wps\n",
      "(85.86%)    Perplexity=142.348    Speed=3772 wps\n",
      "(86.51%)    Perplexity=141.942    Speed=3773 wps\n",
      "(87.15%)    Perplexity=141.576    Speed=3763 wps\n",
      "(87.80%)    Perplexity=141.414    Speed=3756 wps\n",
      "(88.44%)    Perplexity=141.193    Speed=3759 wps\n",
      "(89.09%)    Perplexity=141.164    Speed=3762 wps\n",
      "(89.74%)    Perplexity=141.237    Speed=3765 wps\n",
      "(90.38%)    Perplexity=141.086    Speed=3768 wps\n",
      "(91.03%)    Perplexity=140.984    Speed=3771 wps\n",
      "(91.67%)    Perplexity=140.645    Speed=3766 wps\n",
      "(92.32%)    Perplexity=140.454    Speed=3755 wps\n",
      "(92.96%)    Perplexity=140.298    Speed=3753 wps\n",
      "(93.61%)    Perplexity=139.954    Speed=3754 wps\n",
      "(94.25%)    Perplexity=139.733    Speed=3755 wps\n",
      "(94.90%)    Perplexity=139.483    Speed=3758 wps\n",
      "(95.55%)    Perplexity=139.274    Speed=3761 wps\n",
      "(96.19%)    Perplexity=139.107    Speed=3763 wps\n",
      "(96.84%)    Perplexity=139.086    Speed=3764 wps\n",
      "(97.48%)    Perplexity=139.003    Speed=3765 wps\n",
      "(98.13%)    Perplexity=138.931    Speed=3752 wps\n",
      "(98.77%)    Perplexity=138.822    Speed=3741 wps\n",
      "(99.42%)    Perplexity=138.777    Speed=3744 wps\n",
      "Epoch 2 : Train Perplexity: 138.718\n",
      "Epoch 2 : Valid Perplexity: 147.653\n",
      "Epoch 3 : Learning rate: 0.462\n",
      "(0.00%)    Perplexity=184.911    Speed=1804 wps\n",
      "(0.65%)    Perplexity=145.385    Speed=2767 wps\n",
      "(1.29%)    Perplexity=135.595    Speed=3223 wps\n",
      "(1.94%)    Perplexity=126.650    Speed=3423 wps\n",
      "(2.58%)    Perplexity=123.804    Speed=3569 wps\n",
      "(3.23%)    Perplexity=125.624    Speed=3680 wps\n",
      "(3.87%)    Perplexity=124.690    Speed=3756 wps\n",
      "(4.52%)    Perplexity=122.813    Speed=3783 wps\n",
      "(5.16%)    Perplexity=123.855    Speed=3796 wps\n",
      "(5.81%)    Perplexity=125.999    Speed=3812 wps\n",
      "(6.46%)    Perplexity=125.905    Speed=3847 wps\n",
      "(7.10%)    Perplexity=128.006    Speed=3874 wps\n",
      "(7.75%)    Perplexity=129.928    Speed=3887 wps\n",
      "(8.39%)    Perplexity=130.172    Speed=3889 wps\n",
      "(9.04%)    Perplexity=130.441    Speed=3889 wps\n",
      "(9.68%)    Perplexity=131.263    Speed=3751 wps\n",
      "(10.33%)    Perplexity=131.334    Speed=3649 wps\n",
      "(10.97%)    Perplexity=132.366    Speed=3677 wps\n",
      "(11.62%)    Perplexity=132.316    Speed=3700 wps\n",
      "(12.27%)    Perplexity=131.794    Speed=3724 wps\n",
      "(12.91%)    Perplexity=131.133    Speed=3745 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13.56%)    Perplexity=130.713    Speed=3764 wps\n",
      "(14.20%)    Perplexity=129.844    Speed=3725 wps\n",
      "(14.85%)    Perplexity=129.943    Speed=3667 wps\n",
      "(15.49%)    Perplexity=129.432    Speed=3667 wps\n",
      "(16.14%)    Perplexity=129.510    Speed=3676 wps\n",
      "(16.79%)    Perplexity=129.795    Speed=3687 wps\n",
      "(17.43%)    Perplexity=129.669    Speed=3704 wps\n",
      "(18.08%)    Perplexity=129.246    Speed=3719 wps\n",
      "(18.72%)    Perplexity=129.689    Speed=3730 wps\n",
      "(19.37%)    Perplexity=129.695    Speed=3736 wps\n",
      "(20.01%)    Perplexity=129.531    Speed=3736 wps\n",
      "(20.66%)    Perplexity=128.593    Speed=3669 wps\n",
      "(21.30%)    Perplexity=128.146    Speed=3632 wps\n",
      "(21.95%)    Perplexity=127.622    Speed=3647 wps\n",
      "(22.60%)    Perplexity=126.989    Speed=3660 wps\n",
      "(23.24%)    Perplexity=126.812    Speed=3674 wps\n",
      "(23.89%)    Perplexity=127.061    Speed=3686 wps\n",
      "(24.53%)    Perplexity=127.232    Speed=3698 wps\n",
      "(25.18%)    Perplexity=126.868    Speed=3673 wps\n",
      "(25.82%)    Perplexity=126.385    Speed=3641 wps\n",
      "(26.47%)    Perplexity=126.246    Speed=3651 wps\n",
      "(27.11%)    Perplexity=125.789    Speed=3662 wps\n",
      "(27.76%)    Perplexity=125.483    Speed=3674 wps\n",
      "(28.41%)    Perplexity=125.164    Speed=3684 wps\n",
      "(29.05%)    Perplexity=125.197    Speed=3694 wps\n",
      "(29.70%)    Perplexity=124.793    Speed=3689 wps\n",
      "(30.34%)    Perplexity=124.315    Speed=3659 wps\n",
      "(30.99%)    Perplexity=124.035    Speed=3651 wps\n",
      "(31.63%)    Perplexity=123.816    Speed=3656 wps\n",
      "(32.28%)    Perplexity=123.396    Speed=3659 wps\n",
      "(32.92%)    Perplexity=123.276    Speed=3635 wps\n",
      "(33.57%)    Perplexity=122.743    Speed=3617 wps\n",
      "(34.22%)    Perplexity=122.483    Speed=3627 wps\n",
      "(34.86%)    Perplexity=122.033    Speed=3636 wps\n",
      "(35.51%)    Perplexity=121.367    Speed=3644 wps\n",
      "(36.15%)    Perplexity=120.438    Speed=3653 wps\n",
      "(36.80%)    Perplexity=119.693    Speed=3660 wps\n",
      "(37.44%)    Perplexity=119.633    Speed=3648 wps\n",
      "(38.09%)    Perplexity=119.427    Speed=3627 wps\n",
      "(38.73%)    Perplexity=119.300    Speed=3629 wps\n",
      "(39.38%)    Perplexity=119.292    Speed=3624 wps\n",
      "(40.03%)    Perplexity=119.305    Speed=3629 wps\n",
      "(40.67%)    Perplexity=119.067    Speed=3633 wps\n",
      "(41.32%)    Perplexity=118.947    Speed=3630 wps\n",
      "(41.96%)    Perplexity=118.662    Speed=3611 wps\n",
      "(42.61%)    Perplexity=118.473    Speed=3589 wps\n",
      "(43.25%)    Perplexity=118.132    Speed=3593 wps\n",
      "(43.90%)    Perplexity=118.033    Speed=3598 wps\n",
      "(44.54%)    Perplexity=118.412    Speed=3589 wps\n",
      "(45.19%)    Perplexity=118.577    Speed=3573 wps\n",
      "(45.84%)    Perplexity=118.596    Speed=3573 wps\n",
      "(46.48%)    Perplexity=118.499    Speed=3577 wps\n",
      "(47.13%)    Perplexity=118.417    Speed=3582 wps\n",
      "(47.77%)    Perplexity=118.192    Speed=3589 wps\n",
      "(48.42%)    Perplexity=117.845    Speed=3597 wps\n",
      "(49.06%)    Perplexity=117.696    Speed=3603 wps\n",
      "(49.71%)    Perplexity=117.656    Speed=3607 wps\n",
      "(50.36%)    Perplexity=117.676    Speed=3610 wps\n",
      "(51.00%)    Perplexity=117.932    Speed=3616 wps\n",
      "(51.65%)    Perplexity=117.932    Speed=3622 wps\n",
      "(52.29%)    Perplexity=117.833    Speed=3629 wps\n",
      "(52.94%)    Perplexity=117.768    Speed=3633 wps\n",
      "(53.58%)    Perplexity=117.588    Speed=3636 wps\n",
      "(54.23%)    Perplexity=117.435    Speed=3631 wps\n",
      "(54.87%)    Perplexity=117.366    Speed=3606 wps\n",
      "(55.52%)    Perplexity=117.429    Speed=3605 wps\n",
      "(56.17%)    Perplexity=117.371    Speed=3610 wps\n",
      "(56.81%)    Perplexity=117.378    Speed=3617 wps\n",
      "(57.46%)    Perplexity=117.210    Speed=3622 wps\n",
      "(58.10%)    Perplexity=117.059    Speed=3628 wps\n",
      "(58.75%)    Perplexity=116.637    Speed=3630 wps\n",
      "(59.39%)    Perplexity=116.189    Speed=3617 wps\n",
      "(60.04%)    Perplexity=115.893    Speed=3610 wps\n",
      "(60.68%)    Perplexity=115.653    Speed=3616 wps\n",
      "(61.33%)    Perplexity=115.403    Speed=3621 wps\n",
      "(61.98%)    Perplexity=115.012    Speed=3626 wps\n",
      "(62.62%)    Perplexity=114.683    Speed=3631 wps\n",
      "(63.27%)    Perplexity=114.544    Speed=3636 wps\n",
      "(63.91%)    Perplexity=114.410    Speed=3628 wps\n",
      "(64.56%)    Perplexity=114.209    Speed=3616 wps\n",
      "(65.20%)    Perplexity=113.990    Speed=3620 wps\n",
      "(65.85%)    Perplexity=114.137    Speed=3625 wps\n",
      "(66.49%)    Perplexity=114.153    Speed=3630 wps\n",
      "(67.14%)    Perplexity=114.189    Speed=3635 wps\n",
      "(67.79%)    Perplexity=114.213    Speed=3640 wps\n",
      "(68.43%)    Perplexity=114.125    Speed=3639 wps\n",
      "(69.08%)    Perplexity=114.075    Speed=3628 wps\n",
      "(69.72%)    Perplexity=114.109    Speed=3624 wps\n",
      "(70.37%)    Perplexity=113.998    Speed=3626 wps\n",
      "(71.01%)    Perplexity=113.925    Speed=3629 wps\n",
      "(71.66%)    Perplexity=113.975    Speed=3633 wps\n",
      "(72.30%)    Perplexity=113.808    Speed=3637 wps\n",
      "(72.95%)    Perplexity=113.664    Speed=3642 wps\n",
      "(73.60%)    Perplexity=113.474    Speed=3644 wps\n",
      "(74.24%)    Perplexity=113.267    Speed=3646 wps\n",
      "(74.89%)    Perplexity=113.048    Speed=3649 wps\n",
      "(75.53%)    Perplexity=112.986    Speed=3653 wps\n",
      "(76.18%)    Perplexity=112.819    Speed=3657 wps\n",
      "(76.82%)    Perplexity=112.888    Speed=3660 wps\n",
      "(77.47%)    Perplexity=112.896    Speed=3662 wps\n",
      "(78.11%)    Perplexity=112.903    Speed=3664 wps\n",
      "(78.76%)    Perplexity=112.864    Speed=3668 wps\n",
      "(79.41%)    Perplexity=112.933    Speed=3672 wps\n",
      "(80.05%)    Perplexity=112.904    Speed=3675 wps\n",
      "(80.70%)    Perplexity=112.935    Speed=3677 wps\n",
      "(81.34%)    Perplexity=112.912    Speed=3679 wps\n",
      "(81.99%)    Perplexity=112.736    Speed=3669 wps\n",
      "(82.63%)    Perplexity=112.660    Speed=3653 wps\n",
      "(83.28%)    Perplexity=112.540    Speed=3657 wps\n",
      "(83.93%)    Perplexity=112.223    Speed=3660 wps\n",
      "(84.57%)    Perplexity=111.947    Speed=3663 wps\n",
      "(85.22%)    Perplexity=111.508    Speed=3667 wps\n",
      "(85.86%)    Perplexity=111.235    Speed=3670 wps\n",
      "(86.51%)    Perplexity=110.947    Speed=3667 wps\n",
      "(87.15%)    Perplexity=110.696    Speed=3657 wps\n",
      "(87.80%)    Perplexity=110.591    Speed=3656 wps\n",
      "(88.44%)    Perplexity=110.459    Speed=3657 wps\n",
      "(89.09%)    Perplexity=110.466    Speed=3658 wps\n",
      "(89.74%)    Perplexity=110.565    Speed=3649 wps\n",
      "(90.38%)    Perplexity=110.484    Speed=3643 wps\n",
      "(91.03%)    Perplexity=110.447    Speed=3647 wps\n",
      "(91.67%)    Perplexity=110.230    Speed=3651 wps\n",
      "(92.32%)    Perplexity=110.116    Speed=3654 wps\n",
      "(92.96%)    Perplexity=110.021    Speed=3657 wps\n",
      "(93.61%)    Perplexity=109.779    Speed=3661 wps\n",
      "(94.25%)    Perplexity=109.633    Speed=3656 wps\n",
      "(94.90%)    Perplexity=109.468    Speed=3647 wps\n",
      "(95.55%)    Perplexity=109.339    Speed=3649 wps\n",
      "(96.19%)    Perplexity=109.242    Speed=3652 wps\n",
      "(96.84%)    Perplexity=109.256    Speed=3655 wps\n",
      "(97.48%)    Perplexity=109.218    Speed=3659 wps\n",
      "(98.13%)    Perplexity=109.198    Speed=3662 wps\n",
      "(98.77%)    Perplexity=109.143    Speed=3662 wps\n",
      "(99.42%)    Perplexity=109.143    Speed=3654 wps\n",
      "Epoch 3 : Train Perplexity: 109.121\n",
      "Epoch 3 : Valid Perplexity: 134.245\n",
      "Epoch 4 : Learning rate: 0.423\n",
      "(0.00%)    Perplexity=152.518    Speed=3001 wps\n",
      "(0.65%)    Perplexity=116.841    Speed=3966 wps\n",
      "(1.29%)    Perplexity=109.494    Speed=3938 wps\n",
      "(1.94%)    Perplexity=102.025    Speed=3929 wps\n",
      "(2.58%)    Perplexity=99.129    Speed=3972 wps\n",
      "(3.23%)    Perplexity=100.668    Speed=4013 wps\n",
      "(3.87%)    Perplexity=99.769    Speed=4038 wps\n",
      "(4.52%)    Perplexity=98.489    Speed=4021 wps\n",
      "(5.16%)    Perplexity=99.550    Speed=4006 wps\n",
      "(5.81%)    Perplexity=101.186    Speed=4005 wps\n",
      "(6.46%)    Perplexity=101.329    Speed=4025 wps\n",
      "(7.10%)    Perplexity=103.339    Speed=4043 wps\n",
      "(7.75%)    Perplexity=105.054    Speed=4044 wps\n",
      "(8.39%)    Perplexity=105.412    Speed=4033 wps\n",
      "(9.04%)    Perplexity=105.874    Speed=4023 wps\n",
      "(9.68%)    Perplexity=106.656    Speed=4035 wps\n",
      "(10.33%)    Perplexity=106.789    Speed=4045 wps\n",
      "(10.97%)    Perplexity=107.655    Speed=4050 wps\n",
      "(11.62%)    Perplexity=107.558    Speed=4042 wps\n",
      "(12.27%)    Perplexity=107.110    Speed=4033 wps\n",
      "(12.91%)    Perplexity=106.668    Speed=3954 wps\n",
      "(13.56%)    Perplexity=106.424    Speed=3807 wps\n",
      "(14.20%)    Perplexity=105.782    Speed=3823 wps\n",
      "(14.85%)    Perplexity=105.874    Speed=3838 wps\n",
      "(15.49%)    Perplexity=105.512    Speed=3853 wps\n",
      "(16.14%)    Perplexity=105.610    Speed=3867 wps\n",
      "(16.79%)    Perplexity=105.931    Speed=3880 wps\n",
      "(17.43%)    Perplexity=105.877    Speed=3860 wps\n",
      "(18.08%)    Perplexity=105.492    Speed=3803 wps\n",
      "(18.72%)    Perplexity=105.897    Speed=3786 wps\n",
      "(19.37%)    Perplexity=105.972    Speed=3791 wps\n",
      "(20.01%)    Perplexity=105.908    Speed=3794 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20.66%)    Perplexity=105.238    Speed=3748 wps\n",
      "(21.30%)    Perplexity=104.889    Speed=3715 wps\n",
      "(21.95%)    Perplexity=104.480    Speed=3729 wps\n",
      "(22.60%)    Perplexity=103.995    Speed=3742 wps\n",
      "(23.24%)    Perplexity=103.840    Speed=3755 wps\n",
      "(23.89%)    Perplexity=104.101    Speed=3765 wps\n",
      "(24.53%)    Perplexity=104.256    Speed=3775 wps\n",
      "(25.18%)    Perplexity=104.007    Speed=3758 wps\n",
      "(25.82%)    Perplexity=103.673    Speed=3724 wps\n",
      "(26.47%)    Perplexity=103.614    Speed=3722 wps\n",
      "(27.11%)    Perplexity=103.217    Speed=3733 wps\n",
      "(27.76%)    Perplexity=102.995    Speed=3744 wps\n",
      "(28.41%)    Perplexity=102.780    Speed=3754 wps\n",
      "(29.05%)    Perplexity=102.826    Speed=3760 wps\n",
      "(29.70%)    Perplexity=102.445    Speed=3761 wps\n",
      "(30.34%)    Perplexity=102.049    Speed=3731 wps\n",
      "(30.99%)    Perplexity=101.807    Speed=3713 wps\n",
      "(31.63%)    Perplexity=101.656    Speed=3717 wps\n",
      "(32.28%)    Perplexity=101.336    Speed=3721 wps\n",
      "(32.92%)    Perplexity=101.262    Speed=3702 wps\n",
      "(33.57%)    Perplexity=100.837    Speed=3677 wps\n",
      "(34.22%)    Perplexity=100.653    Speed=3679 wps\n",
      "(34.86%)    Perplexity=100.282    Speed=3682 wps\n",
      "(35.51%)    Perplexity=99.737    Speed=3687 wps\n",
      "(36.15%)    Perplexity=99.002    Speed=3695 wps\n",
      "(36.80%)    Perplexity=98.405    Speed=3703 wps\n",
      "(37.44%)    Perplexity=98.390    Speed=3708 wps\n",
      "(38.09%)    Perplexity=98.263    Speed=3711 wps\n",
      "(38.73%)    Perplexity=98.176    Speed=3714 wps\n",
      "(39.38%)    Perplexity=98.190    Speed=3681 wps\n",
      "(40.03%)    Perplexity=98.202    Speed=3660 wps\n",
      "(40.67%)    Perplexity=98.040    Speed=3668 wps\n",
      "(41.32%)    Perplexity=97.983    Speed=3676 wps\n",
      "(41.96%)    Perplexity=97.783    Speed=3683 wps\n",
      "(42.61%)    Perplexity=97.650    Speed=3690 wps\n",
      "(43.25%)    Perplexity=97.396    Speed=3697 wps\n",
      "(43.90%)    Perplexity=97.338    Speed=3683 wps\n",
      "(44.54%)    Perplexity=97.666    Speed=3664 wps\n",
      "(45.19%)    Perplexity=97.820    Speed=3667 wps\n",
      "(45.84%)    Perplexity=97.836    Speed=3670 wps\n",
      "(46.48%)    Perplexity=97.760    Speed=3675 wps\n",
      "(47.13%)    Perplexity=97.716    Speed=3681 wps\n",
      "(47.77%)    Perplexity=97.567    Speed=3688 wps\n",
      "(48.42%)    Perplexity=97.302    Speed=3692 wps\n",
      "(49.06%)    Perplexity=97.217    Speed=3695 wps\n",
      "(49.71%)    Perplexity=97.211    Speed=3698 wps\n",
      "(50.36%)    Perplexity=97.253    Speed=3672 wps\n",
      "(51.00%)    Perplexity=97.477    Speed=3656 wps\n",
      "(51.65%)    Perplexity=97.469    Speed=3662 wps\n",
      "(52.29%)    Perplexity=97.405    Speed=3668 wps\n",
      "(52.94%)    Perplexity=97.383    Speed=3674 wps\n",
      "(53.58%)    Perplexity=97.258    Speed=3680 wps\n",
      "(54.23%)    Perplexity=97.161    Speed=3686 wps\n",
      "(54.87%)    Perplexity=97.134    Speed=3676 wps\n",
      "(55.52%)    Perplexity=97.219    Speed=3661 wps\n",
      "(56.17%)    Perplexity=97.199    Speed=3662 wps\n",
      "(56.81%)    Perplexity=97.233    Speed=3665 wps\n",
      "(57.46%)    Perplexity=97.116    Speed=3663 wps\n",
      "(58.10%)    Perplexity=97.001    Speed=3649 wps\n",
      "(58.75%)    Perplexity=96.664    Speed=3645 wps\n",
      "(59.39%)    Perplexity=96.306    Speed=3650 wps\n",
      "(60.04%)    Perplexity=96.070    Speed=3655 wps\n",
      "(60.68%)    Perplexity=95.890    Speed=3660 wps\n",
      "(61.33%)    Perplexity=95.705    Speed=3665 wps\n",
      "(61.98%)    Perplexity=95.387    Speed=3671 wps\n",
      "(62.62%)    Perplexity=95.131    Speed=3660 wps\n",
      "(63.27%)    Perplexity=95.035    Speed=3648 wps\n",
      "(63.91%)    Perplexity=94.945    Speed=3653 wps\n",
      "(64.56%)    Perplexity=94.781    Speed=3658 wps\n",
      "(65.20%)    Perplexity=94.611    Speed=3663 wps\n",
      "(65.85%)    Perplexity=94.747    Speed=3668 wps\n",
      "(66.49%)    Perplexity=94.775    Speed=3673 wps\n",
      "(67.14%)    Perplexity=94.830    Speed=3670 wps\n",
      "(67.79%)    Perplexity=94.862    Speed=3658 wps\n",
      "(68.43%)    Perplexity=94.805    Speed=3656 wps\n",
      "(69.08%)    Perplexity=94.768    Speed=3660 wps\n",
      "(69.72%)    Perplexity=94.814    Speed=3664 wps\n",
      "(70.37%)    Perplexity=94.739    Speed=3668 wps\n",
      "(71.01%)    Perplexity=94.696    Speed=3672 wps\n",
      "(71.66%)    Perplexity=94.767    Speed=3677 wps\n",
      "(72.30%)    Perplexity=94.640    Speed=3665 wps\n",
      "(72.95%)    Perplexity=94.520    Speed=3656 wps\n",
      "(73.60%)    Perplexity=94.357    Speed=3658 wps\n",
      "(74.24%)    Perplexity=94.181    Speed=3660 wps\n",
      "(74.89%)    Perplexity=94.011    Speed=3654 wps\n",
      "(75.53%)    Perplexity=93.973    Speed=3642 wps\n",
      "(76.18%)    Perplexity=93.854    Speed=3643 wps\n",
      "(76.82%)    Perplexity=93.929    Speed=3647 wps\n",
      "(77.47%)    Perplexity=93.953    Speed=3651 wps\n",
      "(78.11%)    Perplexity=93.987    Speed=3655 wps\n",
      "(78.76%)    Perplexity=93.966    Speed=3659 wps\n",
      "(79.41%)    Perplexity=94.042    Speed=3660 wps\n",
      "(80.05%)    Perplexity=94.045    Speed=3650 wps\n",
      "(80.70%)    Perplexity=94.086    Speed=3645 wps\n",
      "(81.34%)    Perplexity=94.087    Speed=3647 wps\n",
      "(81.99%)    Perplexity=93.968    Speed=3649 wps\n",
      "(82.63%)    Perplexity=93.924    Speed=3652 wps\n",
      "(83.28%)    Perplexity=93.823    Speed=3656 wps\n",
      "(83.93%)    Perplexity=93.566    Speed=3660 wps\n",
      "(84.57%)    Perplexity=93.339    Speed=3662 wps\n",
      "(85.22%)    Perplexity=92.988    Speed=3664 wps\n",
      "(85.86%)    Perplexity=92.774    Speed=3666 wps\n",
      "(86.51%)    Perplexity=92.550    Speed=3670 wps\n",
      "(87.15%)    Perplexity=92.358    Speed=3673 wps\n",
      "(87.80%)    Perplexity=92.281    Speed=3676 wps\n",
      "(88.44%)    Perplexity=92.190    Speed=3677 wps\n",
      "(89.09%)    Perplexity=92.208    Speed=3679 wps\n",
      "(89.74%)    Perplexity=92.307    Speed=3665 wps\n",
      "(90.38%)    Perplexity=92.255    Speed=3656 wps\n",
      "(91.03%)    Perplexity=92.246    Speed=3660 wps\n",
      "(91.67%)    Perplexity=92.086    Speed=3663 wps\n",
      "(92.32%)    Perplexity=92.007    Speed=3666 wps\n",
      "(92.96%)    Perplexity=91.941    Speed=3670 wps\n",
      "(93.61%)    Perplexity=91.753    Speed=3673 wps\n",
      "(94.25%)    Perplexity=91.643    Speed=3667 wps\n",
      "(94.90%)    Perplexity=91.518    Speed=3658 wps\n",
      "(95.55%)    Perplexity=91.431    Speed=3661 wps\n",
      "(96.19%)    Perplexity=91.364    Speed=3664 wps\n",
      "(96.84%)    Perplexity=91.390    Speed=3667 wps\n",
      "(97.48%)    Perplexity=91.370    Speed=3670 wps\n",
      "(98.13%)    Perplexity=91.374    Speed=3673 wps\n",
      "(98.77%)    Perplexity=91.343    Speed=3673 wps\n",
      "(99.42%)    Perplexity=91.356    Speed=3665 wps\n",
      "Epoch 4 : Train Perplexity: 91.353\n",
      "Epoch 4 : Valid Perplexity: 128.081\n",
      "Epoch 5 : Learning rate: 0.385\n",
      "(0.00%)    Perplexity=133.439    Speed=2995 wps\n",
      "(0.65%)    Perplexity=99.369    Speed=3959 wps\n",
      "(1.29%)    Perplexity=93.413    Speed=3947 wps\n",
      "(1.94%)    Perplexity=87.236    Speed=3938 wps\n",
      "(2.58%)    Perplexity=84.533    Speed=3456 wps\n",
      "(3.23%)    Perplexity=86.017    Speed=3241 wps\n",
      "(3.87%)    Perplexity=85.184    Speed=3371 wps\n",
      "(4.52%)    Perplexity=84.116    Speed=3473 wps\n",
      "(5.16%)    Perplexity=85.144    Speed=3551 wps\n",
      "(5.81%)    Perplexity=86.432    Speed=3615 wps\n",
      "(6.46%)    Perplexity=86.584    Speed=3665 wps\n",
      "(7.10%)    Perplexity=88.348    Speed=3606 wps\n",
      "(7.75%)    Perplexity=89.890    Speed=3515 wps\n",
      "(8.39%)    Perplexity=90.229    Speed=3525 wps\n",
      "(9.04%)    Perplexity=90.751    Speed=3552 wps\n",
      "(9.68%)    Perplexity=91.455    Speed=3577 wps\n",
      "(10.33%)    Perplexity=91.577    Speed=3610 wps\n",
      "(10.97%)    Perplexity=92.346    Speed=3641 wps\n",
      "(11.62%)    Perplexity=92.235    Speed=3665 wps\n",
      "(12.27%)    Perplexity=91.847    Speed=3678 wps\n",
      "(12.91%)    Perplexity=91.488    Speed=3689 wps\n",
      "(13.56%)    Perplexity=91.290    Speed=3612 wps\n",
      "(14.20%)    Perplexity=90.771    Speed=3543 wps\n",
      "(14.85%)    Perplexity=90.850    Speed=3567 wps\n",
      "(15.49%)    Perplexity=90.529    Speed=3590 wps\n",
      "(16.14%)    Perplexity=90.597    Speed=3611 wps\n",
      "(16.79%)    Perplexity=90.897    Speed=3631 wps\n",
      "(17.43%)    Perplexity=90.863    Speed=3649 wps\n",
      "(18.08%)    Perplexity=90.511    Speed=3626 wps\n",
      "(18.72%)    Perplexity=90.888    Speed=3586 wps\n",
      "(19.37%)    Perplexity=90.968    Speed=3572 wps\n",
      "(20.01%)    Perplexity=90.935    Speed=3588 wps\n",
      "(20.66%)    Perplexity=90.398    Speed=3602 wps\n",
      "(21.30%)    Perplexity=90.110    Speed=3619 wps\n",
      "(21.95%)    Perplexity=89.788    Speed=3634 wps\n",
      "(22.60%)    Perplexity=89.389    Speed=3650 wps\n",
      "(23.24%)    Perplexity=89.250    Speed=3625 wps\n",
      "(23.89%)    Perplexity=89.482    Speed=3593 wps\n",
      "(24.53%)    Perplexity=89.612    Speed=3601 wps\n",
      "(25.18%)    Perplexity=89.409    Speed=3608 wps\n",
      "(25.82%)    Perplexity=89.158    Speed=3618 wps\n",
      "(26.47%)    Perplexity=89.118    Speed=3631 wps\n",
      "(27.11%)    Perplexity=88.764    Speed=3642 wps\n",
      "(27.76%)    Perplexity=88.603    Speed=3642 wps\n",
      "(28.41%)    Perplexity=88.424    Speed=3642 wps\n",
      "(29.05%)    Perplexity=88.473    Speed=3646 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29.70%)    Perplexity=88.116    Speed=3654 wps\n",
      "(30.34%)    Perplexity=87.772    Speed=3663 wps\n",
      "(30.99%)    Perplexity=87.551    Speed=3670 wps\n",
      "(31.63%)    Perplexity=87.439    Speed=3675 wps\n",
      "(32.28%)    Perplexity=87.174    Speed=3681 wps\n",
      "(32.92%)    Perplexity=87.121    Speed=3648 wps\n",
      "(33.57%)    Perplexity=86.789    Speed=3619 wps\n",
      "(34.22%)    Perplexity=86.646    Speed=3630 wps\n",
      "(34.86%)    Perplexity=86.327    Speed=3640 wps\n",
      "(35.51%)    Perplexity=85.858    Speed=3649 wps\n",
      "(36.15%)    Perplexity=85.240    Speed=3659 wps\n",
      "(36.80%)    Perplexity=84.732    Speed=3669 wps\n",
      "(37.44%)    Perplexity=84.729    Speed=3661 wps\n",
      "(38.09%)    Perplexity=84.633    Speed=3639 wps\n",
      "(38.73%)    Perplexity=84.560    Speed=3640 wps\n",
      "(39.38%)    Perplexity=84.572    Speed=3649 wps\n",
      "(40.03%)    Perplexity=84.577    Speed=3657 wps\n",
      "(40.67%)    Perplexity=84.459    Speed=3666 wps\n",
      "(41.32%)    Perplexity=84.430    Speed=3674 wps\n",
      "(41.96%)    Perplexity=84.282    Speed=3682 wps\n",
      "(42.61%)    Perplexity=84.177    Speed=3690 wps\n",
      "(43.25%)    Perplexity=83.974    Speed=3693 wps\n",
      "(43.90%)    Perplexity=83.931    Speed=3674 wps\n",
      "(44.54%)    Perplexity=84.214    Speed=3664 wps\n",
      "(45.19%)    Perplexity=84.352    Speed=3672 wps\n",
      "(45.84%)    Perplexity=84.379    Speed=3679 wps\n",
      "(46.48%)    Perplexity=84.309    Speed=3687 wps\n",
      "(47.13%)    Perplexity=84.284    Speed=3694 wps\n",
      "(47.77%)    Perplexity=84.161    Speed=3701 wps\n",
      "(48.42%)    Perplexity=83.933    Speed=3693 wps\n",
      "(49.06%)    Perplexity=83.876    Speed=3676 wps\n",
      "(49.71%)    Perplexity=83.884    Speed=3675 wps\n",
      "(50.36%)    Perplexity=83.927    Speed=3678 wps\n",
      "(51.00%)    Perplexity=84.129    Speed=3682 wps\n",
      "(51.65%)    Perplexity=84.125    Speed=3689 wps\n",
      "(52.29%)    Perplexity=84.075    Speed=3695 wps\n",
      "(52.94%)    Perplexity=84.076    Speed=3701 wps\n",
      "(53.58%)    Perplexity=83.989    Speed=3704 wps\n",
      "(54.23%)    Perplexity=83.913    Speed=3707 wps\n",
      "(54.87%)    Perplexity=83.912    Speed=3691 wps\n",
      "(55.52%)    Perplexity=83.998    Speed=3668 wps\n",
      "(56.17%)    Perplexity=83.991    Speed=3674 wps\n",
      "(56.81%)    Perplexity=84.028    Speed=3680 wps\n",
      "(57.46%)    Perplexity=83.935    Speed=3686 wps\n",
      "(58.10%)    Perplexity=83.839    Speed=3692 wps\n",
      "(58.75%)    Perplexity=83.553    Speed=3697 wps\n",
      "(59.39%)    Perplexity=83.260    Speed=3695 wps\n",
      "(60.04%)    Perplexity=83.062    Speed=3681 wps\n",
      "(60.68%)    Perplexity=82.916    Speed=3677 wps\n",
      "(61.33%)    Perplexity=82.774    Speed=3679 wps\n",
      "(61.98%)    Perplexity=82.506    Speed=3682 wps\n",
      "(62.62%)    Perplexity=82.288    Speed=3670 wps\n",
      "(63.27%)    Perplexity=82.213    Speed=3659 wps\n",
      "(63.91%)    Perplexity=82.143    Speed=3662 wps\n",
      "(64.56%)    Perplexity=82.003    Speed=3665 wps\n",
      "(65.20%)    Perplexity=81.860    Speed=3669 wps\n",
      "(65.85%)    Perplexity=81.985    Speed=3674 wps\n",
      "(66.49%)    Perplexity=82.016    Speed=3679 wps\n",
      "(67.14%)    Perplexity=82.069    Speed=3682 wps\n",
      "(67.79%)    Perplexity=82.109    Speed=3684 wps\n",
      "(68.43%)    Perplexity=82.058    Speed=3686 wps\n",
      "(69.08%)    Perplexity=82.032    Speed=3691 wps\n",
      "(69.72%)    Perplexity=82.076    Speed=3696 wps\n",
      "(70.37%)    Perplexity=82.023    Speed=3700 wps\n",
      "(71.01%)    Perplexity=81.990    Speed=3702 wps\n",
      "(71.66%)    Perplexity=82.061    Speed=3704 wps\n",
      "(72.30%)    Perplexity=81.953    Speed=3708 wps\n",
      "(72.95%)    Perplexity=81.851    Speed=3712 wps\n",
      "(73.60%)    Perplexity=81.709    Speed=3716 wps\n",
      "(74.24%)    Perplexity=81.561    Speed=3719 wps\n",
      "(74.89%)    Perplexity=81.421    Speed=3721 wps\n",
      "(75.53%)    Perplexity=81.395    Speed=3723 wps\n",
      "(76.18%)    Perplexity=81.300    Speed=3727 wps\n",
      "(76.82%)    Perplexity=81.374    Speed=3732 wps\n",
      "(77.47%)    Perplexity=81.406    Speed=3735 wps\n",
      "(78.11%)    Perplexity=81.446    Speed=3737 wps\n",
      "(78.76%)    Perplexity=81.430    Speed=3739 wps\n",
      "(79.41%)    Perplexity=81.502    Speed=3731 wps\n",
      "(80.05%)    Perplexity=81.521    Speed=3712 wps\n",
      "(80.70%)    Perplexity=81.561    Speed=3714 wps\n",
      "(81.34%)    Perplexity=81.570    Speed=3718 wps\n",
      "(81.99%)    Perplexity=81.479    Speed=3722 wps\n",
      "(82.63%)    Perplexity=81.446    Speed=3726 wps\n",
      "(83.28%)    Perplexity=81.358    Speed=3730 wps\n",
      "(83.93%)    Perplexity=81.136    Speed=3730 wps\n",
      "(84.57%)    Perplexity=80.946    Speed=3720 wps\n",
      "(85.22%)    Perplexity=80.651    Speed=3715 wps\n",
      "(85.86%)    Perplexity=80.473    Speed=3716 wps\n",
      "(86.51%)    Perplexity=80.283    Speed=3718 wps\n",
      "(87.15%)    Perplexity=80.120    Speed=3711 wps\n",
      "(87.80%)    Perplexity=80.057    Speed=3701 wps\n",
      "(88.44%)    Perplexity=79.987    Speed=3704 wps\n",
      "(89.09%)    Perplexity=80.009    Speed=3707 wps\n",
      "(89.74%)    Perplexity=80.108    Speed=3711 wps\n",
      "(90.38%)    Perplexity=80.068    Speed=3714 wps\n",
      "(91.03%)    Perplexity=80.067    Speed=3718 wps\n",
      "(91.67%)    Perplexity=79.933    Speed=3718 wps\n",
      "(92.32%)    Perplexity=79.868    Speed=3709 wps\n",
      "(92.96%)    Perplexity=79.818    Speed=3704 wps\n",
      "(93.61%)    Perplexity=79.658    Speed=3706 wps\n",
      "(94.25%)    Perplexity=79.563    Speed=3708 wps\n",
      "(94.90%)    Perplexity=79.463    Speed=3711 wps\n",
      "(95.55%)    Perplexity=79.390    Speed=3714 wps\n",
      "(96.19%)    Perplexity=79.338    Speed=3718 wps\n",
      "(96.84%)    Perplexity=79.368    Speed=3720 wps\n",
      "(97.48%)    Perplexity=79.357    Speed=3721 wps\n",
      "(98.13%)    Perplexity=79.366    Speed=3723 wps\n",
      "(98.77%)    Perplexity=79.343    Speed=3726 wps\n",
      "(99.42%)    Perplexity=79.360    Speed=3730 wps\n",
      "Epoch 5 : Train Perplexity: 79.366\n",
      "Epoch 5 : Valid Perplexity: 125.775\n",
      "Epoch 6 : Learning rate: 0.346\n",
      "(0.00%)    Perplexity=122.128    Speed=3103 wps\n",
      "(0.65%)    Perplexity=86.943    Speed=4146 wps\n",
      "(1.29%)    Perplexity=82.253    Speed=4217 wps\n",
      "(1.94%)    Perplexity=77.017    Speed=4240 wps\n",
      "(2.58%)    Perplexity=74.482    Speed=4256 wps\n",
      "(3.23%)    Perplexity=75.769    Speed=4267 wps\n",
      "(3.87%)    Perplexity=75.121    Speed=3980 wps\n",
      "(4.52%)    Perplexity=74.211    Speed=3743 wps\n",
      "(5.16%)    Perplexity=75.132    Speed=3757 wps\n",
      "(5.81%)    Perplexity=76.228    Speed=3780 wps\n",
      "(6.46%)    Perplexity=76.408    Speed=3808 wps\n",
      "(7.10%)    Perplexity=77.999    Speed=3847 wps\n",
      "(7.75%)    Perplexity=79.427    Speed=3878 wps\n",
      "(8.39%)    Perplexity=79.786    Speed=3895 wps\n",
      "(9.04%)    Perplexity=80.269    Speed=3898 wps\n",
      "(9.68%)    Perplexity=80.908    Speed=3901 wps\n",
      "(10.33%)    Perplexity=81.003    Speed=3764 wps\n",
      "(10.97%)    Perplexity=81.697    Speed=3673 wps\n",
      "(11.62%)    Perplexity=81.580    Speed=3701 wps\n",
      "(12.27%)    Perplexity=81.238    Speed=3727 wps\n",
      "(12.91%)    Perplexity=80.938    Speed=3751 wps\n",
      "(13.56%)    Perplexity=80.777    Speed=3772 wps\n",
      "(14.20%)    Perplexity=80.330    Speed=3789 wps\n",
      "(14.85%)    Perplexity=80.372    Speed=3754 wps\n",
      "(15.49%)    Perplexity=80.106    Speed=3697 wps\n",
      "(16.14%)    Perplexity=80.167    Speed=3704 wps\n",
      "(16.79%)    Perplexity=80.414    Speed=3725 wps\n",
      "(17.43%)    Perplexity=80.390    Speed=3743 wps\n",
      "(18.08%)    Perplexity=80.089    Speed=3760 wps\n",
      "(18.72%)    Perplexity=80.430    Speed=3776 wps\n",
      "(19.37%)    Perplexity=80.517    Speed=3785 wps\n",
      "(20.01%)    Perplexity=80.491    Speed=3740 wps\n",
      "(20.66%)    Perplexity=80.038    Speed=3710 wps\n",
      "(21.30%)    Perplexity=79.777    Speed=3713 wps\n",
      "(21.95%)    Perplexity=79.507    Speed=3721 wps\n",
      "(22.60%)    Perplexity=79.157    Speed=3698 wps\n",
      "(23.24%)    Perplexity=79.022    Speed=3662 wps\n",
      "(23.89%)    Perplexity=79.233    Speed=3668 wps\n",
      "(24.53%)    Perplexity=79.332    Speed=3683 wps\n",
      "(25.18%)    Perplexity=79.173    Speed=3696 wps\n",
      "(25.82%)    Perplexity=78.953    Speed=3710 wps\n",
      "(26.47%)    Perplexity=78.930    Speed=3722 wps\n",
      "(27.11%)    Perplexity=78.603    Speed=3729 wps\n",
      "(27.76%)    Perplexity=78.470    Speed=3698 wps\n",
      "(28.41%)    Perplexity=78.312    Speed=3678 wps\n",
      "(29.05%)    Perplexity=78.347    Speed=3684 wps\n",
      "(29.70%)    Perplexity=78.013    Speed=3691 wps\n",
      "(30.34%)    Perplexity=77.702    Speed=3700 wps\n",
      "(30.99%)    Perplexity=77.504    Speed=3711 wps\n",
      "(31.63%)    Perplexity=77.417    Speed=3721 wps\n",
      "(32.28%)    Perplexity=77.191    Speed=3727 wps\n",
      "(32.92%)    Perplexity=77.150    Speed=3732 wps\n",
      "(33.57%)    Perplexity=76.869    Speed=3731 wps\n",
      "(34.22%)    Perplexity=76.739    Speed=3688 wps\n",
      "(34.86%)    Perplexity=76.459    Speed=3674 wps\n",
      "(35.51%)    Perplexity=76.039    Speed=3684 wps\n",
      "(36.15%)    Perplexity=75.500    Speed=3693 wps\n",
      "(36.80%)    Perplexity=75.051    Speed=3702 wps\n",
      "(37.44%)    Perplexity=75.047    Speed=3711 wps\n",
      "(38.09%)    Perplexity=74.970    Speed=3720 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38.73%)    Perplexity=74.909    Speed=3700 wps\n",
      "(39.38%)    Perplexity=74.918    Speed=3679 wps\n",
      "(40.03%)    Perplexity=74.913    Speed=3684 wps\n",
      "(40.67%)    Perplexity=74.824    Speed=3689 wps\n",
      "(41.32%)    Perplexity=74.806    Speed=3683 wps\n",
      "(41.96%)    Perplexity=74.688    Speed=3664 wps\n",
      "(42.61%)    Perplexity=74.597    Speed=3658 wps\n",
      "(43.25%)    Perplexity=74.426    Speed=3663 wps\n",
      "(43.90%)    Perplexity=74.386    Speed=3667 wps\n",
      "(44.54%)    Perplexity=74.641    Speed=3674 wps\n",
      "(45.19%)    Perplexity=74.757    Speed=3678 wps\n",
      "(45.84%)    Perplexity=74.789    Speed=3685 wps\n",
      "(46.48%)    Perplexity=74.726    Speed=3687 wps\n",
      "(47.13%)    Perplexity=74.709    Speed=3686 wps\n",
      "(47.77%)    Perplexity=74.597    Speed=3666 wps\n",
      "(48.42%)    Perplexity=74.398    Speed=3641 wps\n",
      "(49.06%)    Perplexity=74.358    Speed=3648 wps\n",
      "(49.71%)    Perplexity=74.371    Speed=3656 wps\n",
      "(50.36%)    Perplexity=74.410    Speed=3662 wps\n",
      "(51.00%)    Perplexity=74.587    Speed=3669 wps\n",
      "(51.65%)    Perplexity=74.587    Speed=3676 wps\n",
      "(52.29%)    Perplexity=74.550    Speed=3674 wps\n",
      "(52.94%)    Perplexity=74.557    Speed=3659 wps\n",
      "(53.58%)    Perplexity=74.486    Speed=3654 wps\n",
      "(54.23%)    Perplexity=74.421    Speed=3658 wps\n",
      "(54.87%)    Perplexity=74.430    Speed=3661 wps\n",
      "(55.52%)    Perplexity=74.509    Speed=3667 wps\n",
      "(56.17%)    Perplexity=74.507    Speed=3673 wps\n",
      "(56.81%)    Perplexity=74.543    Speed=3678 wps\n",
      "(57.46%)    Perplexity=74.460    Speed=3681 wps\n",
      "(58.10%)    Perplexity=74.372    Speed=3684 wps\n",
      "(58.75%)    Perplexity=74.118    Speed=3689 wps\n",
      "(59.39%)    Perplexity=73.861    Speed=3694 wps\n",
      "(60.04%)    Perplexity=73.693    Speed=3700 wps\n",
      "(60.68%)    Perplexity=73.563    Speed=3705 wps\n",
      "(61.33%)    Perplexity=73.444    Speed=3710 wps\n",
      "(61.98%)    Perplexity=73.212    Speed=3712 wps\n",
      "(62.62%)    Perplexity=73.027    Speed=3715 wps\n",
      "(63.27%)    Perplexity=72.970    Speed=3718 wps\n",
      "(63.91%)    Perplexity=72.906    Speed=3723 wps\n",
      "(64.56%)    Perplexity=72.776    Speed=3728 wps\n",
      "(65.20%)    Perplexity=72.650    Speed=3732 wps\n",
      "(65.85%)    Perplexity=72.761    Speed=3735 wps\n",
      "(66.49%)    Perplexity=72.788    Speed=3737 wps\n",
      "(67.14%)    Perplexity=72.841    Speed=3720 wps\n",
      "(67.79%)    Perplexity=72.883    Speed=3704 wps\n",
      "(68.43%)    Perplexity=72.837    Speed=3709 wps\n",
      "(69.08%)    Perplexity=72.820    Speed=3714 wps\n",
      "(69.72%)    Perplexity=72.865    Speed=3719 wps\n",
      "(70.37%)    Perplexity=72.821    Speed=3724 wps\n",
      "(71.01%)    Perplexity=72.800    Speed=3728 wps\n",
      "(71.66%)    Perplexity=72.870    Speed=3724 wps\n",
      "(72.30%)    Perplexity=72.777    Speed=3712 wps\n",
      "(72.95%)    Perplexity=72.687    Speed=3710 wps\n",
      "(73.60%)    Perplexity=72.558    Speed=3712 wps\n",
      "(74.24%)    Perplexity=72.426    Speed=3714 wps\n",
      "(74.89%)    Perplexity=72.299    Speed=3719 wps\n",
      "(75.53%)    Perplexity=72.273    Speed=3723 wps\n",
      "(76.18%)    Perplexity=72.190    Speed=3727 wps\n",
      "(76.82%)    Perplexity=72.259    Speed=3729 wps\n",
      "(77.47%)    Perplexity=72.289    Speed=3731 wps\n",
      "(78.11%)    Perplexity=72.326    Speed=3722 wps\n",
      "(78.76%)    Perplexity=72.312    Speed=3703 wps\n",
      "(79.41%)    Perplexity=72.377    Speed=3707 wps\n",
      "(80.05%)    Perplexity=72.399    Speed=3711 wps\n",
      "(80.70%)    Perplexity=72.436    Speed=3715 wps\n",
      "(81.34%)    Perplexity=72.449    Speed=3719 wps\n",
      "(81.99%)    Perplexity=72.376    Speed=3723 wps\n",
      "(82.63%)    Perplexity=72.350    Speed=3724 wps\n",
      "(83.28%)    Perplexity=72.270    Speed=3714 wps\n",
      "(83.93%)    Perplexity=72.079    Speed=3708 wps\n",
      "(84.57%)    Perplexity=71.909    Speed=3710 wps\n",
      "(85.22%)    Perplexity=71.649    Speed=3712 wps\n",
      "(85.86%)    Perplexity=71.496    Speed=3715 wps\n",
      "(86.51%)    Perplexity=71.322    Speed=3719 wps\n",
      "(87.15%)    Perplexity=71.180    Speed=3722 wps\n",
      "(87.80%)    Perplexity=71.121    Speed=3724 wps\n",
      "(88.44%)    Perplexity=71.064    Speed=3726 wps\n",
      "(89.09%)    Perplexity=71.086    Speed=3728 wps\n",
      "(89.74%)    Perplexity=71.176    Speed=3732 wps\n",
      "(90.38%)    Perplexity=71.146    Speed=3735 wps\n",
      "(91.03%)    Perplexity=71.147    Speed=3738 wps\n",
      "(91.67%)    Perplexity=71.033    Speed=3740 wps\n",
      "(92.32%)    Perplexity=70.975    Speed=3741 wps\n",
      "(92.96%)    Perplexity=70.932    Speed=3731 wps\n",
      "(93.61%)    Perplexity=70.788    Speed=3717 wps\n",
      "(94.25%)    Perplexity=70.708    Speed=3721 wps\n",
      "(94.90%)    Perplexity=70.618    Speed=3724 wps\n",
      "(95.55%)    Perplexity=70.559    Speed=3728 wps\n",
      "(96.19%)    Perplexity=70.516    Speed=3731 wps\n",
      "(96.84%)    Perplexity=70.544    Speed=3735 wps\n",
      "(97.48%)    Perplexity=70.532    Speed=3733 wps\n",
      "(98.13%)    Perplexity=70.543    Speed=3724 wps\n",
      "(98.77%)    Perplexity=70.523    Speed=3721 wps\n",
      "(99.42%)    Perplexity=70.542    Speed=3723 wps\n",
      "Epoch 6 : Train Perplexity: 70.553\n",
      "Epoch 6 : Valid Perplexity: 126.507\n",
      "Epoch 7 : Learning rate: 0.308\n",
      "(0.00%)    Perplexity=110.689    Speed=2734 wps\n",
      "(0.65%)    Perplexity=77.977    Speed=3836 wps\n",
      "(1.29%)    Perplexity=74.076    Speed=4005 wps\n",
      "(1.94%)    Perplexity=69.417    Speed=4094 wps\n",
      "(2.58%)    Perplexity=67.029    Speed=4134 wps\n",
      "(3.23%)    Perplexity=68.162    Speed=4100 wps\n",
      "(3.87%)    Perplexity=67.553    Speed=4079 wps\n",
      "(4.52%)    Perplexity=66.804    Speed=4078 wps\n",
      "(5.16%)    Perplexity=67.666    Speed=4101 wps\n",
      "(5.81%)    Perplexity=68.610    Speed=4118 wps\n",
      "(6.46%)    Perplexity=68.765    Speed=4119 wps\n",
      "(7.10%)    Perplexity=70.206    Speed=4106 wps\n",
      "(7.75%)    Perplexity=71.476    Speed=4090 wps\n",
      "(8.39%)    Perplexity=71.840    Speed=3885 wps\n",
      "(9.04%)    Perplexity=72.321    Speed=3757 wps\n",
      "(9.68%)    Perplexity=72.904    Speed=3788 wps\n",
      "(10.33%)    Perplexity=72.992    Speed=3817 wps\n",
      "(10.97%)    Perplexity=73.608    Speed=3842 wps\n",
      "(11.62%)    Perplexity=73.491    Speed=3862 wps\n",
      "(12.27%)    Perplexity=73.189    Speed=3881 wps\n",
      "(12.91%)    Perplexity=72.900    Speed=3841 wps\n",
      "(13.56%)    Perplexity=72.758    Speed=3769 wps\n",
      "(14.20%)    Perplexity=72.352    Speed=3770 wps\n",
      "(14.85%)    Perplexity=72.355    Speed=3791 wps\n",
      "(15.49%)    Perplexity=72.130    Speed=3810 wps\n",
      "(16.14%)    Perplexity=72.195    Speed=3827 wps\n",
      "(16.79%)    Perplexity=72.430    Speed=3844 wps\n",
      "(17.43%)    Perplexity=72.422    Speed=3857 wps\n",
      "(18.08%)    Perplexity=72.139    Speed=3803 wps\n",
      "(18.72%)    Perplexity=72.443    Speed=3763 wps\n",
      "(19.37%)    Perplexity=72.524    Speed=3770 wps\n",
      "(20.01%)    Perplexity=72.496    Speed=3776 wps\n",
      "(20.66%)    Perplexity=72.101    Speed=3787 wps\n",
      "(21.30%)    Perplexity=71.869    Speed=3801 wps\n",
      "(21.95%)    Perplexity=71.629    Speed=3814 wps\n",
      "(22.60%)    Perplexity=71.336    Speed=3822 wps\n",
      "(23.24%)    Perplexity=71.206    Speed=3826 wps\n",
      "(23.89%)    Perplexity=71.405    Speed=3830 wps\n",
      "(24.53%)    Perplexity=71.499    Speed=3765 wps\n",
      "(25.18%)    Perplexity=71.363    Speed=3737 wps\n",
      "(25.82%)    Perplexity=71.173    Speed=3750 wps\n",
      "(26.47%)    Perplexity=71.169    Speed=3763 wps\n",
      "(27.11%)    Perplexity=70.879    Speed=3774 wps\n",
      "(27.76%)    Perplexity=70.770    Speed=3785 wps\n",
      "(28.41%)    Perplexity=70.643    Speed=3796 wps\n",
      "(29.05%)    Perplexity=70.673    Speed=3776 wps\n",
      "(29.70%)    Perplexity=70.355    Speed=3745 wps\n",
      "(30.34%)    Perplexity=70.068    Speed=3746 wps\n",
      "(30.99%)    Perplexity=69.879    Speed=3750 wps\n",
      "(31.63%)    Perplexity=69.800    Speed=3756 wps\n",
      "(32.28%)    Perplexity=69.602    Speed=3766 wps\n",
      "(32.92%)    Perplexity=69.565    Speed=3775 wps\n",
      "(33.57%)    Perplexity=69.304    Speed=3782 wps\n",
      "(34.22%)    Perplexity=69.176    Speed=3786 wps\n",
      "(34.86%)    Perplexity=68.933    Speed=3789 wps\n",
      "(35.51%)    Perplexity=68.555    Speed=3759 wps\n",
      "(36.15%)    Perplexity=68.073    Speed=3724 wps\n",
      "(36.80%)    Perplexity=67.671    Speed=3733 wps\n",
      "(37.44%)    Perplexity=67.667    Speed=3742 wps\n",
      "(38.09%)    Perplexity=67.600    Speed=3750 wps\n",
      "(38.73%)    Perplexity=67.546    Speed=3758 wps\n",
      "(39.38%)    Perplexity=67.550    Speed=3766 wps\n",
      "(40.03%)    Perplexity=67.545    Speed=3760 wps\n",
      "(40.67%)    Perplexity=67.474    Speed=3738 wps\n",
      "(41.32%)    Perplexity=67.454    Speed=3732 wps\n",
      "(41.96%)    Perplexity=67.348    Speed=3736 wps\n",
      "(42.61%)    Perplexity=67.269    Speed=3739 wps\n",
      "(43.25%)    Perplexity=67.115    Speed=3746 wps\n",
      "(43.90%)    Perplexity=67.085    Speed=3753 wps\n",
      "(44.54%)    Perplexity=67.308    Speed=3760 wps\n",
      "(45.19%)    Perplexity=67.409    Speed=3763 wps\n",
      "(45.84%)    Perplexity=67.432    Speed=3766 wps\n",
      "(46.48%)    Perplexity=67.378    Speed=3770 wps\n",
      "(47.13%)    Perplexity=67.361    Speed=3777 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47.77%)    Perplexity=67.261    Speed=3783 wps\n",
      "(48.42%)    Perplexity=67.081    Speed=3787 wps\n",
      "(49.06%)    Perplexity=67.055    Speed=3790 wps\n",
      "(49.71%)    Perplexity=67.070    Speed=3792 wps\n",
      "(50.36%)    Perplexity=67.108    Speed=3763 wps\n",
      "(51.00%)    Perplexity=67.263    Speed=3745 wps\n",
      "(51.65%)    Perplexity=67.262    Speed=3752 wps\n",
      "(52.29%)    Perplexity=67.238    Speed=3758 wps\n",
      "(52.94%)    Perplexity=67.243    Speed=3764 wps\n",
      "(53.58%)    Perplexity=67.180    Speed=3770 wps\n",
      "(54.23%)    Perplexity=67.122    Speed=3775 wps\n",
      "(54.87%)    Perplexity=67.130    Speed=3767 wps\n",
      "(55.52%)    Perplexity=67.202    Speed=3750 wps\n",
      "(56.17%)    Perplexity=67.202    Speed=3748 wps\n",
      "(56.81%)    Perplexity=67.232    Speed=3751 wps\n",
      "(57.46%)    Perplexity=67.160    Speed=3754 wps\n",
      "(58.10%)    Perplexity=67.081    Speed=3759 wps\n",
      "(58.75%)    Perplexity=66.859    Speed=3764 wps\n",
      "(59.39%)    Perplexity=66.630    Speed=3768 wps\n",
      "(60.04%)    Perplexity=66.474    Speed=3770 wps\n",
      "(60.68%)    Perplexity=66.355    Speed=3773 wps\n",
      "(61.33%)    Perplexity=66.257    Speed=3777 wps\n",
      "(61.98%)    Perplexity=66.052    Speed=3781 wps\n",
      "(62.62%)    Perplexity=65.887    Speed=3786 wps\n",
      "(63.27%)    Perplexity=65.841    Speed=3789 wps\n",
      "(63.91%)    Perplexity=65.783    Speed=3790 wps\n",
      "(64.56%)    Perplexity=65.662    Speed=3785 wps\n",
      "(65.20%)    Perplexity=65.550    Speed=3760 wps\n",
      "(65.85%)    Perplexity=65.646    Speed=3754 wps\n",
      "(66.49%)    Perplexity=65.670    Speed=3759 wps\n",
      "(67.14%)    Perplexity=65.717    Speed=3763 wps\n",
      "(67.79%)    Perplexity=65.758    Speed=3768 wps\n",
      "(68.43%)    Perplexity=65.713    Speed=3772 wps\n",
      "(69.08%)    Perplexity=65.696    Speed=3777 wps\n",
      "(69.72%)    Perplexity=65.743    Speed=3764 wps\n",
      "(70.37%)    Perplexity=65.701    Speed=3753 wps\n",
      "(71.01%)    Perplexity=65.684    Speed=3755 wps\n",
      "(71.66%)    Perplexity=65.747    Speed=3757 wps\n",
      "(72.30%)    Perplexity=65.662    Speed=3760 wps\n",
      "(72.95%)    Perplexity=65.577    Speed=3764 wps\n",
      "(73.60%)    Perplexity=65.454    Speed=3768 wps\n",
      "(74.24%)    Perplexity=65.333    Speed=3771 wps\n",
      "(74.89%)    Perplexity=65.218    Speed=3773 wps\n",
      "(75.53%)    Perplexity=65.194    Speed=3774 wps\n",
      "(76.18%)    Perplexity=65.118    Speed=3755 wps\n",
      "(76.82%)    Perplexity=65.183    Speed=3744 wps\n",
      "(77.47%)    Perplexity=65.208    Speed=3748 wps\n",
      "(78.11%)    Perplexity=65.246    Speed=3752 wps\n",
      "(78.76%)    Perplexity=65.231    Speed=3756 wps\n",
      "(79.41%)    Perplexity=65.287    Speed=3760 wps\n",
      "(80.05%)    Perplexity=65.309    Speed=3764 wps\n",
      "(80.70%)    Perplexity=65.342    Speed=3757 wps\n",
      "(81.34%)    Perplexity=65.355    Speed=3746 wps\n",
      "(81.99%)    Perplexity=65.294    Speed=3748 wps\n",
      "(82.63%)    Perplexity=65.270    Speed=3752 wps\n",
      "(83.28%)    Perplexity=65.197    Speed=3755 wps\n",
      "(83.93%)    Perplexity=65.024    Speed=3759 wps\n",
      "(84.57%)    Perplexity=64.872    Speed=3763 wps\n",
      "(85.22%)    Perplexity=64.640    Speed=3765 wps\n",
      "(85.86%)    Perplexity=64.504    Speed=3754 wps\n",
      "(86.51%)    Perplexity=64.348    Speed=3748 wps\n",
      "(87.15%)    Perplexity=64.222    Speed=3749 wps\n",
      "(87.80%)    Perplexity=64.166    Speed=3751 wps\n",
      "(88.44%)    Perplexity=64.113    Speed=3754 wps\n",
      "(89.09%)    Perplexity=64.133    Speed=3757 wps\n",
      "(89.74%)    Perplexity=64.215    Speed=3761 wps\n",
      "(90.38%)    Perplexity=64.189    Speed=3763 wps\n",
      "(91.03%)    Perplexity=64.191    Speed=3764 wps\n",
      "(91.67%)    Perplexity=64.090    Speed=3764 wps\n",
      "(92.32%)    Perplexity=64.037    Speed=3747 wps\n",
      "(92.96%)    Perplexity=63.999    Speed=3741 wps\n",
      "(93.61%)    Perplexity=63.869    Speed=3745 wps\n",
      "(94.25%)    Perplexity=63.797    Speed=3748 wps\n",
      "(94.90%)    Perplexity=63.718    Speed=3752 wps\n",
      "(95.55%)    Perplexity=63.667    Speed=3755 wps\n",
      "(96.19%)    Perplexity=63.629    Speed=3758 wps\n",
      "(96.84%)    Perplexity=63.654    Speed=3752 wps\n",
      "(97.48%)    Perplexity=63.643    Speed=3743 wps\n",
      "(98.13%)    Perplexity=63.656    Speed=3744 wps\n",
      "(98.77%)    Perplexity=63.636    Speed=3745 wps\n",
      "(99.42%)    Perplexity=63.653    Speed=3747 wps\n",
      "Epoch 7 : Train Perplexity: 63.663\n",
      "Epoch 7 : Valid Perplexity: 127.341\n",
      "Epoch 8 : Learning rate: 0.269\n",
      "(0.00%)    Perplexity=100.871    Speed=3014 wps\n",
      "(0.65%)    Perplexity=70.915    Speed=4139 wps\n",
      "(1.29%)    Perplexity=67.785    Speed=4222 wps\n",
      "(1.94%)    Perplexity=63.561    Speed=4188 wps\n",
      "(2.58%)    Perplexity=61.332    Speed=4122 wps\n",
      "(3.23%)    Perplexity=62.345    Speed=4094 wps\n",
      "(3.87%)    Perplexity=61.739    Speed=3758 wps\n",
      "(4.52%)    Perplexity=61.071    Speed=3494 wps\n",
      "(5.16%)    Perplexity=61.918    Speed=3575 wps\n",
      "(5.81%)    Perplexity=62.729    Speed=3642 wps\n",
      "(6.46%)    Perplexity=62.877    Speed=3698 wps\n",
      "(7.10%)    Perplexity=64.150    Speed=3743 wps\n",
      "(7.75%)    Perplexity=65.303    Speed=3781 wps\n",
      "(8.39%)    Perplexity=65.651    Speed=3746 wps\n",
      "(9.04%)    Perplexity=66.110    Speed=3651 wps\n",
      "(9.68%)    Perplexity=66.627    Speed=3633 wps\n",
      "(10.33%)    Perplexity=66.712    Speed=3651 wps\n",
      "(10.97%)    Perplexity=67.260    Speed=3667 wps\n",
      "(11.62%)    Perplexity=67.147    Speed=3600 wps\n",
      "(12.27%)    Perplexity=66.893    Speed=3554 wps\n",
      "(12.91%)    Perplexity=66.616    Speed=3572 wps\n",
      "(13.56%)    Perplexity=66.483    Speed=3589 wps\n",
      "(14.20%)    Perplexity=66.109    Speed=3610 wps\n",
      "(14.85%)    Perplexity=66.097    Speed=3634 wps\n",
      "(15.49%)    Perplexity=65.886    Speed=3656 wps\n",
      "(16.14%)    Perplexity=65.943    Speed=3670 wps\n",
      "(16.79%)    Perplexity=66.154    Speed=3681 wps\n",
      "(17.43%)    Perplexity=66.137    Speed=3683 wps\n",
      "(18.08%)    Perplexity=65.867    Speed=3605 wps\n",
      "(18.72%)    Perplexity=66.141    Speed=3579 wps\n",
      "(19.37%)    Perplexity=66.226    Speed=3598 wps\n",
      "(20.01%)    Perplexity=66.203    Speed=3617 wps\n",
      "(20.66%)    Perplexity=65.856    Speed=3635 wps\n",
      "(21.30%)    Perplexity=65.649    Speed=3652 wps\n",
      "(21.95%)    Perplexity=65.427    Speed=3668 wps\n",
      "(22.60%)    Perplexity=65.165    Speed=3640 wps\n",
      "(23.24%)    Perplexity=65.048    Speed=3608 wps\n",
      "(23.89%)    Perplexity=65.237    Speed=3623 wps\n",
      "(24.53%)    Perplexity=65.323    Speed=3637 wps\n",
      "(25.18%)    Perplexity=65.193    Speed=3652 wps\n",
      "(25.82%)    Perplexity=65.023    Speed=3666 wps\n",
      "(26.47%)    Perplexity=65.028    Speed=3680 wps\n",
      "(27.11%)    Perplexity=64.755    Speed=3680 wps\n",
      "(27.76%)    Perplexity=64.664    Speed=3651 wps\n",
      "(28.41%)    Perplexity=64.545    Speed=3641 wps\n",
      "(29.05%)    Perplexity=64.564    Speed=3654 wps\n",
      "(29.70%)    Perplexity=64.266    Speed=3666 wps\n",
      "(30.34%)    Perplexity=64.000    Speed=3677 wps\n",
      "(30.99%)    Perplexity=63.823    Speed=3689 wps\n",
      "(31.63%)    Perplexity=63.750    Speed=3700 wps\n",
      "(32.28%)    Perplexity=63.575    Speed=3684 wps\n",
      "(32.92%)    Perplexity=63.546    Speed=3660 wps\n",
      "(33.57%)    Perplexity=63.311    Speed=3662 wps\n",
      "(34.22%)    Perplexity=63.181    Speed=3667 wps\n",
      "(34.86%)    Perplexity=62.962    Speed=3674 wps\n",
      "(35.51%)    Perplexity=62.608    Speed=3683 wps\n",
      "(36.15%)    Perplexity=62.173    Speed=3693 wps\n",
      "(36.80%)    Perplexity=61.804    Speed=3701 wps\n",
      "(37.44%)    Perplexity=61.798    Speed=3705 wps\n",
      "(38.09%)    Perplexity=61.738    Speed=3710 wps\n",
      "(38.73%)    Perplexity=61.681    Speed=3684 wps\n",
      "(39.38%)    Perplexity=61.683    Speed=3655 wps\n",
      "(40.03%)    Perplexity=61.673    Speed=3664 wps\n",
      "(40.67%)    Perplexity=61.608    Speed=3673 wps\n",
      "(41.32%)    Perplexity=61.593    Speed=3681 wps\n",
      "(41.96%)    Perplexity=61.500    Speed=3690 wps\n",
      "(42.61%)    Perplexity=61.428    Speed=3698 wps\n",
      "(43.25%)    Perplexity=61.288    Speed=3694 wps\n",
      "(43.90%)    Perplexity=61.265    Speed=3675 wps\n",
      "(44.54%)    Perplexity=61.467    Speed=3670 wps\n",
      "(45.19%)    Perplexity=61.558    Speed=3674 wps\n",
      "(45.84%)    Perplexity=61.572    Speed=3677 wps\n",
      "(46.48%)    Perplexity=61.521    Speed=3661 wps\n",
      "(47.13%)    Perplexity=61.508    Speed=3647 wps\n",
      "(47.77%)    Perplexity=61.412    Speed=3654 wps\n",
      "(48.42%)    Perplexity=61.252    Speed=3662 wps\n",
      "(49.06%)    Perplexity=61.231    Speed=3669 wps\n",
      "(49.71%)    Perplexity=61.248    Speed=3675 wps\n",
      "(50.36%)    Perplexity=61.281    Speed=3682 wps\n",
      "(51.00%)    Perplexity=61.425    Speed=3679 wps\n",
      "(51.65%)    Perplexity=61.425    Speed=3664 wps\n",
      "(52.29%)    Perplexity=61.406    Speed=3659 wps\n",
      "(52.94%)    Perplexity=61.410    Speed=3663 wps\n",
      "(53.58%)    Perplexity=61.359    Speed=3667 wps\n",
      "(54.23%)    Perplexity=61.304    Speed=3673 wps\n",
      "(54.87%)    Perplexity=61.311    Speed=3679 wps\n",
      "(55.52%)    Perplexity=61.378    Speed=3685 wps\n",
      "(56.17%)    Perplexity=61.378    Speed=3688 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56.81%)    Perplexity=61.406    Speed=3692 wps\n",
      "(57.46%)    Perplexity=61.343    Speed=3683 wps\n",
      "(58.10%)    Perplexity=61.271    Speed=3658 wps\n",
      "(58.75%)    Perplexity=61.067    Speed=3660 wps\n",
      "(59.39%)    Perplexity=60.864    Speed=3666 wps\n",
      "(60.04%)    Perplexity=60.722    Speed=3672 wps\n",
      "(60.68%)    Perplexity=60.614    Speed=3678 wps\n",
      "(61.33%)    Perplexity=60.528    Speed=3683 wps\n",
      "(61.98%)    Perplexity=60.340    Speed=3686 wps\n",
      "(62.62%)    Perplexity=60.188    Speed=3673 wps\n",
      "(63.27%)    Perplexity=60.146    Speed=3666 wps\n",
      "(63.91%)    Perplexity=60.090    Speed=3671 wps\n",
      "(64.56%)    Perplexity=59.980    Speed=3677 wps\n",
      "(65.20%)    Perplexity=59.875    Speed=3682 wps\n",
      "(65.85%)    Perplexity=59.956    Speed=3688 wps\n",
      "(66.49%)    Perplexity=59.975    Speed=3693 wps\n",
      "(67.14%)    Perplexity=60.021    Speed=3688 wps\n",
      "(67.79%)    Perplexity=60.061    Speed=3676 wps\n",
      "(68.43%)    Perplexity=60.017    Speed=3676 wps\n",
      "(69.08%)    Perplexity=60.001    Speed=3681 wps\n",
      "(69.72%)    Perplexity=60.044    Speed=3686 wps\n",
      "(70.37%)    Perplexity=60.003    Speed=3691 wps\n",
      "(71.01%)    Perplexity=59.985    Speed=3696 wps\n",
      "(71.66%)    Perplexity=60.044    Speed=3700 wps\n",
      "(72.30%)    Perplexity=59.965    Speed=3689 wps\n",
      "(72.95%)    Perplexity=59.884    Speed=3680 wps\n",
      "(73.60%)    Perplexity=59.766    Speed=3682 wps\n",
      "(74.24%)    Perplexity=59.656    Speed=3685 wps\n",
      "(74.89%)    Perplexity=59.550    Speed=3688 wps\n",
      "(75.53%)    Perplexity=59.524    Speed=3693 wps\n",
      "(76.18%)    Perplexity=59.452    Speed=3697 wps\n",
      "(76.82%)    Perplexity=59.512    Speed=3700 wps\n",
      "(77.47%)    Perplexity=59.535    Speed=3703 wps\n",
      "(78.11%)    Perplexity=59.570    Speed=3705 wps\n",
      "(78.76%)    Perplexity=59.553    Speed=3686 wps\n",
      "(79.41%)    Perplexity=59.604    Speed=3678 wps\n",
      "(80.05%)    Perplexity=59.622    Speed=3683 wps\n",
      "(80.70%)    Perplexity=59.652    Speed=3686 wps\n",
      "(81.34%)    Perplexity=59.665    Speed=3690 wps\n",
      "(81.99%)    Perplexity=59.611    Speed=3694 wps\n",
      "(82.63%)    Perplexity=59.587    Speed=3698 wps\n",
      "(83.28%)    Perplexity=59.522    Speed=3692 wps\n",
      "(83.93%)    Perplexity=59.361    Speed=3682 wps\n",
      "(84.57%)    Perplexity=59.220    Speed=3683 wps\n",
      "(85.22%)    Perplexity=59.008    Speed=3685 wps\n",
      "(85.86%)    Perplexity=58.881    Speed=3683 wps\n",
      "(86.51%)    Perplexity=58.740    Speed=3674 wps\n",
      "(87.15%)    Perplexity=58.624    Speed=3670 wps\n",
      "(87.80%)    Perplexity=58.570    Speed=3672 wps\n",
      "(88.44%)    Perplexity=58.520    Speed=3674 wps\n",
      "(89.09%)    Perplexity=58.537    Speed=3677 wps\n",
      "(89.74%)    Perplexity=58.610    Speed=3681 wps\n",
      "(90.38%)    Perplexity=58.586    Speed=3684 wps\n",
      "(91.03%)    Perplexity=58.588    Speed=3686 wps\n",
      "(91.67%)    Perplexity=58.498    Speed=3688 wps\n",
      "(92.32%)    Perplexity=58.448    Speed=3690 wps\n",
      "(92.96%)    Perplexity=58.413    Speed=3694 wps\n",
      "(93.61%)    Perplexity=58.293    Speed=3698 wps\n",
      "(94.25%)    Perplexity=58.228    Speed=3700 wps\n",
      "(94.90%)    Perplexity=58.154    Speed=3702 wps\n",
      "(95.55%)    Perplexity=58.108    Speed=3704 wps\n",
      "(96.19%)    Perplexity=58.072    Speed=3707 wps\n",
      "(96.84%)    Perplexity=58.095    Speed=3711 wps\n",
      "(97.48%)    Perplexity=58.084    Speed=3713 wps\n",
      "(98.13%)    Perplexity=58.095    Speed=3715 wps\n",
      "(98.77%)    Perplexity=58.073    Speed=3716 wps\n",
      "(99.42%)    Perplexity=58.090    Speed=3719 wps\n",
      "Epoch 8 : Train Perplexity: 58.100\n",
      "Epoch 8 : Valid Perplexity: 129.435\n",
      "Epoch 9 : Learning rate: 0.231\n",
      "(0.00%)    Perplexity=93.105    Speed=1527 wps\n",
      "(0.65%)    Perplexity=65.206    Speed=2211 wps\n",
      "(1.29%)    Perplexity=62.561    Speed=2600 wps\n",
      "(1.94%)    Perplexity=58.733    Speed=2982 wps\n",
      "(2.58%)    Perplexity=56.552    Speed=3226 wps\n",
      "(3.23%)    Perplexity=57.460    Speed=3393 wps\n",
      "(3.87%)    Perplexity=56.938    Speed=3514 wps\n",
      "(4.52%)    Perplexity=56.340    Speed=3608 wps\n",
      "(5.16%)    Perplexity=57.105    Speed=3491 wps\n",
      "(5.81%)    Perplexity=57.833    Speed=3399 wps\n",
      "(6.46%)    Perplexity=58.013    Speed=3449 wps\n",
      "(7.10%)    Perplexity=59.167    Speed=3490 wps\n",
      "(7.75%)    Perplexity=60.235    Speed=3533 wps\n",
      "(8.39%)    Perplexity=60.546    Speed=3583 wps\n",
      "(9.04%)    Perplexity=60.965    Speed=3626 wps\n",
      "(9.68%)    Perplexity=61.441    Speed=3655 wps\n",
      "(10.33%)    Perplexity=61.520    Speed=3675 wps\n",
      "(10.97%)    Perplexity=62.016    Speed=3690 wps\n",
      "(11.62%)    Perplexity=61.899    Speed=3584 wps\n",
      "(12.27%)    Perplexity=61.658    Speed=3527 wps\n",
      "(12.91%)    Perplexity=61.418    Speed=3559 wps\n",
      "(13.56%)    Perplexity=61.319    Speed=3589 wps\n",
      "(14.20%)    Perplexity=60.966    Speed=3617 wps\n",
      "(14.85%)    Perplexity=60.953    Speed=3642 wps\n",
      "(15.49%)    Perplexity=60.765    Speed=3666 wps\n",
      "(16.14%)    Perplexity=60.812    Speed=3643 wps\n",
      "(16.79%)    Perplexity=61.007    Speed=3598 wps\n",
      "(17.43%)    Perplexity=61.007    Speed=3598 wps\n",
      "(18.08%)    Perplexity=60.749    Speed=3611 wps\n",
      "(18.72%)    Perplexity=61.012    Speed=3623 wps\n",
      "(19.37%)    Perplexity=61.087    Speed=3643 wps\n",
      "(20.01%)    Perplexity=61.070    Speed=3661 wps\n",
      "(20.66%)    Perplexity=60.751    Speed=3675 wps\n",
      "(21.30%)    Perplexity=60.550    Speed=3684 wps\n",
      "(21.95%)    Perplexity=60.352    Speed=3692 wps\n",
      "(22.60%)    Perplexity=60.118    Speed=3705 wps\n",
      "(23.24%)    Perplexity=60.014    Speed=3719 wps\n",
      "(23.89%)    Perplexity=60.191    Speed=3733 wps\n",
      "(24.53%)    Perplexity=60.267    Speed=3740 wps\n",
      "(25.18%)    Perplexity=60.152    Speed=3746 wps\n",
      "(25.82%)    Perplexity=59.993    Speed=3746 wps\n",
      "(26.47%)    Perplexity=59.991    Speed=3690 wps\n",
      "(27.11%)    Perplexity=59.731    Speed=3672 wps\n",
      "(27.76%)    Perplexity=59.651    Speed=3685 wps\n",
      "(28.41%)    Perplexity=59.546    Speed=3697 wps\n",
      "(29.05%)    Perplexity=59.559    Speed=3708 wps\n",
      "(29.70%)    Perplexity=59.283    Speed=3720 wps\n",
      "(30.34%)    Perplexity=59.038    Speed=3731 wps\n",
      "(30.99%)    Perplexity=58.875    Speed=3710 wps\n",
      "(31.63%)    Perplexity=58.806    Speed=3684 wps\n",
      "(32.28%)    Perplexity=58.646    Speed=3693 wps\n",
      "(32.92%)    Perplexity=58.614    Speed=3703 wps\n",
      "(33.57%)    Perplexity=58.402    Speed=3714 wps\n",
      "(34.22%)    Perplexity=58.277    Speed=3724 wps\n",
      "(34.86%)    Perplexity=58.082    Speed=3733 wps\n",
      "(35.51%)    Perplexity=57.752    Speed=3734 wps\n",
      "(36.15%)    Perplexity=57.356    Speed=3710 wps\n",
      "(36.80%)    Perplexity=57.008    Speed=3698 wps\n",
      "(37.44%)    Perplexity=56.997    Speed=3702 wps\n",
      "(38.09%)    Perplexity=56.943    Speed=3706 wps\n",
      "(38.73%)    Perplexity=56.886    Speed=3714 wps\n",
      "(39.38%)    Perplexity=56.891    Speed=3722 wps\n",
      "(40.03%)    Perplexity=56.877    Speed=3730 wps\n",
      "(40.67%)    Perplexity=56.820    Speed=3734 wps\n",
      "(41.32%)    Perplexity=56.806    Speed=3738 wps\n",
      "(41.96%)    Perplexity=56.722    Speed=3742 wps\n",
      "(42.61%)    Perplexity=56.656    Speed=3750 wps\n",
      "(43.25%)    Perplexity=56.527    Speed=3756 wps\n",
      "(43.90%)    Perplexity=56.512    Speed=3760 wps\n",
      "(44.54%)    Perplexity=56.695    Speed=3763 wps\n",
      "(45.19%)    Perplexity=56.778    Speed=3766 wps\n",
      "(45.84%)    Perplexity=56.790    Speed=3741 wps\n",
      "(46.48%)    Perplexity=56.746    Speed=3717 wps\n",
      "(47.13%)    Perplexity=56.734    Speed=3724 wps\n",
      "(47.77%)    Perplexity=56.642    Speed=3730 wps\n",
      "(48.42%)    Perplexity=56.500    Speed=3737 wps\n",
      "(49.06%)    Perplexity=56.481    Speed=3743 wps\n",
      "(49.71%)    Perplexity=56.495    Speed=3749 wps\n",
      "(50.36%)    Perplexity=56.523    Speed=3743 wps\n",
      "(51.00%)    Perplexity=56.653    Speed=3726 wps\n",
      "(51.65%)    Perplexity=56.657    Speed=3723 wps\n",
      "(52.29%)    Perplexity=56.642    Speed=3725 wps\n",
      "(52.94%)    Perplexity=56.644    Speed=3728 wps\n",
      "(53.58%)    Perplexity=56.597    Speed=3734 wps\n",
      "(54.23%)    Perplexity=56.550    Speed=3740 wps\n",
      "(54.87%)    Perplexity=56.560    Speed=3746 wps\n",
      "(55.52%)    Perplexity=56.622    Speed=3748 wps\n",
      "(56.17%)    Perplexity=56.625    Speed=3750 wps\n",
      "(56.81%)    Perplexity=56.645    Speed=3754 wps\n",
      "(57.46%)    Perplexity=56.584    Speed=3760 wps\n",
      "(58.10%)    Perplexity=56.518    Speed=3765 wps\n",
      "(58.75%)    Perplexity=56.332    Speed=3768 wps\n",
      "(59.39%)    Perplexity=56.141    Speed=3770 wps\n",
      "(60.04%)    Perplexity=56.011    Speed=3772 wps\n",
      "(60.68%)    Perplexity=55.910    Speed=3746 wps\n",
      "(61.33%)    Perplexity=55.831    Speed=3734 wps\n",
      "(61.98%)    Perplexity=55.657    Speed=3739 wps\n",
      "(62.62%)    Perplexity=55.516    Speed=3744 wps\n",
      "(63.27%)    Perplexity=55.478    Speed=3749 wps\n",
      "(63.91%)    Perplexity=55.423    Speed=3754 wps\n",
      "(64.56%)    Perplexity=55.320    Speed=3759 wps\n",
      "(65.20%)    Perplexity=55.218    Speed=3748 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65.85%)    Perplexity=55.290    Speed=3735 wps\n",
      "(66.49%)    Perplexity=55.309    Speed=3736 wps\n",
      "(67.14%)    Perplexity=55.351    Speed=3739 wps\n",
      "(67.79%)    Perplexity=55.386    Speed=3742 wps\n",
      "(68.43%)    Perplexity=55.342    Speed=3746 wps\n",
      "(69.08%)    Perplexity=55.327    Speed=3750 wps\n",
      "(69.72%)    Perplexity=55.366    Speed=3754 wps\n",
      "(70.37%)    Perplexity=55.327    Speed=3755 wps\n",
      "(71.01%)    Perplexity=55.312    Speed=3757 wps\n",
      "(71.66%)    Perplexity=55.369    Speed=3741 wps\n",
      "(72.30%)    Perplexity=55.299    Speed=3726 wps\n",
      "(72.95%)    Perplexity=55.222    Speed=3731 wps\n",
      "(73.60%)    Perplexity=55.110    Speed=3735 wps\n",
      "(74.24%)    Perplexity=55.008    Speed=3740 wps\n",
      "(74.89%)    Perplexity=54.910    Speed=3744 wps\n",
      "(75.53%)    Perplexity=54.884    Speed=3748 wps\n",
      "(76.18%)    Perplexity=54.817    Speed=3743 wps\n",
      "(76.82%)    Perplexity=54.868    Speed=3732 wps\n",
      "(77.47%)    Perplexity=54.886    Speed=3730 wps\n",
      "(78.11%)    Perplexity=54.918    Speed=3732 wps\n",
      "(78.76%)    Perplexity=54.901    Speed=3733 wps\n",
      "(79.41%)    Perplexity=54.945    Speed=3722 wps\n",
      "(80.05%)    Perplexity=54.959    Speed=3714 wps\n",
      "(80.70%)    Perplexity=54.984    Speed=3718 wps\n",
      "(81.34%)    Perplexity=54.998    Speed=3722 wps\n",
      "(81.99%)    Perplexity=54.948    Speed=3726 wps\n",
      "(82.63%)    Perplexity=54.928    Speed=3730 wps\n",
      "(83.28%)    Perplexity=54.866    Speed=3734 wps\n",
      "(83.93%)    Perplexity=54.719    Speed=3730 wps\n",
      "(84.57%)    Perplexity=54.588    Speed=3720 wps\n",
      "(85.22%)    Perplexity=54.394    Speed=3718 wps\n",
      "(85.86%)    Perplexity=54.274    Speed=3719 wps\n",
      "(86.51%)    Perplexity=54.144    Speed=3721 wps\n",
      "(87.15%)    Perplexity=54.037    Speed=3725 wps\n",
      "(87.80%)    Perplexity=53.986    Speed=3728 wps\n",
      "(88.44%)    Perplexity=53.938    Speed=3732 wps\n",
      "(89.09%)    Perplexity=53.954    Speed=3733 wps\n",
      "(89.74%)    Perplexity=54.019    Speed=3735 wps\n",
      "(90.38%)    Perplexity=53.993    Speed=3737 wps\n",
      "(91.03%)    Perplexity=53.993    Speed=3741 wps\n",
      "(91.67%)    Perplexity=53.911    Speed=3744 wps\n",
      "(92.32%)    Perplexity=53.863    Speed=3745 wps\n",
      "(92.96%)    Perplexity=53.830    Speed=3747 wps\n",
      "(93.61%)    Perplexity=53.720    Speed=3747 wps\n",
      "(94.25%)    Perplexity=53.660    Speed=3731 wps\n",
      "(94.90%)    Perplexity=53.590    Speed=3725 wps\n",
      "(95.55%)    Perplexity=53.550    Speed=3728 wps\n",
      "(96.19%)    Perplexity=53.514    Speed=3731 wps\n",
      "(96.84%)    Perplexity=53.535    Speed=3734 wps\n",
      "(97.48%)    Perplexity=53.523    Speed=3737 wps\n",
      "(98.13%)    Perplexity=53.533    Speed=3740 wps\n",
      "(98.77%)    Perplexity=53.511    Speed=3733 wps\n",
      "(99.42%)    Perplexity=53.525    Speed=3723 wps\n",
      "Epoch 9 : Train Perplexity: 53.536\n",
      "Epoch 9 : Valid Perplexity: 132.022\n",
      "Epoch 10 : Learning rate: 0.192\n",
      "(0.00%)    Perplexity=87.183    Speed=3045 wps\n",
      "(0.65%)    Perplexity=60.769    Speed=3826 wps\n",
      "(1.29%)    Perplexity=58.287    Speed=3868 wps\n",
      "(1.94%)    Perplexity=54.825    Speed=3923 wps\n",
      "(2.58%)    Perplexity=52.787    Speed=4006 wps\n",
      "(3.23%)    Perplexity=53.620    Speed=4054 wps\n",
      "(3.87%)    Perplexity=53.156    Speed=4066 wps\n",
      "(4.52%)    Perplexity=52.606    Speed=4054 wps\n",
      "(5.16%)    Perplexity=53.379    Speed=4044 wps\n",
      "(5.81%)    Perplexity=54.022    Speed=3771 wps\n",
      "(6.46%)    Perplexity=54.167    Speed=3626 wps\n",
      "(7.10%)    Perplexity=55.240    Speed=3680 wps\n",
      "(7.75%)    Perplexity=56.236    Speed=3726 wps\n",
      "(8.39%)    Perplexity=56.540    Speed=3765 wps\n",
      "(9.04%)    Perplexity=56.946    Speed=3799 wps\n",
      "(9.68%)    Perplexity=57.371    Speed=3830 wps\n",
      "(10.33%)    Perplexity=57.434    Speed=3785 wps\n",
      "(10.97%)    Perplexity=57.911    Speed=3702 wps\n",
      "(11.62%)    Perplexity=57.809    Speed=3696 wps\n",
      "(12.27%)    Perplexity=57.559    Speed=3710 wps\n",
      "(12.91%)    Perplexity=57.333    Speed=3715 wps\n",
      "(13.56%)    Perplexity=57.235    Speed=3653 wps\n",
      "(14.20%)    Perplexity=56.904    Speed=3616 wps\n",
      "(14.85%)    Perplexity=56.894    Speed=3630 wps\n",
      "(15.49%)    Perplexity=56.717    Speed=3644 wps\n",
      "(16.14%)    Perplexity=56.757    Speed=3663 wps\n",
      "(16.79%)    Perplexity=56.934    Speed=3683 wps\n",
      "(17.43%)    Perplexity=56.928    Speed=3703 wps\n",
      "(18.08%)    Perplexity=56.680    Speed=3715 wps\n",
      "(18.72%)    Perplexity=56.922    Speed=3724 wps\n",
      "(19.37%)    Perplexity=56.987    Speed=3733 wps\n",
      "(20.01%)    Perplexity=56.966    Speed=3749 wps\n",
      "(20.66%)    Perplexity=56.670    Speed=3764 wps\n",
      "(21.30%)    Perplexity=56.481    Speed=3777 wps\n",
      "(21.95%)    Perplexity=56.298    Speed=3783 wps\n",
      "(22.60%)    Perplexity=56.087    Speed=3788 wps\n",
      "(23.24%)    Perplexity=55.991    Speed=3752 wps\n",
      "(23.89%)    Perplexity=56.158    Speed=3691 wps\n",
      "(24.53%)    Perplexity=56.216    Speed=3704 wps\n",
      "(25.18%)    Perplexity=56.102    Speed=3718 wps\n",
      "(25.82%)    Perplexity=55.952    Speed=3731 wps\n",
      "(26.47%)    Perplexity=55.946    Speed=3743 wps\n",
      "(27.11%)    Perplexity=55.699    Speed=3755 wps\n",
      "(27.76%)    Perplexity=55.630    Speed=3754 wps\n",
      "(28.41%)    Perplexity=55.540    Speed=3723 wps\n",
      "(29.05%)    Perplexity=55.542    Speed=3709 wps\n",
      "(29.70%)    Perplexity=55.278    Speed=3714 wps\n",
      "(30.34%)    Perplexity=55.048    Speed=3718 wps\n",
      "(30.99%)    Perplexity=54.893    Speed=3728 wps\n",
      "(31.63%)    Perplexity=54.823    Speed=3738 wps\n",
      "(32.28%)    Perplexity=54.671    Speed=3748 wps\n",
      "(32.92%)    Perplexity=54.639    Speed=3752 wps\n",
      "(33.57%)    Perplexity=54.435    Speed=3756 wps\n",
      "(34.22%)    Perplexity=54.309    Speed=3762 wps\n",
      "(34.86%)    Perplexity=54.125    Speed=3771 wps\n",
      "(35.51%)    Perplexity=53.810    Speed=3779 wps\n",
      "(36.15%)    Perplexity=53.440    Speed=3786 wps\n",
      "(36.80%)    Perplexity=53.116    Speed=3789 wps\n",
      "(37.44%)    Perplexity=53.104    Speed=3791 wps\n",
      "(38.09%)    Perplexity=53.051    Speed=3797 wps\n",
      "(38.73%)    Perplexity=52.991    Speed=3804 wps\n",
      "(39.38%)    Perplexity=52.994    Speed=3811 wps\n",
      "(40.03%)    Perplexity=52.976    Speed=3815 wps\n",
      "(40.67%)    Perplexity=52.926    Speed=3817 wps\n",
      "(41.32%)    Perplexity=52.914    Speed=3809 wps\n",
      "(41.96%)    Perplexity=52.835    Speed=3771 wps\n",
      "(42.61%)    Perplexity=52.772    Speed=3765 wps\n",
      "(43.25%)    Perplexity=52.652    Speed=3772 wps\n",
      "(43.90%)    Perplexity=52.639    Speed=3779 wps\n",
      "(44.54%)    Perplexity=52.809    Speed=3786 wps\n",
      "(45.19%)    Perplexity=52.887    Speed=3792 wps\n",
      "(45.84%)    Perplexity=52.900    Speed=3799 wps\n",
      "(46.48%)    Perplexity=52.855    Speed=3779 wps\n",
      "(47.13%)    Perplexity=52.841    Speed=3763 wps\n",
      "(47.77%)    Perplexity=52.754    Speed=3765 wps\n",
      "(48.42%)    Perplexity=52.621    Speed=3768 wps\n",
      "(49.06%)    Perplexity=52.605    Speed=3761 wps\n",
      "(49.71%)    Perplexity=52.615    Speed=3742 wps\n",
      "(50.36%)    Perplexity=52.641    Speed=3738 wps\n",
      "(51.00%)    Perplexity=52.763    Speed=3741 wps\n",
      "(51.65%)    Perplexity=52.769    Speed=3744 wps\n",
      "(52.29%)    Perplexity=52.756    Speed=3750 wps\n",
      "(52.94%)    Perplexity=52.756    Speed=3756 wps\n",
      "(53.58%)    Perplexity=52.708    Speed=3761 wps\n",
      "(54.23%)    Perplexity=52.664    Speed=3763 wps\n",
      "(54.87%)    Perplexity=52.672    Speed=3765 wps\n",
      "(55.52%)    Perplexity=52.729    Speed=3752 wps\n",
      "(56.17%)    Perplexity=52.733    Speed=3726 wps\n",
      "(56.81%)    Perplexity=52.747    Speed=3710 wps\n",
      "(57.46%)    Perplexity=52.691    Speed=3698 wps\n",
      "(58.10%)    Perplexity=52.626    Speed=3704 wps\n",
      "(58.75%)    Perplexity=52.453    Speed=3709 wps\n",
      "(59.39%)    Perplexity=52.275    Speed=3715 wps\n",
      "(60.04%)    Perplexity=52.153    Speed=3721 wps\n",
      "(60.68%)    Perplexity=52.058    Speed=3726 wps\n",
      "(61.33%)    Perplexity=51.985    Speed=3723 wps\n",
      "(61.98%)    Perplexity=51.823    Speed=3710 wps\n",
      "(62.62%)    Perplexity=51.690    Speed=3705 wps\n",
      "(63.27%)    Perplexity=51.655    Speed=3708 wps\n",
      "(63.91%)    Perplexity=51.603    Speed=3710 wps\n",
      "(64.56%)    Perplexity=51.508    Speed=3716 wps\n",
      "(65.20%)    Perplexity=51.411    Speed=3720 wps\n",
      "(65.85%)    Perplexity=51.475    Speed=3725 wps\n",
      "(66.49%)    Perplexity=51.491    Speed=3728 wps\n",
      "(67.14%)    Perplexity=51.529    Speed=3730 wps\n",
      "(67.79%)    Perplexity=51.557    Speed=3723 wps\n",
      "(68.43%)    Perplexity=51.515    Speed=3701 wps\n",
      "(69.08%)    Perplexity=51.499    Speed=3702 wps\n",
      "(69.72%)    Perplexity=51.529    Speed=3707 wps\n",
      "(70.37%)    Perplexity=51.493    Speed=3712 wps\n",
      "(71.01%)    Perplexity=51.478    Speed=3716 wps\n",
      "(71.66%)    Perplexity=51.531    Speed=3721 wps\n",
      "(72.30%)    Perplexity=51.464    Speed=3724 wps\n",
      "(72.95%)    Perplexity=51.390    Speed=3712 wps\n",
      "(73.60%)    Perplexity=51.280    Speed=3704 wps\n",
      "(74.24%)    Perplexity=51.185    Speed=3706 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74.89%)    Perplexity=51.092    Speed=3709 wps\n",
      "(75.53%)    Perplexity=51.067    Speed=3712 wps\n",
      "(76.18%)    Perplexity=51.003    Speed=3717 wps\n",
      "(76.82%)    Perplexity=51.049    Speed=3721 wps\n",
      "(77.47%)    Perplexity=51.064    Speed=3724 wps\n",
      "(78.11%)    Perplexity=51.090    Speed=3726 wps\n",
      "(78.76%)    Perplexity=51.072    Speed=3726 wps\n",
      "(79.41%)    Perplexity=51.111    Speed=3707 wps\n",
      "(80.05%)    Perplexity=51.123    Speed=3701 wps\n",
      "(80.70%)    Perplexity=51.145    Speed=3705 wps\n",
      "(81.34%)    Perplexity=51.159    Speed=3709 wps\n",
      "(81.99%)    Perplexity=51.114    Speed=3713 wps\n",
      "(82.63%)    Perplexity=51.094    Speed=3717 wps\n",
      "(83.28%)    Perplexity=51.036    Speed=3721 wps\n",
      "(83.93%)    Perplexity=50.899    Speed=3713 wps\n",
      "(84.57%)    Perplexity=50.776    Speed=3703 wps\n",
      "(85.22%)    Perplexity=50.597    Speed=3704 wps\n",
      "(85.86%)    Perplexity=50.483    Speed=3706 wps\n",
      "(86.51%)    Perplexity=50.359    Speed=3709 wps\n",
      "(87.15%)    Perplexity=50.258    Speed=3713 wps\n",
      "(87.80%)    Perplexity=50.208    Speed=3716 wps\n",
      "(88.44%)    Perplexity=50.160    Speed=3719 wps\n",
      "(89.09%)    Perplexity=50.172    Speed=3721 wps\n",
      "(89.74%)    Perplexity=50.229    Speed=3723 wps\n",
      "(90.38%)    Perplexity=50.203    Speed=3726 wps\n",
      "(91.03%)    Perplexity=50.202    Speed=3730 wps\n",
      "(91.67%)    Perplexity=50.124    Speed=3733 wps\n",
      "(92.32%)    Perplexity=50.079    Speed=3735 wps\n",
      "(92.96%)    Perplexity=50.047    Speed=3737 wps\n",
      "(93.61%)    Perplexity=49.945    Speed=3738 wps\n",
      "(94.25%)    Perplexity=49.889    Speed=3739 wps\n",
      "(94.90%)    Perplexity=49.824    Speed=3736 wps\n",
      "(95.55%)    Perplexity=49.784    Speed=3720 wps\n",
      "(96.19%)    Perplexity=49.749    Speed=3718 wps\n",
      "(96.84%)    Perplexity=49.768    Speed=3721 wps\n",
      "(97.48%)    Perplexity=49.755    Speed=3724 wps\n",
      "(98.13%)    Perplexity=49.764    Speed=3728 wps\n",
      "(98.77%)    Perplexity=49.741    Speed=3731 wps\n",
      "(99.42%)    Perplexity=49.752    Speed=3734 wps\n",
      "Epoch 10 : Train Perplexity: 49.761\n",
      "Epoch 10 : Valid Perplexity: 134.884\n",
      "Epoch 11 : Learning rate: 0.154\n",
      "(0.00%)    Perplexity=84.593    Speed=2736 wps\n",
      "(0.65%)    Perplexity=57.367    Speed=3900 wps\n",
      "(1.29%)    Perplexity=54.948    Speed=4083 wps\n",
      "(1.94%)    Perplexity=51.696    Speed=4157 wps\n",
      "(2.58%)    Perplexity=49.732    Speed=4148 wps\n",
      "(3.23%)    Perplexity=50.493    Speed=4111 wps\n",
      "(3.87%)    Perplexity=50.046    Speed=4091 wps\n",
      "(4.52%)    Perplexity=49.515    Speed=4108 wps\n",
      "(5.16%)    Perplexity=50.261    Speed=4130 wps\n",
      "(5.81%)    Perplexity=50.835    Speed=4149 wps\n",
      "(6.46%)    Perplexity=50.979    Speed=4133 wps\n",
      "(7.10%)    Perplexity=52.006    Speed=4119 wps\n",
      "(7.75%)    Perplexity=52.939    Speed=4029 wps\n",
      "(8.39%)    Perplexity=53.233    Speed=3812 wps\n",
      "(9.04%)    Perplexity=53.628    Speed=3789 wps\n",
      "(9.68%)    Perplexity=54.048    Speed=3821 wps\n",
      "(10.33%)    Perplexity=54.094    Speed=3849 wps\n",
      "(10.97%)    Perplexity=54.542    Speed=3873 wps\n",
      "(11.62%)    Perplexity=54.446    Speed=3895 wps\n",
      "(12.27%)    Perplexity=54.191    Speed=3913 wps\n",
      "(12.91%)    Perplexity=53.983    Speed=3832 wps\n",
      "(13.56%)    Perplexity=53.890    Speed=3776 wps\n",
      "(14.20%)    Perplexity=53.579    Speed=3774 wps\n",
      "(14.85%)    Perplexity=53.578    Speed=3770 wps\n",
      "(15.49%)    Perplexity=53.411    Speed=3727 wps\n",
      "(16.14%)    Perplexity=53.454    Speed=3675 wps\n",
      "(16.79%)    Perplexity=53.617    Speed=3686 wps\n",
      "(17.43%)    Perplexity=53.618    Speed=3705 wps\n",
      "(18.08%)    Perplexity=53.383    Speed=3723 wps\n",
      "(18.72%)    Perplexity=53.616    Speed=3739 wps\n",
      "(19.37%)    Perplexity=53.685    Speed=3755 wps\n",
      "(20.01%)    Perplexity=53.659    Speed=3759 wps\n",
      "(20.66%)    Perplexity=53.381    Speed=3717 wps\n",
      "(21.30%)    Perplexity=53.204    Speed=3692 wps\n",
      "(21.95%)    Perplexity=53.027    Speed=3698 wps\n",
      "(22.60%)    Perplexity=52.827    Speed=3705 wps\n",
      "(23.24%)    Perplexity=52.738    Speed=3716 wps\n",
      "(23.89%)    Perplexity=52.895    Speed=3729 wps\n",
      "(24.53%)    Perplexity=52.943    Speed=3741 wps\n",
      "(25.18%)    Perplexity=52.834    Speed=3746 wps\n",
      "(25.82%)    Perplexity=52.689    Speed=3750 wps\n",
      "(26.47%)    Perplexity=52.682    Speed=3756 wps\n",
      "(27.11%)    Perplexity=52.442    Speed=3767 wps\n",
      "(27.76%)    Perplexity=52.378    Speed=3777 wps\n",
      "(28.41%)    Perplexity=52.298    Speed=3784 wps\n",
      "(29.05%)    Perplexity=52.294    Speed=3787 wps\n",
      "(29.70%)    Perplexity=52.039    Speed=3790 wps\n",
      "(30.34%)    Perplexity=51.823    Speed=3746 wps\n",
      "(30.99%)    Perplexity=51.672    Speed=3712 wps\n",
      "(31.63%)    Perplexity=51.604    Speed=3722 wps\n",
      "(32.28%)    Perplexity=51.456    Speed=3731 wps\n",
      "(32.92%)    Perplexity=51.421    Speed=3740 wps\n",
      "(33.57%)    Perplexity=51.224    Speed=3749 wps\n",
      "(34.22%)    Perplexity=51.098    Speed=3757 wps\n",
      "(34.86%)    Perplexity=50.918    Speed=3743 wps\n",
      "(35.51%)    Perplexity=50.616    Speed=3718 wps\n",
      "(36.15%)    Perplexity=50.264    Speed=3720 wps\n",
      "(36.80%)    Perplexity=49.951    Speed=3728 wps\n",
      "(37.44%)    Perplexity=49.939    Speed=3736 wps\n",
      "(38.09%)    Perplexity=49.885    Speed=3744 wps\n",
      "(38.73%)    Perplexity=49.827    Speed=3752 wps\n",
      "(39.38%)    Perplexity=49.826    Speed=3755 wps\n",
      "(40.03%)    Perplexity=49.805    Speed=3732 wps\n",
      "(40.67%)    Perplexity=49.755    Speed=3719 wps\n",
      "(41.32%)    Perplexity=49.745    Speed=3722 wps\n",
      "(41.96%)    Perplexity=49.674    Speed=3726 wps\n",
      "(42.61%)    Perplexity=49.613    Speed=3712 wps\n",
      "(43.25%)    Perplexity=49.502    Speed=3693 wps\n",
      "(43.90%)    Perplexity=49.489    Speed=3693 wps\n",
      "(44.54%)    Perplexity=49.646    Speed=3697 wps\n",
      "(45.19%)    Perplexity=49.719    Speed=3701 wps\n",
      "(45.84%)    Perplexity=49.729    Speed=3708 wps\n",
      "(46.48%)    Perplexity=49.686    Speed=3715 wps\n",
      "(47.13%)    Perplexity=49.668    Speed=3720 wps\n",
      "(47.77%)    Perplexity=49.585    Speed=3723 wps\n",
      "(48.42%)    Perplexity=49.456    Speed=3725 wps\n",
      "(49.06%)    Perplexity=49.440    Speed=3731 wps\n",
      "(49.71%)    Perplexity=49.449    Speed=3737 wps\n",
      "(50.36%)    Perplexity=49.474    Speed=3743 wps\n",
      "(51.00%)    Perplexity=49.584    Speed=3746 wps\n",
      "(51.65%)    Perplexity=49.587    Speed=3748 wps\n",
      "(52.29%)    Perplexity=49.576    Speed=3751 wps\n",
      "(52.94%)    Perplexity=49.573    Speed=3756 wps\n",
      "(53.58%)    Perplexity=49.524    Speed=3761 wps\n",
      "(54.23%)    Perplexity=49.482    Speed=3765 wps\n",
      "(54.87%)    Perplexity=49.487    Speed=3767 wps\n",
      "(55.52%)    Perplexity=49.539    Speed=3769 wps\n",
      "(56.17%)    Perplexity=49.543    Speed=3773 wps\n",
      "(56.81%)    Perplexity=49.554    Speed=3778 wps\n",
      "(57.46%)    Perplexity=49.501    Speed=3783 wps\n",
      "(58.10%)    Perplexity=49.437    Speed=3785 wps\n",
      "(58.75%)    Perplexity=49.272    Speed=3786 wps\n",
      "(59.39%)    Perplexity=49.105    Speed=3789 wps\n",
      "(60.04%)    Perplexity=48.989    Speed=3793 wps\n",
      "(60.68%)    Perplexity=48.894    Speed=3797 wps\n",
      "(61.33%)    Perplexity=48.823    Speed=3800 wps\n",
      "(61.98%)    Perplexity=48.673    Speed=3802 wps\n",
      "(62.62%)    Perplexity=48.547    Speed=3803 wps\n",
      "(63.27%)    Perplexity=48.515    Speed=3780 wps\n",
      "(63.91%)    Perplexity=48.463    Speed=3765 wps\n",
      "(64.56%)    Perplexity=48.375    Speed=3770 wps\n",
      "(65.20%)    Perplexity=48.283    Speed=3774 wps\n",
      "(65.85%)    Perplexity=48.339    Speed=3778 wps\n",
      "(66.49%)    Perplexity=48.355    Speed=3782 wps\n",
      "(67.14%)    Perplexity=48.391    Speed=3786 wps\n",
      "(67.79%)    Perplexity=48.415    Speed=3777 wps\n",
      "(68.43%)    Perplexity=48.370    Speed=3764 wps\n",
      "(69.08%)    Perplexity=48.350    Speed=3763 wps\n",
      "(69.72%)    Perplexity=48.376    Speed=3765 wps\n",
      "(70.37%)    Perplexity=48.341    Speed=3763 wps\n",
      "(71.01%)    Perplexity=48.325    Speed=3750 wps\n",
      "(71.66%)    Perplexity=48.376    Speed=3743 wps\n",
      "(72.30%)    Perplexity=48.309    Speed=3745 wps\n",
      "(72.95%)    Perplexity=48.237    Speed=3746 wps\n",
      "(73.60%)    Perplexity=48.129    Speed=3749 wps\n",
      "(74.24%)    Perplexity=48.038    Speed=3753 wps\n",
      "(74.89%)    Perplexity=47.951    Speed=3757 wps\n",
      "(75.53%)    Perplexity=47.925    Speed=3758 wps\n",
      "(76.18%)    Perplexity=47.864    Speed=3760 wps\n",
      "(76.82%)    Perplexity=47.904    Speed=3752 wps\n",
      "(77.47%)    Perplexity=47.915    Speed=3733 wps\n",
      "(78.11%)    Perplexity=47.938    Speed=3733 wps\n",
      "(78.76%)    Perplexity=47.921    Speed=3736 wps\n",
      "(79.41%)    Perplexity=47.953    Speed=3740 wps\n",
      "(80.05%)    Perplexity=47.962    Speed=3744 wps\n",
      "(80.70%)    Perplexity=47.979    Speed=3747 wps\n",
      "(81.34%)    Perplexity=47.992    Speed=3748 wps\n",
      "(81.99%)    Perplexity=47.948    Speed=3737 wps\n",
      "(82.63%)    Perplexity=47.929    Speed=3731 wps\n",
      "(83.28%)    Perplexity=47.874    Speed=3733 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83.93%)    Perplexity=47.746    Speed=3734 wps\n",
      "(84.57%)    Perplexity=47.629    Speed=3737 wps\n",
      "(85.22%)    Perplexity=47.461    Speed=3741 wps\n",
      "(85.86%)    Perplexity=47.353    Speed=3744 wps\n",
      "(86.51%)    Perplexity=47.236    Speed=3746 wps\n",
      "(87.15%)    Perplexity=47.140    Speed=3747 wps\n",
      "(87.80%)    Perplexity=47.091    Speed=3749 wps\n",
      "(88.44%)    Perplexity=47.044    Speed=3752 wps\n",
      "(89.09%)    Perplexity=47.053    Speed=3756 wps\n",
      "(89.74%)    Perplexity=47.106    Speed=3758 wps\n",
      "(90.38%)    Perplexity=47.078    Speed=3760 wps\n",
      "(91.03%)    Perplexity=47.076    Speed=3761 wps\n",
      "(91.67%)    Perplexity=47.001    Speed=3749 wps\n",
      "(92.32%)    Perplexity=46.959    Speed=3736 wps\n",
      "(92.96%)    Perplexity=46.928    Speed=3740 wps\n",
      "(93.61%)    Perplexity=46.831    Speed=3743 wps\n",
      "(94.25%)    Perplexity=46.778    Speed=3746 wps\n",
      "(94.90%)    Perplexity=46.716    Speed=3749 wps\n",
      "(95.55%)    Perplexity=46.677    Speed=3752 wps\n",
      "(96.19%)    Perplexity=46.641    Speed=3748 wps\n",
      "(96.84%)    Perplexity=46.659    Speed=3739 wps\n",
      "(97.48%)    Perplexity=46.645    Speed=3738 wps\n",
      "(98.13%)    Perplexity=46.652    Speed=3739 wps\n",
      "(98.77%)    Perplexity=46.629    Speed=3739 wps\n",
      "(99.42%)    Perplexity=46.639    Speed=3730 wps\n",
      "Epoch 11 : Train Perplexity: 46.646\n",
      "Epoch 11 : Valid Perplexity: 137.018\n",
      "Epoch 12 : Learning rate: 0.115\n",
      "(0.00%)    Perplexity=80.919    Speed=3028 wps\n",
      "(0.65%)    Perplexity=54.288    Speed=4068 wps\n",
      "(1.29%)    Perplexity=52.209    Speed=4010 wps\n",
      "(1.94%)    Perplexity=49.190    Speed=3990 wps\n",
      "(2.58%)    Perplexity=47.341    Speed=3657 wps\n",
      "(3.23%)    Perplexity=48.027    Speed=3281 wps\n",
      "(3.87%)    Perplexity=47.592    Speed=3391 wps\n",
      "(4.52%)    Perplexity=47.075    Speed=3494 wps\n",
      "(5.16%)    Perplexity=47.798    Speed=3576 wps\n",
      "(5.81%)    Perplexity=48.338    Speed=3642 wps\n",
      "(6.46%)    Perplexity=48.470    Speed=3694 wps\n",
      "(7.10%)    Perplexity=49.461    Speed=3695 wps\n",
      "(7.75%)    Perplexity=50.361    Speed=3592 wps\n",
      "(8.39%)    Perplexity=50.638    Speed=3561 wps\n",
      "(9.04%)    Perplexity=51.009    Speed=3605 wps\n",
      "(9.68%)    Perplexity=51.414    Speed=3641 wps\n",
      "(10.33%)    Perplexity=51.460    Speed=3675 wps\n",
      "(10.97%)    Perplexity=51.890    Speed=3705 wps\n",
      "(11.62%)    Perplexity=51.798    Speed=3731 wps\n",
      "(12.27%)    Perplexity=51.553    Speed=3682 wps\n",
      "(12.91%)    Perplexity=51.353    Speed=3620 wps\n",
      "(13.56%)    Perplexity=51.273    Speed=3639 wps\n",
      "(14.20%)    Perplexity=50.979    Speed=3664 wps\n",
      "(14.85%)    Perplexity=50.978    Speed=3687 wps\n",
      "(15.49%)    Perplexity=50.817    Speed=3708 wps\n",
      "(16.14%)    Perplexity=50.860    Speed=3727 wps\n",
      "(16.79%)    Perplexity=51.016    Speed=3728 wps\n",
      "(17.43%)    Perplexity=51.023    Speed=3679 wps\n",
      "(18.08%)    Perplexity=50.800    Speed=3660 wps\n",
      "(18.72%)    Perplexity=51.026    Speed=3678 wps\n",
      "(19.37%)    Perplexity=51.092    Speed=3696 wps\n",
      "(20.01%)    Perplexity=51.063    Speed=3712 wps\n",
      "(20.66%)    Perplexity=50.803    Speed=3727 wps\n",
      "(21.30%)    Perplexity=50.636    Speed=3741 wps\n",
      "(21.95%)    Perplexity=50.461    Speed=3716 wps\n",
      "(22.60%)    Perplexity=50.268    Speed=3679 wps\n",
      "(23.24%)    Perplexity=50.180    Speed=3679 wps\n",
      "(23.89%)    Perplexity=50.328    Speed=3683 wps\n",
      "(24.53%)    Perplexity=50.371    Speed=3692 wps\n",
      "(25.18%)    Perplexity=50.266    Speed=3704 wps\n",
      "(25.82%)    Perplexity=50.125    Speed=3716 wps\n",
      "(26.47%)    Perplexity=50.119    Speed=3725 wps\n",
      "(27.11%)    Perplexity=49.894    Speed=3730 wps\n",
      "(27.76%)    Perplexity=49.833    Speed=3733 wps\n",
      "(28.41%)    Perplexity=49.759    Speed=3689 wps\n",
      "(29.05%)    Perplexity=49.752    Speed=3658 wps\n",
      "(29.70%)    Perplexity=49.510    Speed=3670 wps\n",
      "(30.34%)    Perplexity=49.305    Speed=3681 wps\n",
      "(30.99%)    Perplexity=49.158    Speed=3692 wps\n",
      "(31.63%)    Perplexity=49.094    Speed=3700 wps\n",
      "(32.28%)    Perplexity=48.951    Speed=3709 wps\n",
      "(32.92%)    Perplexity=48.914    Speed=3695 wps\n",
      "(33.57%)    Perplexity=48.724    Speed=3670 wps\n",
      "(34.22%)    Perplexity=48.600    Speed=3670 wps\n",
      "(34.86%)    Perplexity=48.424    Speed=3675 wps\n",
      "(35.51%)    Perplexity=48.130    Speed=3681 wps\n",
      "(36.15%)    Perplexity=47.793    Speed=3690 wps\n",
      "(36.80%)    Perplexity=47.493    Speed=3699 wps\n",
      "(37.44%)    Perplexity=47.479    Speed=3705 wps\n",
      "(38.09%)    Perplexity=47.426    Speed=3709 wps\n",
      "(38.73%)    Perplexity=47.366    Speed=3713 wps\n",
      "(39.38%)    Perplexity=47.362    Speed=3719 wps\n",
      "(40.03%)    Perplexity=47.341    Speed=3727 wps\n",
      "(40.67%)    Perplexity=47.292    Speed=3735 wps\n",
      "(41.32%)    Perplexity=47.283    Speed=3737 wps\n",
      "(41.96%)    Perplexity=47.216    Speed=3740 wps\n",
      "(42.61%)    Perplexity=47.156    Speed=3732 wps\n",
      "(43.25%)    Perplexity=47.051    Speed=3697 wps\n",
      "(43.90%)    Perplexity=47.039    Speed=3693 wps\n",
      "(44.54%)    Perplexity=47.187    Speed=3700 wps\n",
      "(45.19%)    Perplexity=47.256    Speed=3707 wps\n",
      "(45.84%)    Perplexity=47.262    Speed=3714 wps\n",
      "(46.48%)    Perplexity=47.220    Speed=3720 wps\n",
      "(47.13%)    Perplexity=47.199    Speed=3724 wps\n",
      "(47.77%)    Perplexity=47.120    Speed=3706 wps\n",
      "(48.42%)    Perplexity=46.995    Speed=3694 wps\n",
      "(49.06%)    Perplexity=46.979    Speed=3697 wps\n",
      "(49.71%)    Perplexity=46.985    Speed=3699 wps\n",
      "(50.36%)    Perplexity=47.009    Speed=3704 wps\n",
      "(51.00%)    Perplexity=47.111    Speed=3710 wps\n",
      "(51.65%)    Perplexity=47.112    Speed=3716 wps\n",
      "(52.29%)    Perplexity=47.100    Speed=3719 wps\n",
      "(52.94%)    Perplexity=47.096    Speed=3722 wps\n",
      "(53.58%)    Perplexity=47.049    Speed=3719 wps\n",
      "(54.23%)    Perplexity=47.006    Speed=3691 wps\n",
      "(54.87%)    Perplexity=47.009    Speed=3686 wps\n",
      "(55.52%)    Perplexity=47.057    Speed=3692 wps\n",
      "(56.17%)    Perplexity=47.058    Speed=3697 wps\n",
      "(56.81%)    Perplexity=47.066    Speed=3703 wps\n",
      "(57.46%)    Perplexity=47.012    Speed=3708 wps\n",
      "(58.10%)    Perplexity=46.950    Speed=3713 wps\n",
      "(58.75%)    Perplexity=46.790    Speed=3698 wps\n",
      "(59.39%)    Perplexity=46.630    Speed=3687 wps\n",
      "(60.04%)    Perplexity=46.517    Speed=3690 wps\n",
      "(60.68%)    Perplexity=46.423    Speed=3692 wps\n",
      "(61.33%)    Perplexity=46.354    Speed=3696 wps\n",
      "(61.98%)    Perplexity=46.208    Speed=3702 wps\n",
      "(62.62%)    Perplexity=46.087    Speed=3706 wps\n",
      "(63.27%)    Perplexity=46.055    Speed=3709 wps\n",
      "(63.91%)    Perplexity=46.004    Speed=3711 wps\n",
      "(64.56%)    Perplexity=45.919    Speed=3710 wps\n",
      "(65.20%)    Perplexity=45.831    Speed=3688 wps\n",
      "(65.85%)    Perplexity=45.881    Speed=3681 wps\n",
      "(66.49%)    Perplexity=45.894    Speed=3685 wps\n",
      "(67.14%)    Perplexity=45.925    Speed=3690 wps\n",
      "(67.79%)    Perplexity=45.945    Speed=3695 wps\n",
      "(68.43%)    Perplexity=45.898    Speed=3699 wps\n",
      "(69.08%)    Perplexity=45.876    Speed=3703 wps\n",
      "(69.72%)    Perplexity=45.898    Speed=3692 wps\n",
      "(70.37%)    Perplexity=45.863    Speed=3682 wps\n",
      "(71.01%)    Perplexity=45.845    Speed=3686 wps\n",
      "(71.66%)    Perplexity=45.892    Speed=3690 wps\n",
      "(72.30%)    Perplexity=45.826    Speed=3695 wps\n",
      "(72.95%)    Perplexity=45.756    Speed=3699 wps\n",
      "(73.60%)    Perplexity=45.651    Speed=3703 wps\n",
      "(74.24%)    Perplexity=45.561    Speed=3700 wps\n",
      "(74.89%)    Perplexity=45.478    Speed=3688 wps\n",
      "(75.53%)    Perplexity=45.450    Speed=3686 wps\n",
      "(76.18%)    Perplexity=45.391    Speed=3688 wps\n",
      "(76.82%)    Perplexity=45.426    Speed=3690 wps\n",
      "(77.47%)    Perplexity=45.434    Speed=3694 wps\n",
      "(78.11%)    Perplexity=45.454    Speed=3698 wps\n",
      "(78.76%)    Perplexity=45.436    Speed=3701 wps\n",
      "(79.41%)    Perplexity=45.463    Speed=3703 wps\n",
      "(80.05%)    Perplexity=45.468    Speed=3705 wps\n",
      "(80.70%)    Perplexity=45.481    Speed=3708 wps\n",
      "(81.34%)    Perplexity=45.490    Speed=3711 wps\n",
      "(81.99%)    Perplexity=45.448    Speed=3714 wps\n",
      "(82.63%)    Perplexity=45.428    Speed=3716 wps\n",
      "(83.28%)    Perplexity=45.374    Speed=3718 wps\n",
      "(83.93%)    Perplexity=45.250    Speed=3720 wps\n",
      "(84.57%)    Perplexity=45.138    Speed=3724 wps\n",
      "(85.22%)    Perplexity=44.979    Speed=3727 wps\n",
      "(85.86%)    Perplexity=44.875    Speed=3730 wps\n",
      "(86.51%)    Perplexity=44.763    Speed=3731 wps\n",
      "(87.15%)    Perplexity=44.671    Speed=3733 wps\n",
      "(87.80%)    Perplexity=44.622    Speed=3721 wps\n",
      "(88.44%)    Perplexity=44.575    Speed=3707 wps\n",
      "(89.09%)    Perplexity=44.581    Speed=3711 wps\n",
      "(89.74%)    Perplexity=44.627    Speed=3714 wps\n",
      "(90.38%)    Perplexity=44.600    Speed=3717 wps\n",
      "(91.03%)    Perplexity=44.597    Speed=3720 wps\n",
      "(91.67%)    Perplexity=44.525    Speed=3724 wps\n",
      "(92.32%)    Perplexity=44.484    Speed=3720 wps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92.96%)    Perplexity=44.454    Speed=3711 wps\n",
      "(93.61%)    Perplexity=44.360    Speed=3709 wps\n",
      "(94.25%)    Perplexity=44.309    Speed=3711 wps\n",
      "(94.90%)    Perplexity=44.250    Speed=3712 wps\n",
      "(95.55%)    Perplexity=44.212    Speed=3716 wps\n",
      "(96.19%)    Perplexity=44.175    Speed=3719 wps\n",
      "(96.84%)    Perplexity=44.187    Speed=3721 wps\n",
      "(97.48%)    Perplexity=44.172    Speed=3723 wps\n",
      "(98.13%)    Perplexity=44.176    Speed=3724 wps\n",
      "(98.77%)    Perplexity=44.152    Speed=3716 wps\n",
      "(99.42%)    Perplexity=44.160    Speed=3702 wps\n",
      "Epoch 12 : Train Perplexity: 44.165\n",
      "Epoch 12 : Valid Perplexity: 139.006\n",
      "Epoch 13 : Learning rate: 0.077\n",
      "(0.00%)    Perplexity=77.454    Speed=2969 wps\n",
      "(0.65%)    Perplexity=52.123    Speed=3014 wps\n",
      "(1.29%)    Perplexity=50.311    Speed=2872 wps\n",
      "(1.94%)    Perplexity=47.490    Speed=3114 wps\n",
      "(2.58%)    Perplexity=45.689    Speed=3280 wps\n",
      "(3.23%)    Perplexity=46.339    Speed=3404 wps\n",
      "(3.87%)    Perplexity=45.920    Speed=3518 wps\n",
      "(4.52%)    Perplexity=45.436    Speed=3607 wps\n",
      "(5.16%)    Perplexity=46.136    Speed=3662 wps\n",
      "(5.81%)    Perplexity=46.648    Speed=3689 wps\n",
      "(6.46%)    Perplexity=46.772    Speed=3713 wps\n",
      "(7.10%)    Perplexity=47.727    Speed=3557 wps\n",
      "(7.75%)    Perplexity=48.603    Speed=3451 wps\n",
      "(8.39%)    Perplexity=48.872    Speed=3502 wps\n",
      "(9.04%)    Perplexity=49.221    Speed=3547 wps\n",
      "(9.68%)    Perplexity=49.614    Speed=3583 wps\n",
      "(10.33%)    Perplexity=49.667    Speed=3615 wps\n",
      "(10.97%)    Perplexity=50.083    Speed=3645 wps\n",
      "(11.62%)    Perplexity=50.000    Speed=3608 wps\n",
      "(12.27%)    Perplexity=49.755    Speed=3548 wps\n",
      "(12.91%)    Perplexity=49.558    Speed=3555 wps\n",
      "(13.56%)    Perplexity=49.484    Speed=3572 wps\n",
      "(14.20%)    Perplexity=49.205    Speed=3589 wps\n",
      "(14.85%)    Perplexity=49.210    Speed=3614 wps\n",
      "(15.49%)    Perplexity=49.059    Speed=3636 wps\n",
      "(16.14%)    Perplexity=49.103    Speed=3653 wps\n",
      "(16.79%)    Perplexity=49.257    Speed=3659 wps\n",
      "(17.43%)    Perplexity=49.273    Speed=3668 wps\n",
      "(18.08%)    Perplexity=49.067    Speed=3608 wps\n",
      "(18.72%)    Perplexity=49.296    Speed=3559 wps\n",
      "(19.37%)    Perplexity=49.356    Speed=3579 wps\n",
      "(20.01%)    Perplexity=49.323    Speed=3597 wps\n",
      "(20.66%)    Perplexity=49.073    Speed=3615 wps\n",
      "(21.30%)    Perplexity=48.906    Speed=3631 wps\n",
      "(21.95%)    Perplexity=48.737    Speed=3647 wps\n",
      "(22.60%)    Perplexity=48.544    Speed=3631 wps\n",
      "(23.24%)    Perplexity=48.459    Speed=3599 wps\n",
      "(23.89%)    Perplexity=48.603    Speed=3598 wps\n",
      "(24.53%)    Perplexity=48.644    Speed=3606 wps\n",
      "(25.18%)    Perplexity=48.541    Speed=3607 wps\n",
      "(25.82%)    Perplexity=48.401    Speed=3579 wps\n",
      "(26.47%)    Perplexity=48.395    Speed=3563 wps\n",
      "(27.11%)    Perplexity=48.176    Speed=3571 wps\n",
      "(27.76%)    Perplexity=48.115    Speed=3579 wps\n",
      "(28.41%)    Perplexity=48.046    Speed=3591 wps\n",
      "(29.05%)    Perplexity=48.039    Speed=3603 wps\n",
      "(29.70%)    Perplexity=47.802    Speed=3615 wps\n",
      "(30.34%)    Perplexity=47.607    Speed=3622 wps\n",
      "(30.99%)    Perplexity=47.467    Speed=3628 wps\n",
      "(31.63%)    Perplexity=47.407    Speed=3635 wps\n",
      "(32.28%)    Perplexity=47.265    Speed=3646 wps\n",
      "(32.92%)    Perplexity=47.229    Speed=3656 wps\n",
      "(33.57%)    Perplexity=47.048    Speed=3664 wps\n",
      "(34.22%)    Perplexity=46.922    Speed=3669 wps\n",
      "(34.86%)    Perplexity=46.750    Speed=3674 wps\n",
      "(35.51%)    Perplexity=46.462    Speed=3682 wps\n",
      "(36.15%)    Perplexity=46.136    Speed=3691 wps\n",
      "(36.80%)    Perplexity=45.844    Speed=3699 wps\n",
      "(37.44%)    Perplexity=45.823    Speed=3703 wps\n",
      "(38.09%)    Perplexity=45.766    Speed=3707 wps\n",
      "(38.73%)    Perplexity=45.702    Speed=3697 wps\n",
      "(39.38%)    Perplexity=45.696    Speed=3660 wps\n",
      "(40.03%)    Perplexity=45.671    Speed=3659 wps\n",
      "(40.67%)    Perplexity=45.621    Speed=3667 wps\n",
      "(41.32%)    Perplexity=45.611    Speed=3675 wps\n",
      "(41.96%)    Perplexity=45.541    Speed=3683 wps\n",
      "(42.61%)    Perplexity=45.484    Speed=3690 wps\n",
      "(43.25%)    Perplexity=45.380    Speed=3694 wps\n",
      "(43.90%)    Perplexity=45.365    Speed=3675 wps\n",
      "(44.54%)    Perplexity=45.505    Speed=3663 wps\n",
      "(45.19%)    Perplexity=45.567    Speed=3666 wps\n",
      "(45.84%)    Perplexity=45.572    Speed=3670 wps\n",
      "(46.48%)    Perplexity=45.527    Speed=3659 wps\n",
      "(47.13%)    Perplexity=45.506    Speed=3642 wps\n",
      "(47.77%)    Perplexity=45.426    Speed=3640 wps\n",
      "(48.42%)    Perplexity=45.304    Speed=3644 wps\n",
      "(49.06%)    Perplexity=45.287    Speed=3642 wps\n",
      "(49.71%)    Perplexity=45.291    Speed=3626 wps\n",
      "(50.36%)    Perplexity=45.312    Speed=3620 wps\n",
      "(51.00%)    Perplexity=45.409    Speed=3624 wps\n",
      "(51.65%)    Perplexity=45.409    Speed=3628 wps\n",
      "(52.29%)    Perplexity=45.397    Speed=3634 wps\n",
      "(52.94%)    Perplexity=45.391    Speed=3640 wps\n",
      "(53.58%)    Perplexity=45.344    Speed=3647 wps\n",
      "(54.23%)    Perplexity=45.300    Speed=3650 wps\n",
      "(54.87%)    Perplexity=45.300    Speed=3653 wps\n",
      "(55.52%)    Perplexity=45.343    Speed=3657 wps\n",
      "(56.17%)    Perplexity=45.344    Speed=3663 wps\n",
      "(56.81%)    Perplexity=45.350    Speed=3669 wps\n",
      "(57.46%)    Perplexity=45.295    Speed=3673 wps\n",
      "(58.10%)    Perplexity=45.232    Speed=3676 wps\n",
      "(58.75%)    Perplexity=45.075    Speed=3679 wps\n",
      "(59.39%)    Perplexity=44.918    Speed=3657 wps\n",
      "(60.04%)    Perplexity=44.807    Speed=3644 wps\n",
      "(60.68%)    Perplexity=44.711    Speed=3649 wps\n",
      "(61.33%)    Perplexity=44.642    Speed=3655 wps\n",
      "(61.98%)    Perplexity=44.499    Speed=3660 wps\n",
      "(62.62%)    Perplexity=44.380    Speed=3665 wps\n",
      "(63.27%)    Perplexity=44.345    Speed=3671 wps\n",
      "(63.91%)    Perplexity=44.293    Speed=3662 wps\n",
      "(64.56%)    Perplexity=44.208    Speed=3649 wps\n",
      "(65.20%)    Perplexity=44.120    Speed=3652 wps\n",
      "(65.85%)    Perplexity=44.166    Speed=3657 wps\n",
      "(66.49%)    Perplexity=44.178    Speed=3662 wps\n",
      "(67.14%)    Perplexity=44.205    Speed=3667 wps\n",
      "(67.79%)    Perplexity=44.221    Speed=3672 wps\n",
      "(68.43%)    Perplexity=44.174    Speed=3672 wps\n",
      "(69.08%)    Perplexity=44.149    Speed=3660 wps\n",
      "(69.72%)    Perplexity=44.167    Speed=3654 wps\n",
      "(70.37%)    Perplexity=44.132    Speed=3657 wps\n",
      "(71.01%)    Perplexity=44.110    Speed=3659 wps\n",
      "(71.66%)    Perplexity=44.151    Speed=3663 wps\n",
      "(72.30%)    Perplexity=44.086    Speed=3668 wps\n",
      "(72.95%)    Perplexity=44.014    Speed=3672 wps\n",
      "(73.60%)    Perplexity=43.911    Speed=3674 wps\n",
      "(74.24%)    Perplexity=43.822    Speed=3677 wps\n",
      "(74.89%)    Perplexity=43.740    Speed=3680 wps\n",
      "(75.53%)    Perplexity=43.709    Speed=3684 wps\n",
      "(76.18%)    Perplexity=43.649    Speed=3688 wps\n",
      "(76.82%)    Perplexity=43.680    Speed=3691 wps\n",
      "(77.47%)    Perplexity=43.685    Speed=3693 wps\n",
      "(78.11%)    Perplexity=43.701    Speed=3695 wps\n",
      "(78.76%)    Perplexity=43.680    Speed=3678 wps\n",
      "(79.41%)    Perplexity=43.702    Speed=3668 wps\n",
      "(80.05%)    Perplexity=43.702    Speed=3672 wps\n",
      "(80.70%)    Perplexity=43.710    Speed=3676 wps\n",
      "(81.34%)    Perplexity=43.716    Speed=3680 wps\n",
      "(81.99%)    Perplexity=43.673    Speed=3683 wps\n",
      "(82.63%)    Perplexity=43.649    Speed=3687 wps\n",
      "(83.28%)    Perplexity=43.594    Speed=3680 wps\n",
      "(83.93%)    Perplexity=43.472    Speed=3670 wps\n",
      "(84.57%)    Perplexity=43.361    Speed=3673 wps\n",
      "(85.22%)    Perplexity=43.207    Speed=3677 wps\n",
      "(85.86%)    Perplexity=43.105    Speed=3680 wps\n",
      "(86.51%)    Perplexity=42.995    Speed=3684 wps\n",
      "(87.15%)    Perplexity=42.903    Speed=3688 wps\n",
      "(87.80%)    Perplexity=42.853    Speed=3687 wps\n",
      "(88.44%)    Perplexity=42.805    Speed=3678 wps\n",
      "(89.09%)    Perplexity=42.808    Speed=3673 wps\n",
      "(89.74%)    Perplexity=42.848    Speed=3675 wps\n",
      "(90.38%)    Perplexity=42.819    Speed=3677 wps\n",
      "(91.03%)    Perplexity=42.815    Speed=3680 wps\n",
      "(91.67%)    Perplexity=42.744    Speed=3683 wps\n",
      "(92.32%)    Perplexity=42.703    Speed=3686 wps\n",
      "(92.96%)    Perplexity=42.673    Speed=3687 wps\n",
      "(93.61%)    Perplexity=42.581    Speed=3689 wps\n",
      "(94.25%)    Perplexity=42.531    Speed=3691 wps\n",
      "(94.90%)    Perplexity=42.472    Speed=3694 wps\n",
      "(95.55%)    Perplexity=42.433    Speed=3697 wps\n",
      "(96.19%)    Perplexity=42.395    Speed=3699 wps\n",
      "(96.84%)    Perplexity=42.404    Speed=3701 wps\n",
      "(97.48%)    Perplexity=42.386    Speed=3701 wps\n",
      "(98.13%)    Perplexity=42.387    Speed=3686 wps\n",
      "(98.77%)    Perplexity=42.361    Speed=3675 wps\n",
      "(99.42%)    Perplexity=42.366    Speed=3667 wps\n",
      "Epoch 13 : Train Perplexity: 42.369\n",
      "Epoch 13 : Valid Perplexity: 139.761\n",
      "Test Perplexity: 134.081\n"
     ]
    }
   ],
   "source": [
    "#Initializes the Execution Graph and the Session\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    initializer = tf.random_uniform_initializer(-init_scale,init_scale)\n",
    "    \n",
    "    # Instantiates the model for training\n",
    "    # tf.variable_scope add a prefix to the variables created with tf.get_variable\n",
    "    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "        m = PTBModel(is_training=True)\n",
    "        \n",
    "    # Reuses the trained parameters for the validation and testing models\n",
    "    # They are different instances but use the same variables for weights and biases, \n",
    "    # they just don't change when data is input\n",
    "    with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "        mvalid = PTBModel(is_training=False)\n",
    "        mtest = PTBModel(is_training=False)\n",
    "\n",
    "    #Initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # Set initial learning rate\n",
    "    m.assign_lr(session=session, lr_value=learning_rate)\n",
    "    \n",
    "    for i in range(max_max_epoch):\n",
    "        print(\"Epoch %d : Learning rate: %.3f\" % (i + 1, session.run(m.lr)))\n",
    "               \n",
    "        # Run the loop for this epoch in the training model\n",
    "        train_perplexity = run_epoch(session, m, train_data, m.train_op,\n",
    "                                   verbose=True)\n",
    "        print(\"Epoch %d : Train Perplexity: %.3f\" % (i + 1, train_perplexity))\n",
    "        \n",
    "        # Run the loop for this epoch in the validation model\n",
    "        valid_perplexity = run_epoch(session, mvalid, valid_data, tf.no_op())\n",
    "        print(\"Epoch %d : Valid Perplexity: %.3f\" % (i + 1, valid_perplexity))\n",
    "        \n",
    "        # Define the decay for the next epoch \n",
    "        lr_decay = decay * ((max_max_epoch - i) / max_max_epoch)\n",
    "        \n",
    "        # Set the decayed learning rate as the learning rate for the next epoch\n",
    "        m.assign_lr(session, learning_rate * lr_decay)\n",
    "    \n",
    "    # Run the loop in the testing model to see how effective was our training\n",
    "    test_perplexity = run_epoch(session, mtest, test_data, tf.no_op())\n",
    "    \n",
    "    print(\"Test Perplexity: %.3f\" % test_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model's perplexity rating drops very quickly after a few iterations. As was elaborated before, **lower Perplexity means that the model is more certain about its prediction**. As such, we can be sure that this model is performing well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the **Applying Recurrent Neural Networks to Text Processing** notebook. Hopefully you now have a better understanding of Recurrent Neural Networks and how to implement one utilizing TensorFlow. Thank you for reading this notebook, and good luck on your studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by <a href=\"https://br.linkedin.com/in/walter-gomes-de-amorim-junior-624726121\">Walter Gomes de Amorim Junior</a>, <a href = \"https://linkedin.com/in/saeedaghabozorgi\"> Saeed Aghabozorgi </a></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
